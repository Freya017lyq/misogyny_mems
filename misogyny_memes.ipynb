{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "misogyny_memes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_vouUU4MCECo",
        "G0cBYx6PnKjZ",
        "_VeBpJ_tmQlK",
        "2tFLj-3IutU8",
        "vrhMol_wPi8E",
        "a4dE5xeSUr3B",
        "nN5SKEjIv2Cq",
        "b1BcTXCnrnVb",
        "x2FH1NW8rvmO",
        "Qw9UhYbkrhA_"
      ],
      "machine_shape": "hm",
      "background_execution": "on"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Env Config"
      ],
      "metadata": {
        "id": "_vouUU4MCECo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6rodbvYuws8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "Un4rNmqtvdJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "home = \"/content/drive/MyDrive\"\n",
        "os.chdir(home)\n",
        "os.getcwd()"
      ],
      "metadata": {
        "id": "cKs7DZ3Dve2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install specified versions of `torch` and `torchvision`, before installing mmf (causes an issue)\n",
        "!pip install torch==1.6.0 torchvision==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "metadata": {
        "id": "LBk6aZZ3vgZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(os.path.join(home, \"mmf\"))"
      ],
      "metadata": {
        "id": "3uulnu7Ovnk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --editable ."
      ],
      "metadata": {
        "id": "l12YS1HBvoBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "G0cBYx6PnKjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_ZIP_FILE = \"/content/drive/MyDrive/data/misogyny_memes.zip\"\n",
        "!cp $PATH_TO_ZIP_FILE /content/drive/MyDrive/mmf/"
      ],
      "metadata": {
        "id": "5cgAIwRrv--B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['PYTHONPATH'] += \":/content/drive/MyDrive/mmf/\""
      ],
      "metadata": {
        "id": "4DWJzlX6v_Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mmf_convert_hm --zip_file=\"misogyny_memes.zip\""
      ],
      "metadata": {
        "id": "-vEqDmhUwAwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/torch/mmf/data/datasets/misogyny_memes/defaults/images/img/ | wc -l"
      ],
      "metadata": {
        "id": "YN_K43itwEXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/.cache/torch/mmf/data/datasets/misogyny_memes/defaults/images/misogyny_memes.zip\n",
        "!rm -rf $home/mmf/misogyny_memes.zip"
      ],
      "metadata": {
        "id": "Y48sWeDYwGSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Features"
      ],
      "metadata": {
        "id": "_VeBpJ_tmQlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "psGxdFutmjon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(home)\n",
        "!git clone https://gitlab.com/vedanuj/vqa-maskrcnn-benchmark"
      ],
      "metadata": {
        "id": "KbApKS2Zm53-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja yacs cython matplotlib"
      ],
      "metadata": {
        "id": "QevVQ1LWm9bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(os.path.join(home, \"vqa-maskrcnn-benchmark\"))\n",
        "!rm -rf build\n",
        "!python setup.py build develop"
      ],
      "metadata": {
        "id": "gTO55zJTm_eY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "TaYDeyUcreer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(os.path.join(home, \"mmf/tools/scripts/features/\"))\n",
        "out_folder = os.path.join(home, \"features/\")\n",
        "\n",
        "!python extract_features_vmb.py --config_file \"https://dl.fbaipublicfiles.com/pythia/detectron_model/detectron_model_x152.yaml\" \\\n",
        "                                --model_name \"X-152\" \\\n",
        "                                --output_folder $out_folder \\\n",
        "                                --image_dir \"/root/.cache/torch/mmf/data/datasets/misogyny_memes/defaults/images/img/\" \\\n",
        "                                --num_features 100 \\"
      ],
      "metadata": {
        "id": "oQo-wyKImT2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation\n",
        "os.chdir(os.path.join(home, \"mmf/tools/scripts/features/\"))\n",
        "\n",
        "!python extract_features_pic.py --image_dir \"/content/drive/MyDrive/Rub/MAMI/test/15009.jpg\" \\\n",
        "                                --output_folder \"/content/drive/MyDrive/mask_pic\" \\\n",
        "                                --show_visual_info True \\\n",
        "                                --num_features 20 \\"
      ],
      "metadata": {
        "id": "LrJbf1gVi2BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dir = '/content/drive/MyDrive/features'\n",
        "for i in img_dir:\n",
        "  if i == '645.npy':\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "HWMEr7O89ydX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning with pretrained models\n"
      ],
      "metadata": {
        "id": "pPiJywXAdCBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unimodel-sub1: VisualBert"
      ],
      "metadata": {
        "id": "Hmr8yEbWGiQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visual_bert_0801_2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sxWS2koElBRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "!mmf_run config=\"projects/visual_bert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5e-5 \\\n",
        "        training.lr_ratio=0.7 \\\n",
        "        env.save_dir=./visualbert_0801_2 \\\n",
        "        env.tensorboard_logdir=logs/fit/visualbert_0801_2 \\"
      ],
      "metadata": {
        "id": "58kIRH9h4Ron",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a4d454f-5c18-4fea-d2b6-5b50fac25606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5e-5\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.7\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./visualbert_0801_2\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/visualbert_0801_2\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf: \u001b[0mLogging to: ./visualbert_0801_2/train.log\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/misogyny_memes/defaults.yaml', 'model=visual_bert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.batch_size=32', 'optimizer.params.lr=5e-5', 'training.lr_ratio=0.7', 'env.save_dir=./visualbert_0801_2', 'env.tensorboard_logdir=logs/fit/visualbert_0801_2'])\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf_cli.run: \u001b[0mUsing seed 20179667\n",
            "\u001b[32m2022-08-01T03:50:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-01T03:50:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-01T03:50:36 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-01T03:50:36 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-01T03:50:36 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-01T03:50:36 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-01T03:50:36 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-08-01T03:50:36 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-01T03:51:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:51:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:51:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T03:51:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.6362, val/total_loss: 0.6362, val/misogyny_memes/accuracy: 0.6690, val/misogyny_memes/binary_f1: 0.6633, val/misogyny_memes/roc_auc: 0.7406, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 31s 074ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.740589\n",
            "\u001b[32m2022-08-01T03:52:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.3341, train/misogyny_memes/cross_entropy/avg: 0.3341, train/total_loss: 0.3341, train/total_loss/avg: 0.3341, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 739ms, time_since_start: 01m 50s 411ms, eta: 06m 04s 805ms\n",
            "\u001b[32m2022-08-01T03:52:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:52:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:52:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T03:52:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.5347, val/total_loss: 0.5347, val/misogyny_memes/accuracy: 0.7540, val/misogyny_memes/binary_f1: 0.7437, val/misogyny_memes/roc_auc: 0.8340, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 31s 676ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.833956\n",
            "\u001b[32m2022-08-01T03:53:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:53:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:53:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T03:54:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.4686, val/total_loss: 0.4686, val/misogyny_memes/accuracy: 0.8040, val/misogyny_memes/binary_f1: 0.8063, val/misogyny_memes/roc_auc: 0.8606, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 27s 636ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.860638\n",
            "\u001b[32m2022-08-01T03:54:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.3341, train/misogyny_memes/cross_entropy/avg: 0.4884, train/total_loss: 0.3341, train/total_loss/avg: 0.4884, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 467ms, time_since_start: 04m 08s 527ms, eta: 05m 22s 055ms\n",
            "\u001b[32m2022-08-01T03:54:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:54:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:54:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T03:55:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.4770, val/total_loss: 0.4770, val/misogyny_memes/accuracy: 0.7810, val/misogyny_memes/binary_f1: 0.7967, val/misogyny_memes/roc_auc: 0.8703, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 28s 786ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.870264\n",
            "\u001b[32m2022-08-01T03:55:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:56:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:56:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T03:56:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4507, val/total_loss: 0.4507, val/misogyny_memes/accuracy: 0.8020, val/misogyny_memes/binary_f1: 0.8074, val/misogyny_memes/roc_auc: 0.8824, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 31s 686ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.882384\n",
            "\u001b[32m2022-08-01T03:57:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.4109, train/misogyny_memes/cross_entropy/avg: 0.4626, train/total_loss: 0.4109, train/total_loss/avg: 0.4626, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 797ms, time_since_start: 06m 28s 554ms, eta: 04m 44s 153ms\n",
            "\u001b[32m2022-08-01T03:57:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:57:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:57:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T03:57:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4658, val/total_loss: 0.4658, val/misogyny_memes/accuracy: 0.7870, val/misogyny_memes/binary_f1: 0.8107, val/misogyny_memes/roc_auc: 0.8833, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 31s 825ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.883297\n",
            "\u001b[32m2022-08-01T03:58:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:58:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:58:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T03:58:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4249, val/total_loss: 0.4249, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8214, val/misogyny_memes/roc_auc: 0.9013, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 32s 080ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.901309\n",
            "\u001b[32m2022-08-01T03:59:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.3341, train/misogyny_memes/cross_entropy/avg: 0.4021, train/total_loss: 0.3341, train/total_loss/avg: 0.4021, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 867ms, time_since_start: 08m 51s 773ms, eta: 04m 03s 989ms\n",
            "\u001b[32m2022-08-01T03:59:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:59:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T03:59:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T03:59:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4277, val/total_loss: 0.4277, val/misogyny_memes/accuracy: 0.8090, val/misogyny_memes/binary_f1: 0.8217, val/misogyny_memes/roc_auc: 0.8997, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 23s 163ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.901309\n",
            "\u001b[32m2022-08-01T04:00:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:00:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:00:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:00:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4092, val/total_loss: 0.4092, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8215, val/misogyny_memes/roc_auc: 0.8973, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 23s 830ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.901309\n",
            "\u001b[32m2022-08-01T04:01:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.3341, train/misogyny_memes/cross_entropy/avg: 0.3858, train/total_loss: 0.3341, train/total_loss/avg: 0.3858, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 732ms, time_since_start: 10m 57s 801ms, eta: 03m 22s 636ms\n",
            "\u001b[32m2022-08-01T04:01:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:01:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:01:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:02:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4198, val/total_loss: 0.4198, val/misogyny_memes/accuracy: 0.8040, val/misogyny_memes/binary_f1: 0.8228, val/misogyny_memes/roc_auc: 0.9029, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 33s 987ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.902922\n",
            "\u001b[32m2022-08-01T04:02:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:02:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:02:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:03:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4388, val/total_loss: 0.4388, val/misogyny_memes/accuracy: 0.8300, val/misogyny_memes/binary_f1: 0.8443, val/misogyny_memes/roc_auc: 0.9049, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 30s 332ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.904944\n",
            "\u001b[32m2022-08-01T04:03:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.3341, train/misogyny_memes/cross_entropy/avg: 0.3861, train/total_loss: 0.3341, train/total_loss/avg: 0.3861, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 901ms, time_since_start: 13m 21s 268ms, eta: 02m 42s 796ms\n",
            "\u001b[32m2022-08-01T04:03:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:04:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:04:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:04:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4681, val/total_loss: 0.4681, val/misogyny_memes/accuracy: 0.8260, val/misogyny_memes/binary_f1: 0.8365, val/misogyny_memes/roc_auc: 0.9062, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 32s 232ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.906161\n",
            "\u001b[32m2022-08-01T04:05:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:05:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:05:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:05:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.5427, val/total_loss: 0.5427, val/misogyny_memes/accuracy: 0.7940, val/misogyny_memes/binary_f1: 0.7859, val/misogyny_memes/roc_auc: 0.8945, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 26s 205ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.906161\n",
            "\u001b[32m2022-08-01T04:06:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.3341, train/misogyny_memes/cross_entropy/avg: 0.3598, train/total_loss: 0.3341, train/total_loss/avg: 0.3598, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 577ms, time_since_start: 15m 39s 044ms, eta: 02m 01s 106ms\n",
            "\u001b[32m2022-08-01T04:06:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:06:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:06:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:06:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.5206, val/total_loss: 0.5206, val/misogyny_memes/accuracy: 0.8030, val/misogyny_memes/binary_f1: 0.8264, val/misogyny_memes/roc_auc: 0.8343, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 24s 738ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.906161\n",
            "\u001b[32m2022-08-01T04:07:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:07:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:07:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:07:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4385, val/total_loss: 0.4385, val/misogyny_memes/accuracy: 0.8220, val/misogyny_memes/binary_f1: 0.8349, val/misogyny_memes/roc_auc: 0.9021, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 20s 906ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.906161\n",
            "\u001b[32m2022-08-01T04:08:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.3205, train/misogyny_memes/cross_entropy/avg: 0.3457, train/total_loss: 0.3205, train/total_loss/avg: 0.3457, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 363ms, time_since_start: 17m 43s 427ms, eta: 01m 20s 301ms\n",
            "\u001b[32m2022-08-01T04:08:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:08:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:08:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:08:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4697, val/total_loss: 0.4697, val/misogyny_memes/accuracy: 0.8240, val/misogyny_memes/binary_f1: 0.8298, val/misogyny_memes/roc_auc: 0.9069, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 28s 556ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.906949\n",
            "\u001b[32m2022-08-01T04:09:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:09:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:09:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:10:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4739, val/total_loss: 0.4739, val/misogyny_memes/accuracy: 0.8290, val/misogyny_memes/binary_f1: 0.8335, val/misogyny_memes/roc_auc: 0.9078, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 33s 592ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.907754\n",
            "\u001b[32m2022-08-01T04:10:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.3205, train/misogyny_memes/cross_entropy/avg: 0.3103, train/total_loss: 0.3205, train/total_loss/avg: 0.3103, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 635ms, time_since_start: 20m 04s 966ms, eta: 40s 427ms\n",
            "\u001b[32m2022-08-01T04:10:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:10:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:10:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:11:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.6845, val/total_loss: 0.6845, val/misogyny_memes/accuracy: 0.8300, val/misogyny_memes/binary_f1: 0.8353, val/misogyny_memes/roc_auc: 0.8960, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 23s 721ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.907754\n",
            "\u001b[32m2022-08-01T04:11:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:11:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:11:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:12:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.5622, val/total_loss: 0.5622, val/misogyny_memes/accuracy: 0.8300, val/misogyny_memes/binary_f1: 0.8359, val/misogyny_memes/roc_auc: 0.9047, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 23s 739ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.907754\n",
            "\u001b[32m2022-08-01T04:12:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:12:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:12:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:12:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.2472, train/misogyny_memes/cross_entropy/avg: 0.2919, train/total_loss: 0.2472, train/total_loss/avg: 0.2919, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 2.04, time: 49s 553ms, time_since_start: 22m 21s 549ms, eta: 0ms\n",
            "\u001b[32m2022-08-01T04:12:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:13:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:13:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:13:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.5300, val/total_loss: 0.5300, val/misogyny_memes/accuracy: 0.8180, val/misogyny_memes/binary_f1: 0.8124, val/misogyny_memes/roc_auc: 0.9060, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 23s 312ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.907754\n",
            "\u001b[32m2022-08-01T04:13:21 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-01T04:13:21 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-01T04:13:21 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-08-01T04:13:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T04:13:23 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 850\n",
            "\u001b[32m2022-08-01T04:13:23 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 850\n",
            "\u001b[32m2022-08-01T04:13:23 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
            "\u001b[32m2022-08-01T04:13:25 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:11<00:00,  2.89it/s]\n",
            "\u001b[32m2022-08-01T04:13:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4739, val/total_loss: 0.4739, val/misogyny_memes/accuracy: 0.8290, val/misogyny_memes/binary_f1: 0.8335, val/misogyny_memes/roc_auc: 0.9078\n",
            "\u001b[32m2022-08-01T04:13:36 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 23m 099ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear any logs from previous runs\n",
        "#!rm -rf ./logs/ "
      ],
      "metadata": {
        "id": "VAh9s7kKVFhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "import torch, gc\n",
        "import os\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Fjnrj8f0nf03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visual_bert_0801_3"
      ],
      "metadata": {
        "id": "4tU4Qd3IlMlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "!mmf_run config=\"projects/visual_bert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5e-5 \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        env.save_dir=./visualbert_0801_3 \\\n",
        "        env.tensorboard_logdir=logs/fit/visualbert_0801_3 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0JVtaaeSQ3B",
        "outputId": "f7053ffb-b69c-4df3-f366-08a8e9e50757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5e-5\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.3\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./visualbert_0801_3\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/visualbert_0801_3\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf: \u001b[0mLogging to: ./visualbert_0801_3/train.log\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/misogyny_memes/defaults.yaml', 'model=visual_bert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.batch_size=32', 'optimizer.params.lr=5e-5', 'training.lr_ratio=0.3', 'env.save_dir=./visualbert_0801_3', 'env.tensorboard_logdir=logs/fit/visualbert_0801_3'])\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf_cli.run: \u001b[0mUsing seed 15340316\n",
            "\u001b[32m2022-08-01T04:17:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-01T04:17:19 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-01T04:17:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-01T04:17:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-01T04:17:28 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-01T04:17:28 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-01T04:17:28 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-08-01T04:17:28 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-01T04:18:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:18:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:18:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:18:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.7149, val/total_loss: 0.7149, val/misogyny_memes/accuracy: 0.4870, val/misogyny_memes/binary_f1: 0.0000, val/misogyny_memes/roc_auc: 0.5603, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 30s 704ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.560328\n",
            "\u001b[32m2022-08-01T04:19:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.7167, train/misogyny_memes/cross_entropy/avg: 0.7167, train/total_loss: 0.7167, train/total_loss/avg: 0.7167, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 862ms, time_since_start: 01m 49s 887ms, eta: 06m 05s 942ms\n",
            "\u001b[32m2022-08-01T04:19:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:19:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:19:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:19:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.6992, val/total_loss: 0.6992, val/misogyny_memes/accuracy: 0.4870, val/misogyny_memes/binary_f1: 0.0000, val/misogyny_memes/roc_auc: 0.6985, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 28s 092ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.698470\n",
            "\u001b[32m2022-08-01T04:20:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:20:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:20:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:20:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.6732, val/total_loss: 0.6732, val/misogyny_memes/accuracy: 0.6370, val/misogyny_memes/binary_f1: 0.5399, val/misogyny_memes/roc_auc: 0.7367, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 33s 672ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.736698\n",
            "\u001b[32m2022-08-01T04:21:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.4649, train/misogyny_memes/cross_entropy/avg: 0.5908, train/total_loss: 0.4649, train/total_loss/avg: 0.5908, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 666ms, time_since_start: 04m 11s 182ms, eta: 05m 23s 677ms\n",
            "\u001b[32m2022-08-01T04:21:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:21:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:21:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:22:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.6748, val/total_loss: 0.6748, val/misogyny_memes/accuracy: 0.6550, val/misogyny_memes/binary_f1: 0.7319, val/misogyny_memes/roc_auc: 0.7918, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 31s 193ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.791759\n",
            "\u001b[32m2022-08-01T04:22:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:23:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:23:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:23:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.5246, val/total_loss: 0.5246, val/misogyny_memes/accuracy: 0.7680, val/misogyny_memes/binary_f1: 0.7965, val/misogyny_memes/roc_auc: 0.8440, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 34s 070ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.844035\n",
            "\u001b[32m2022-08-01T04:24:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.4649, train/misogyny_memes/cross_entropy/avg: 0.5089, train/total_loss: 0.4649, train/total_loss/avg: 0.5089, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 821ms, time_since_start: 06m 35s 894ms, eta: 04m 44s 327ms\n",
            "\u001b[32m2022-08-01T04:24:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:24:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:24:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:24:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4729, val/total_loss: 0.4729, val/misogyny_memes/accuracy: 0.8080, val/misogyny_memes/binary_f1: 0.8182, val/misogyny_memes/roc_auc: 0.8701, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 32s 660ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.870080\n",
            "\u001b[32m2022-08-01T04:25:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:25:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:25:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:25:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4724, val/total_loss: 0.4724, val/misogyny_memes/accuracy: 0.7970, val/misogyny_memes/binary_f1: 0.7996, val/misogyny_memes/roc_auc: 0.8859, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 32s 208ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.885915\n",
            "\u001b[32m2022-08-01T04:26:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.3452, train/misogyny_memes/cross_entropy/avg: 0.4559, train/total_loss: 0.3452, train/total_loss/avg: 0.4559, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 675ms, time_since_start: 08m 59s 886ms, eta: 04m 02s 815ms\n",
            "\u001b[32m2022-08-01T04:26:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:26:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:26:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:26:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4362, val/total_loss: 0.4362, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8284, val/misogyny_memes/roc_auc: 0.8943, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 28s 771ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.894341\n",
            "\u001b[32m2022-08-01T04:27:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:27:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:27:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:28:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4211, val/total_loss: 0.4211, val/misogyny_memes/accuracy: 0.8240, val/misogyny_memes/binary_f1: 0.8314, val/misogyny_memes/roc_auc: 0.8964, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 33s 479ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.896402\n",
            "\u001b[32m2022-08-01T04:28:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.4649, train/misogyny_memes/cross_entropy/avg: 0.4747, train/total_loss: 0.4649, train/total_loss/avg: 0.4747, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 815ms, time_since_start: 11m 21s 519ms, eta: 03m 23s 057ms\n",
            "\u001b[32m2022-08-01T04:28:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:29:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:29:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:29:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4305, val/total_loss: 0.4305, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8332, val/misogyny_memes/roc_auc: 0.9091, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 31s 625ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.909123\n",
            "\u001b[32m2022-08-01T04:30:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:30:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:30:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:30:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4086, val/total_loss: 0.4086, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8221, val/misogyny_memes/roc_auc: 0.9089, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 23s 959ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.909123\n",
            "\u001b[32m2022-08-01T04:31:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.3452, train/misogyny_memes/cross_entropy/avg: 0.4211, train/total_loss: 0.3452, train/total_loss/avg: 0.4211, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 688ms, time_since_start: 13m 36s 371ms, eta: 02m 41s 928ms\n",
            "\u001b[32m2022-08-01T04:31:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:31:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:31:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:31:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4065, val/total_loss: 0.4065, val/misogyny_memes/accuracy: 0.8210, val/misogyny_memes/binary_f1: 0.8303, val/misogyny_memes/roc_auc: 0.8994, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 24s 182ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.909123\n",
            "\u001b[32m2022-08-01T04:32:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:32:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:32:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:32:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.5027, val/total_loss: 0.5027, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8325, val/misogyny_memes/roc_auc: 0.8991, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 25s 858ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.909123\n",
            "\u001b[32m2022-08-01T04:33:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.3452, train/misogyny_memes/cross_entropy/avg: 0.3992, train/total_loss: 0.3452, train/total_loss/avg: 0.3992, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 247ms, time_since_start: 15m 45s 406ms, eta: 02m 098ms\n",
            "\u001b[32m2022-08-01T04:33:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:33:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:33:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:33:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4215, val/total_loss: 0.4215, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8325, val/misogyny_memes/roc_auc: 0.9065, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 22s 511ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.909123\n",
            "\u001b[32m2022-08-01T04:34:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:34:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:34:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:34:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4711, val/total_loss: 0.4711, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8402, val/misogyny_memes/roc_auc: 0.9050, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 26s 285ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.909123\n",
            "\u001b[32m2022-08-01T04:35:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.2968, train/misogyny_memes/cross_entropy/avg: 0.3836, train/total_loss: 0.2968, train/total_loss/avg: 0.3836, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 257ms, time_since_start: 17m 53s 104ms, eta: 01m 20s 085ms\n",
            "\u001b[32m2022-08-01T04:35:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:35:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:35:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:35:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4077, val/total_loss: 0.4077, val/misogyny_memes/accuracy: 0.8180, val/misogyny_memes/binary_f1: 0.8389, val/misogyny_memes/roc_auc: 0.9120, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 35s 507ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.911960\n",
            "\u001b[32m2022-08-01T04:36:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:36:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:36:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:37:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4978, val/total_loss: 0.4978, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8358, val/misogyny_memes/roc_auc: 0.9095, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 24s 104ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.911960\n",
            "\u001b[32m2022-08-01T04:37:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.2968, train/misogyny_memes/cross_entropy/avg: 0.3462, train/total_loss: 0.2968, train/total_loss/avg: 0.3462, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 450ms, time_since_start: 20m 11s 688ms, eta: 40s 239ms\n",
            "\u001b[32m2022-08-01T04:37:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:37:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:37:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:38:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5780, val/total_loss: 0.5780, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8373, val/misogyny_memes/roc_auc: 0.9037, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 23s 557ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.911960\n",
            "\u001b[32m2022-08-01T04:38:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:38:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:38:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:39:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.6130, val/total_loss: 0.6130, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8114, val/misogyny_memes/roc_auc: 0.8932, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 28s 602ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.911960\n",
            "\u001b[32m2022-08-01T04:39:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:39:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:39:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:40:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.2746, train/misogyny_memes/cross_entropy/avg: 0.3374, train/total_loss: 0.2746, train/total_loss/avg: 0.3374, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 1.85, time: 54s 891ms, time_since_start: 22m 37s 975ms, eta: 0ms\n",
            "\u001b[32m2022-08-01T04:40:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:40:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T04:40:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T04:40:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.7274, val/total_loss: 0.7274, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8103, val/misogyny_memes/roc_auc: 0.8685, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 23s 802ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.911960\n",
            "\u001b[32m2022-08-01T04:40:30 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-01T04:40:30 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-01T04:40:30 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-08-01T04:40:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T04:40:33 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 800\n",
            "\u001b[32m2022-08-01T04:40:33 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 800\n",
            "\u001b[32m2022-08-01T04:40:33 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-01T04:40:35 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:11<00:00,  2.86it/s]\n",
            "\u001b[32m2022-08-01T04:40:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4077, val/total_loss: 0.4077, val/misogyny_memes/accuracy: 0.8180, val/misogyny_memes/binary_f1: 0.8389, val/misogyny_memes/roc_auc: 0.9120\n",
            "\u001b[32m2022-08-01T04:40:46 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 23m 18s 255ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hHXVgPuhlq7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visual_bert_0801_4\n"
      ],
      "metadata": {
        "id": "8lj34dQ6liWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "!mmf_run config=\"projects/visual_bert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume=True \\\n",
        "        checkpoint.resume_best=True \\\n",
        "        training.tensorboard=True \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5e-5 \\\n",
        "        training.lr_ratio=0.7 \\\n",
        "        env.save_dir=./visualbert_0801_4 \\\n",
        "        env.tensorboard_logdir=logs/fit/visualbert_0801_4 \\"
      ],
      "metadata": {
        "id": "yuDS6jDOGkfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8ecda7-7dfe-4cf3-f16d-9eed236a939a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume to True\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_best to True\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5e-5\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.7\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./visualbert_0801_4\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/visualbert_0801_4\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf: \u001b[0mLogging to: ./visualbert_0801_4/train.log\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/misogyny_memes/defaults.yaml', 'model=visual_bert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'checkpoint.resume=True', 'checkpoint.resume_best=True', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.batch_size=32', 'optimizer.params.lr=5e-5', 'training.lr_ratio=0.7', 'env.save_dir=./visualbert_0801_4', 'env.tensorboard_logdir=logs/fit/visualbert_0801_4'])\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf_cli.run: \u001b[0mUsing seed 4939825\n",
            "\u001b[32m2022-08-01T05:01:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-01T05:01:09 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-01T05:01:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-01T05:01:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:01:15 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:171: UserWarning: Tried to resume but checkpoint filepath ./visualbert_0801_4/best.ckpt is not present. Trying current.ckpt, otherwise skipping.\n",
            "  ckpt_filepath, reverse_suffix\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:01:15 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:171: UserWarning: Tried to resume but checkpoint filepath ./visualbert_0801_4/best.ckpt is not present. Trying current.ckpt, otherwise skipping.\n",
            "  ckpt_filepath, reverse_suffix\n",
            "\n",
            "\u001b[32m2022-08-01T05:01:15 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-01T05:01:15 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-01T05:01:16 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-08-01T05:01:16 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-01T05:01:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:02:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:02:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:02:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.6653, val/total_loss: 0.6653, val/misogyny_memes/accuracy: 0.5120, val/misogyny_memes/binary_f1: 0.6772, val/misogyny_memes/roc_auc: 0.7237, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 33s 409ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.723709\n",
            "\u001b[32m2022-08-01T05:03:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.6492, train/misogyny_memes/cross_entropy/avg: 0.6492, train/total_loss: 0.6492, train/total_loss/avg: 0.6492, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 444ms, time_since_start: 01m 52s 349ms, eta: 06m 02s 102ms\n",
            "\u001b[32m2022-08-01T05:03:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:03:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:03:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:03:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.6824, val/total_loss: 0.6824, val/misogyny_memes/accuracy: 0.5640, val/misogyny_memes/binary_f1: 0.2829, val/misogyny_memes/roc_auc: 0.7508, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 31s 106ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.750800\n",
            "\u001b[32m2022-08-01T05:04:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:04:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:04:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:04:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.5423, val/total_loss: 0.5423, val/misogyny_memes/accuracy: 0.7730, val/misogyny_memes/binary_f1: 0.7934, val/misogyny_memes/roc_auc: 0.8419, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 33s 821ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.841873\n",
            "\u001b[32m2022-08-01T05:05:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.3830, train/misogyny_memes/cross_entropy/avg: 0.5161, train/total_loss: 0.3830, train/total_loss/avg: 0.5161, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 303ms, time_since_start: 04m 15s 831ms, eta: 05m 20s 713ms\n",
            "\u001b[32m2022-08-01T05:05:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:05:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:05:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:06:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.5228, val/total_loss: 0.5228, val/misogyny_memes/accuracy: 0.7910, val/misogyny_memes/binary_f1: 0.8041, val/misogyny_memes/roc_auc: 0.8695, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 29s 309ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.869492\n",
            "\u001b[32m2022-08-01T05:06:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:06:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:06:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:07:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4338, val/total_loss: 0.4338, val/misogyny_memes/accuracy: 0.8100, val/misogyny_memes/binary_f1: 0.8187, val/misogyny_memes/roc_auc: 0.8844, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 30s 511ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.884434\n",
            "\u001b[32m2022-08-01T05:07:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.6481, train/misogyny_memes/cross_entropy/avg: 0.5601, train/total_loss: 0.6481, train/total_loss/avg: 0.5601, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 003ms, time_since_start: 06m 34s 204ms, eta: 04m 38s 483ms\n",
            "\u001b[32m2022-08-01T05:07:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:08:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:08:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:08:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4793, val/total_loss: 0.4793, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8121, val/misogyny_memes/roc_auc: 0.8855, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 31s 533ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.885471\n",
            "\u001b[32m2022-08-01T05:09:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:09:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:09:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:09:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4365, val/total_loss: 0.4365, val/misogyny_memes/accuracy: 0.8040, val/misogyny_memes/binary_f1: 0.8175, val/misogyny_memes/roc_auc: 0.8918, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 32s 147ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.891803\n",
            "\u001b[32m2022-08-01T05:10:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.3830, train/misogyny_memes/cross_entropy/avg: 0.5015, train/total_loss: 0.3830, train/total_loss/avg: 0.5015, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 783ms, time_since_start: 08m 56s 880ms, eta: 04m 03s 475ms\n",
            "\u001b[32m2022-08-01T05:10:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:10:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:10:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:10:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4452, val/total_loss: 0.4452, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8116, val/misogyny_memes/roc_auc: 0.8904, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 23s 699ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.891803\n",
            "\u001b[32m2022-08-01T05:11:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:11:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:11:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:11:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4414, val/total_loss: 0.4414, val/misogyny_memes/accuracy: 0.7910, val/misogyny_memes/binary_f1: 0.8184, val/misogyny_memes/roc_auc: 0.8962, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 34s 035ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.896170\n",
            "\u001b[32m2022-08-01T05:12:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.3830, train/misogyny_memes/cross_entropy/avg: 0.4666, train/total_loss: 0.3830, train/total_loss/avg: 0.4666, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 740ms, time_since_start: 11m 13s 861ms, eta: 03m 22s 677ms\n",
            "\u001b[32m2022-08-01T05:12:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:12:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:12:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:13:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4092, val/total_loss: 0.4092, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8108, val/misogyny_memes/roc_auc: 0.8995, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 32s 601ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.899508\n",
            "\u001b[32m2022-08-01T05:13:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:13:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:13:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:14:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4294, val/total_loss: 0.4294, val/misogyny_memes/accuracy: 0.8070, val/misogyny_memes/binary_f1: 0.8135, val/misogyny_memes/roc_auc: 0.8908, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 26s 593ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.899508\n",
            "\u001b[32m2022-08-01T05:14:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.3271, train/misogyny_memes/cross_entropy/avg: 0.4180, train/total_loss: 0.3271, train/total_loss/avg: 0.4180, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 811ms, time_since_start: 13m 32s 460ms, eta: 02m 42s 432ms\n",
            "\u001b[32m2022-08-01T05:14:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:14:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:14:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:15:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4701, val/total_loss: 0.4701, val/misogyny_memes/accuracy: 0.8090, val/misogyny_memes/binary_f1: 0.8190, val/misogyny_memes/roc_auc: 0.9019, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 32s 508ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.901946\n",
            "\u001b[32m2022-08-01T05:16:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:16:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:16:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:16:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4926, val/total_loss: 0.4926, val/misogyny_memes/accuracy: 0.8040, val/misogyny_memes/binary_f1: 0.7992, val/misogyny_memes/roc_auc: 0.9042, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 33s 302ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.904231\n",
            "\u001b[32m2022-08-01T05:17:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.3603, train/misogyny_memes/cross_entropy/avg: 0.4098, train/total_loss: 0.3603, train/total_loss/avg: 0.4098, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 729ms, time_since_start: 15m 57s 566ms, eta: 02m 01s 572ms\n",
            "\u001b[32m2022-08-01T05:17:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:17:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:17:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:17:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4297, val/total_loss: 0.4297, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8234, val/misogyny_memes/roc_auc: 0.9171, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 34s 142ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.917140\n",
            "\u001b[32m2022-08-01T05:18:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:18:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:18:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:18:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4208, val/total_loss: 0.4208, val/misogyny_memes/accuracy: 0.8300, val/misogyny_memes/binary_f1: 0.8399, val/misogyny_memes/roc_auc: 0.9124, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 26s 017ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.917140\n",
            "\u001b[32m2022-08-01T05:19:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.3271, train/misogyny_memes/cross_entropy/avg: 0.3769, train/total_loss: 0.3271, train/total_loss/avg: 0.3769, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 610ms, time_since_start: 18m 16s 908ms, eta: 01m 20s 805ms\n",
            "\u001b[32m2022-08-01T05:19:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:19:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:19:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:19:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4664, val/total_loss: 0.4664, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8204, val/misogyny_memes/roc_auc: 0.9042, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 26s 148ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.917140\n",
            "\u001b[32m2022-08-01T05:20:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:20:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:20:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:21:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4686, val/total_loss: 0.4686, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8329, val/misogyny_memes/roc_auc: 0.9099, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 23s 070ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.917140\n",
            "\u001b[32m2022-08-01T05:21:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.3271, train/misogyny_memes/cross_entropy/avg: 0.3472, train/total_loss: 0.3271, train/total_loss/avg: 0.3472, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 660ms, time_since_start: 20m 25s 367ms, eta: 40s 453ms\n",
            "\u001b[32m2022-08-01T05:21:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:21:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:21:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:22:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5897, val/total_loss: 0.5897, val/misogyny_memes/accuracy: 0.8350, val/misogyny_memes/binary_f1: 0.8361, val/misogyny_memes/roc_auc: 0.9150, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 27s 861ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.917140\n",
            "\u001b[32m2022-08-01T05:22:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:22:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:22:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:23:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.5050, val/total_loss: 0.5050, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8366, val/misogyny_memes/roc_auc: 0.9049, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 28s 162ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.917140\n",
            "\u001b[32m2022-08-01T05:23:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:23:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:23:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:24:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.3256, train/misogyny_memes/cross_entropy/avg: 0.3348, train/total_loss: 0.3256, train/total_loss/avg: 0.3348, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 1.92, time: 52s 430ms, time_since_start: 22m 53s 344ms, eta: 0ms\n",
            "\u001b[32m2022-08-01T05:24:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:24:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:24:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:24:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.5327, val/total_loss: 0.5327, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8378, val/misogyny_memes/roc_auc: 0.9117, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 23s 640ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.917140\n",
            "\u001b[32m2022-08-01T05:24:35 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-01T05:24:35 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-01T05:24:35 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-08-01T05:24:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T05:24:46 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 700\n",
            "\u001b[32m2022-08-01T05:24:46 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 700\n",
            "\u001b[32m2022-08-01T05:24:46 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-01T05:24:47 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:11<00:00,  2.83it/s]\n",
            "\u001b[32m2022-08-01T05:24:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4297, val/total_loss: 0.4297, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8234, val/misogyny_memes/roc_auc: 0.9171\n",
            "\u001b[32m2022-08-01T05:24:59 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 23m 43s 278ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "8-ClUkCQGvYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visual_bert_0801_5\n"
      ],
      "metadata": {
        "id": "yk1MolT0n-4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/visual_bert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct \\\n",
        "        training.tensorboard=True \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5e-5 \\\n",
        "        training.lr_ratio=0.7 \\\n",
        "        env.save_dir=./visualbert_0801_5 \\\n",
        "        env.tensorboard_logdir=logs/fit/visualbert_0801_5 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHfIFb8BoO_P",
        "outputId": "9105754b-e315-4a1d-dae8-ad63b025cb95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.direct\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5e-5\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.7\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./visualbert_0801_5\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/visualbert_0801_5\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf: \u001b[0mLogging to: ./visualbert_0801_5/train.log\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/misogyny_memes/defaults.yaml', 'model=visual_bert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.batch_size=32', 'optimizer.params.lr=5e-5', 'training.lr_ratio=0.7', 'env.save_dir=./visualbert_0801_5', 'env.tensorboard_logdir=logs/fit/visualbert_0801_5'])\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf_cli.run: \u001b[0mUsing seed 47596002\n",
            "\u001b[32m2022-08-01T05:30:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-01T05:30:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-01T05:30:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-01T05:30:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-01T05:30:58 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_direct.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.finetuned.hateful_memes.direct/visual_bert.finetuned.hateful_memes_direct.tar.gz ]\n",
            "Downloading visual_bert.finetuned.hateful_memes_direct.tar.gz: 100% 415M/415M [00:12<00:00, 34.4MB/s]\n",
            "[ Starting checksum for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
            "[ Checksum successful for visual_bert.finetuned.hateful_memes_direct.tar.gz]\n",
            "Unpacking visual_bert.finetuned.hateful_memes_direct.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:31:18 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:31:18 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:31:18 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:31:18 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:31:18 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
            "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
            "Unexpected keys if any: []\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:31:18 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:31:18 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[32m2022-08-01T05:31:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T05:31:18 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-01T05:31:18 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-01T05:31:18 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-01T05:31:18 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-01T05:31:18 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-01T05:31:18 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-08-01T05:31:18 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-01T05:31:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:32:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:32:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:32:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.5573, val/total_loss: 0.5573, val/misogyny_memes/accuracy: 0.7340, val/misogyny_memes/binary_f1: 0.7671, val/misogyny_memes/roc_auc: 0.8284, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 33s 468ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.828392\n",
            "\u001b[32m2022-08-01T05:33:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.3351, train/misogyny_memes/cross_entropy/avg: 0.3351, train/total_loss: 0.3351, train/total_loss/avg: 0.3351, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 838ms, time_since_start: 02m 14s 366ms, eta: 06m 05s 713ms\n",
            "\u001b[32m2022-08-01T05:33:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:33:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:33:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:33:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.4559, val/total_loss: 0.4559, val/misogyny_memes/accuracy: 0.7940, val/misogyny_memes/binary_f1: 0.8019, val/misogyny_memes/roc_auc: 0.8738, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 34s 726ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.873799\n",
            "\u001b[32m2022-08-01T05:34:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:34:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:34:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:35:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.4421, val/total_loss: 0.4421, val/misogyny_memes/accuracy: 0.8060, val/misogyny_memes/binary_f1: 0.8083, val/misogyny_memes/roc_auc: 0.8810, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 34s 395ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.881036\n",
            "\u001b[32m2022-08-01T05:35:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.3351, train/misogyny_memes/cross_entropy/avg: 0.3631, train/total_loss: 0.3351, train/total_loss/avg: 0.3631, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 438ms, time_since_start: 04m 42s 382ms, eta: 05m 21s 822ms\n",
            "\u001b[32m2022-08-01T05:35:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:35:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:35:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:36:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.4624, val/total_loss: 0.4624, val/misogyny_memes/accuracy: 0.7900, val/misogyny_memes/binary_f1: 0.7707, val/misogyny_memes/roc_auc: 0.8987, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 30s 959ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.898655\n",
            "\u001b[32m2022-08-01T05:36:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:37:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:37:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:37:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4113, val/total_loss: 0.4113, val/misogyny_memes/accuracy: 0.8080, val/misogyny_memes/binary_f1: 0.8270, val/misogyny_memes/roc_auc: 0.9053, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 31s 276ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.905256\n",
            "\u001b[32m2022-08-01T05:38:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.3351, train/misogyny_memes/cross_entropy/avg: 0.3480, train/total_loss: 0.3351, train/total_loss/avg: 0.3480, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00005, ups: 2.50, time: 40s 043ms, time_since_start: 07m 04s 052ms, eta: 04m 45s 913ms\n",
            "\u001b[32m2022-08-01T05:38:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:38:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:38:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:38:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4310, val/total_loss: 0.4310, val/misogyny_memes/accuracy: 0.8060, val/misogyny_memes/binary_f1: 0.8197, val/misogyny_memes/roc_auc: 0.9026, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 23s 990ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.905256\n",
            "\u001b[32m2022-08-01T05:39:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:39:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:39:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:39:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4117, val/total_loss: 0.4117, val/misogyny_memes/accuracy: 0.8080, val/misogyny_memes/binary_f1: 0.8140, val/misogyny_memes/roc_auc: 0.9034, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 23s 042ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.905256\n",
            "\u001b[32m2022-08-01T05:40:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.3179, train/misogyny_memes/cross_entropy/avg: 0.3100, train/total_loss: 0.3179, train/total_loss/avg: 0.3100, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 700ms, time_since_start: 09m 10s 308ms, eta: 04m 02s 969ms\n",
            "\u001b[32m2022-08-01T05:40:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:40:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:40:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:40:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4051, val/total_loss: 0.4051, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8329, val/misogyny_memes/roc_auc: 0.9028, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 24s 358ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.905256\n",
            "\u001b[32m2022-08-01T05:41:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:41:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:41:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:41:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4244, val/total_loss: 0.4244, val/misogyny_memes/accuracy: 0.8180, val/misogyny_memes/binary_f1: 0.8293, val/misogyny_memes/roc_auc: 0.9037, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 21s 382ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.905256\n",
            "\u001b[32m2022-08-01T05:42:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.3179, train/misogyny_memes/cross_entropy/avg: 0.2982, train/total_loss: 0.3179, train/total_loss/avg: 0.2982, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 675ms, time_since_start: 11m 15s 093ms, eta: 03m 22s 345ms\n",
            "\u001b[32m2022-08-01T05:42:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:42:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:42:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:42:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4420, val/total_loss: 0.4420, val/misogyny_memes/accuracy: 0.8120, val/misogyny_memes/binary_f1: 0.8246, val/misogyny_memes/roc_auc: 0.9041, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 24s 314ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.905256\n",
            "\u001b[32m2022-08-01T05:43:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:43:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:43:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:43:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4069, val/total_loss: 0.4069, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8110, val/misogyny_memes/roc_auc: 0.9044, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 24s 256ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.905256\n",
            "\u001b[32m2022-08-01T05:44:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.2713, train/misogyny_memes/cross_entropy/avg: 0.2937, train/total_loss: 0.2713, train/total_loss/avg: 0.2937, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 927ms, time_since_start: 13m 22s 899ms, eta: 02m 42s 905ms\n",
            "\u001b[32m2022-08-01T05:44:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:44:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:44:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:44:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4977, val/total_loss: 0.4977, val/misogyny_memes/accuracy: 0.8120, val/misogyny_memes/binary_f1: 0.8116, val/misogyny_memes/roc_auc: 0.9048, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 26s 082ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.905256\n",
            "\u001b[32m2022-08-01T05:45:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:45:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:45:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:45:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4076, val/total_loss: 0.4076, val/misogyny_memes/accuracy: 0.8390, val/misogyny_memes/binary_f1: 0.8450, val/misogyny_memes/roc_auc: 0.9055, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 26s 902ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.905548\n",
            "\u001b[32m2022-08-01T05:46:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.2713, train/misogyny_memes/cross_entropy/avg: 0.2772, train/total_loss: 0.2713, train/total_loss/avg: 0.2772, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 231ms, time_since_start: 15m 34s 878ms, eta: 02m 049ms\n",
            "\u001b[32m2022-08-01T05:46:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:46:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:46:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:46:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4770, val/total_loss: 0.4770, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8324, val/misogyny_memes/roc_auc: 0.9045, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 23s 191ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.905548\n",
            "\u001b[32m2022-08-01T05:47:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:47:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:47:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:48:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.5163, val/total_loss: 0.5163, val/misogyny_memes/accuracy: 0.8200, val/misogyny_memes/binary_f1: 0.8249, val/misogyny_memes/roc_auc: 0.8996, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 26s 263ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.905548\n",
            "\u001b[32m2022-08-01T05:48:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.2509, train/misogyny_memes/cross_entropy/avg: 0.2691, train/total_loss: 0.2509, train/total_loss/avg: 0.2691, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 343ms, time_since_start: 17m 43s 207ms, eta: 01m 20s 261ms\n",
            "\u001b[32m2022-08-01T05:48:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:48:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:48:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:49:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.5401, val/total_loss: 0.5401, val/misogyny_memes/accuracy: 0.8000, val/misogyny_memes/binary_f1: 0.7934, val/misogyny_memes/roc_auc: 0.8949, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 23s 904ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.905548\n",
            "\u001b[32m2022-08-01T05:49:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:49:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:49:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:50:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4841, val/total_loss: 0.4841, val/misogyny_memes/accuracy: 0.8380, val/misogyny_memes/binary_f1: 0.8448, val/misogyny_memes/roc_auc: 0.9150, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 30s 624ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.914998\n",
            "\u001b[32m2022-08-01T05:50:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.2509, train/misogyny_memes/cross_entropy/avg: 0.2422, train/total_loss: 0.2509, train/total_loss/avg: 0.2422, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 756ms, time_since_start: 19m 56s 926ms, eta: 40s 551ms\n",
            "\u001b[32m2022-08-01T05:50:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:51:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:51:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:51:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5564, val/total_loss: 0.5564, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8329, val/misogyny_memes/roc_auc: 0.9062, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 22s 715ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.914998\n",
            "\u001b[32m2022-08-01T05:51:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:52:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:52:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:52:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.6044, val/total_loss: 0.6044, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8193, val/misogyny_memes/roc_auc: 0.8964, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 23s 767ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.914998\n",
            "\u001b[32m2022-08-01T05:53:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:53:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:53:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:53:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.2122, train/misogyny_memes/cross_entropy/avg: 0.2335, train/total_loss: 0.2122, train/total_loss/avg: 0.2335, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 2.00, time: 50s 351ms, time_since_start: 22m 13s 154ms, eta: 0ms\n",
            "\u001b[32m2022-08-01T05:53:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:53:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T05:53:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T05:53:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.6541, val/total_loss: 0.6541, val/misogyny_memes/accuracy: 0.7950, val/misogyny_memes/binary_f1: 0.7817, val/misogyny_memes/roc_auc: 0.8964, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 22s 941ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.914998\n",
            "\u001b[32m2022-08-01T05:53:35 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-01T05:53:35 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-01T05:53:35 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-08-01T05:53:42 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T05:53:42 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 850\n",
            "\u001b[32m2022-08-01T05:53:42 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 850\n",
            "\u001b[32m2022-08-01T05:53:42 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
            "\u001b[32m2022-08-01T05:53:44 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:11<00:00,  2.81it/s]\n",
            "\u001b[32m2022-08-01T05:53:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4841, val/total_loss: 0.4841, val/misogyny_memes/accuracy: 0.8380, val/misogyny_memes/binary_f1: 0.8448, val/misogyny_memes/roc_auc: 0.9150\n",
            "\u001b[32m2022-08-01T05:53:55 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 22m 57s 256ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "p1i39h14oqBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visual_bert_0801_6\n"
      ],
      "metadata": {
        "id": "KVON0WS8otbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "!mmf_run config=\"projects/visual_bert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct \\\n",
        "        training.tensorboard=True \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        training.lr_ratio=0.7 \\\n",
        "        training.use_warmup=True \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./visualbert_0801_6 \\\n",
        "        env.tensorboard_logdir=logs/fit/visualbert_0801_6 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1cE8rlEpvBY",
        "outputId": "ecc617e0-8e33-4003-93bf-a695cb27e7c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.direct\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.7\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./visualbert_0801_6\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/visualbert_0801_6\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf: \u001b[0mLogging to: ./visualbert_0801_6/train.log\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/misogyny_memes/defaults.yaml', 'model=visual_bert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.direct', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'training.lr_ratio=0.7', 'training.use_warmup=True', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./visualbert_0801_6', 'env.tensorboard_logdir=logs/fit/visualbert_0801_6'])\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf_cli.run: \u001b[0mUsing seed 4581206\n",
            "\u001b[32m2022-08-01T06:13:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-01T06:13:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-01T06:13:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-01T06:13:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-01T06:13:15 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:13:19 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:13:19 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:13:19 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:13:19 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:13:19 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
            "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
            "Unexpected keys if any: []\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:13:19 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:13:19 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[32m2022-08-01T06:13:19 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T06:13:19 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-01T06:13:19 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-01T06:13:19 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-01T06:13:19 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-01T06:13:19 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-01T06:13:19 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-08-01T06:13:19 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-01T06:13:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:14:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:14:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:14:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.7312, val/total_loss: 0.7312, val/misogyny_memes/accuracy: 0.7030, val/misogyny_memes/binary_f1: 0.7671, val/misogyny_memes/roc_auc: 0.8568, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 34s 206ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.856767\n",
            "\u001b[32m2022-08-01T06:15:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.7185, train/misogyny_memes/cross_entropy/avg: 0.7185, train/total_loss: 0.7185, train/total_loss/avg: 0.7185, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 997ms, time_since_start: 01m 58s 229ms, eta: 06m 07s 176ms\n",
            "\u001b[32m2022-08-01T06:15:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:15:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:15:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:15:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.5705, val/total_loss: 0.5705, val/misogyny_memes/accuracy: 0.7520, val/misogyny_memes/binary_f1: 0.7175, val/misogyny_memes/roc_auc: 0.8711, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 32s 737ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.871137\n",
            "\u001b[32m2022-08-01T06:16:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:16:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:16:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:16:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.5545, val/total_loss: 0.5545, val/misogyny_memes/accuracy: 0.7420, val/misogyny_memes/binary_f1: 0.7214, val/misogyny_memes/roc_auc: 0.8356, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 23s 066ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.871137\n",
            "\u001b[32m2022-08-01T06:17:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.4267, train/misogyny_memes/cross_entropy/avg: 0.5726, train/total_loss: 0.4267, train/total_loss/avg: 0.5726, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 527ms, time_since_start: 04m 13s 862ms, eta: 05m 22s 542ms\n",
            "\u001b[32m2022-08-01T06:17:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:17:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:17:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:18:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.4601, val/total_loss: 0.4601, val/misogyny_memes/accuracy: 0.7960, val/misogyny_memes/binary_f1: 0.7821, val/misogyny_memes/roc_auc: 0.8877, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 31s 857ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.887712\n",
            "\u001b[32m2022-08-01T06:18:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:18:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:18:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:19:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4266, val/total_loss: 0.4266, val/misogyny_memes/accuracy: 0.7980, val/misogyny_memes/binary_f1: 0.8160, val/misogyny_memes/roc_auc: 0.8956, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 28s 886ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.895641\n",
            "\u001b[32m2022-08-01T06:19:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.4267, train/misogyny_memes/cross_entropy/avg: 0.5028, train/total_loss: 0.4267, train/total_loss/avg: 0.5028, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00005, ups: 2.50, time: 40s 264ms, time_since_start: 06m 35s 044ms, eta: 04m 47s 486ms\n",
            "\u001b[32m2022-08-01T06:19:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:20:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:20:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:20:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4791, val/total_loss: 0.4791, val/misogyny_memes/accuracy: 0.7820, val/misogyny_memes/binary_f1: 0.8130, val/misogyny_memes/roc_auc: 0.8984, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 34s 091ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.898407\n",
            "\u001b[32m2022-08-01T06:21:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:21:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:21:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:21:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4478, val/total_loss: 0.4478, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8025, val/misogyny_memes/roc_auc: 0.8916, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 26s 314ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.898407\n",
            "\u001b[32m2022-08-01T06:22:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.3631, train/misogyny_memes/cross_entropy/avg: 0.4462, train/total_loss: 0.3631, train/total_loss/avg: 0.4462, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 900ms, time_since_start: 08m 54s 953ms, eta: 04m 04s 189ms\n",
            "\u001b[32m2022-08-01T06:22:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:22:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:22:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:22:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4160, val/total_loss: 0.4160, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8347, val/misogyny_memes/roc_auc: 0.9087, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 32s 031ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.908694\n",
            "\u001b[32m2022-08-01T06:23:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:23:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:23:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:23:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4193, val/total_loss: 0.4193, val/misogyny_memes/accuracy: 0.8030, val/misogyny_memes/binary_f1: 0.8255, val/misogyny_memes/roc_auc: 0.9111, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 31s 303ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.911124\n",
            "\u001b[32m2022-08-01T06:24:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.3631, train/misogyny_memes/cross_entropy/avg: 0.3928, train/total_loss: 0.3631, train/total_loss/avg: 0.3928, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 2.50, time: 40s 034ms, time_since_start: 11m 17s 941ms, eta: 03m 24s 177ms\n",
            "\u001b[32m2022-08-01T06:24:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:24:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:24:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:25:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.5013, val/total_loss: 0.5013, val/misogyny_memes/accuracy: 0.8060, val/misogyny_memes/binary_f1: 0.8277, val/misogyny_memes/roc_auc: 0.9105, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 27s 144ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.911124\n",
            "\u001b[32m2022-08-01T06:25:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:25:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:25:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:26:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4186, val/total_loss: 0.4186, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8285, val/misogyny_memes/roc_auc: 0.9106, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 25s 856ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.911124\n",
            "\u001b[32m2022-08-01T06:26:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.2764, train/misogyny_memes/cross_entropy/avg: 0.3706, train/total_loss: 0.2764, train/total_loss/avg: 0.3706, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 712ms, time_since_start: 13m 29s 792ms, eta: 02m 42s 027ms\n",
            "\u001b[32m2022-08-01T06:26:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:26:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:26:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:27:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4736, val/total_loss: 0.4736, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8393, val/misogyny_memes/roc_auc: 0.9138, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 32s 553ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.913782\n",
            "\u001b[32m2022-08-01T06:27:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:28:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:28:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:28:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4894, val/total_loss: 0.4894, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8467, val/misogyny_memes/roc_auc: 0.9085, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 26s 701ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.913782\n",
            "\u001b[32m2022-08-01T06:29:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.3094, train/misogyny_memes/cross_entropy/avg: 0.3618, train/total_loss: 0.3094, train/total_loss/avg: 0.3618, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.50, time: 40s 027ms, time_since_start: 15m 48s 600ms, eta: 02m 02s 482ms\n",
            "\u001b[32m2022-08-01T06:29:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:29:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:29:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:29:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4509, val/total_loss: 0.4509, val/misogyny_memes/accuracy: 0.8350, val/misogyny_memes/binary_f1: 0.8421, val/misogyny_memes/roc_auc: 0.9121, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 23s 682ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.913782\n",
            "\u001b[32m2022-08-01T06:30:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:30:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:30:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:30:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.5200, val/total_loss: 0.5200, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8181, val/misogyny_memes/roc_auc: 0.9047, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 26s 408ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.913782\n",
            "\u001b[32m2022-08-01T06:31:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.2764, train/misogyny_memes/cross_entropy/avg: 0.3451, train/total_loss: 0.2764, train/total_loss/avg: 0.3451, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 651ms, time_since_start: 17m 58s 020ms, eta: 01m 20s 889ms\n",
            "\u001b[32m2022-08-01T06:31:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:31:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:31:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:31:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.5419, val/total_loss: 0.5419, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8448, val/misogyny_memes/roc_auc: 0.9184, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 31s 766ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.918366\n",
            "\u001b[32m2022-08-01T06:32:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:32:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:32:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:32:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4742, val/total_loss: 0.4742, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8366, val/misogyny_memes/roc_auc: 0.9173, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 25s 583ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.918366\n",
            "\u001b[32m2022-08-01T06:33:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.2764, train/misogyny_memes/cross_entropy/avg: 0.3097, train/total_loss: 0.2764, train/total_loss/avg: 0.3097, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 857ms, time_since_start: 20m 15s 225ms, eta: 40s 654ms\n",
            "\u001b[32m2022-08-01T06:33:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:33:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:33:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:33:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.4733, val/total_loss: 0.4733, val/misogyny_memes/accuracy: 0.8180, val/misogyny_memes/binary_f1: 0.8273, val/misogyny_memes/roc_auc: 0.9173, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 24s 154ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.918366\n",
            "\u001b[32m2022-08-01T06:34:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:34:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:34:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:35:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.5100, val/total_loss: 0.5100, val/misogyny_memes/accuracy: 0.8300, val/misogyny_memes/binary_f1: 0.8390, val/misogyny_memes/roc_auc: 0.9118, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 27s 177ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.918366\n",
            "\u001b[32m2022-08-01T06:35:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:35:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:35:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:35:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.2597, train/misogyny_memes/cross_entropy/avg: 0.2893, train/total_loss: 0.2597, train/total_loss/avg: 0.2893, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 1.92, time: 52s 460ms, time_since_start: 22m 38s 336ms, eta: 0ms\n",
            "\u001b[32m2022-08-01T06:35:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:36:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:36:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:36:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.4714, val/total_loss: 0.4714, val/misogyny_memes/accuracy: 0.8290, val/misogyny_memes/binary_f1: 0.8421, val/misogyny_memes/roc_auc: 0.9167, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 26s 716ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.918366\n",
            "\u001b[32m2022-08-01T06:36:21 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-01T06:36:21 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-01T06:36:21 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-08-01T06:36:27 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T06:36:27 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 800\n",
            "\u001b[32m2022-08-01T06:36:27 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 800\n",
            "\u001b[32m2022-08-01T06:36:27 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-01T06:36:29 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:11<00:00,  2.86it/s]\n",
            "\u001b[32m2022-08-01T06:36:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.5419, val/total_loss: 0.5419, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8448, val/misogyny_memes/roc_auc: 0.9184\n",
            "\u001b[32m2022-08-01T06:36:40 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 23m 24s 899ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "nNGgOYnrqZti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visual_bert_0801_7\n"
      ],
      "metadata": {
        "id": "xkryPPYJqfVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/visual_bert/configs/misogyny_memes/from_coco.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.pretrained.cc.full \\\n",
        "        training.tensorboard=True \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        training.lr_ratio=0.7 \\\n",
        "        training.use_warmup=True \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./visualbert_0801_7 \\\n",
        "        env.tensorboard_logdir=logs/fit/visualbert_0801_7 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4samjEu1q-Kp",
        "outputId": "54cafb2b-d52d-4fc7-8a32-ebba9a37ce69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/misogyny_memes/from_coco.yaml\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc.full\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.7\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./visualbert_0801_7\n",
            "\u001b[32m2022-08-01T06:47:37 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/visualbert_0801_7\n",
            "\u001b[32m2022-08-01T06:47:38 | mmf: \u001b[0mLogging to: ./visualbert_0801_7/train.log\n",
            "\u001b[32m2022-08-01T06:47:38 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/misogyny_memes/from_coco.yaml', 'model=visual_bert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'checkpoint.resume_zoo=visual_bert.pretrained.cc.full', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'training.lr_ratio=0.7', 'training.use_warmup=True', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./visualbert_0801_7', 'env.tensorboard_logdir=logs/fit/visualbert_0801_7'])\n",
            "\u001b[32m2022-08-01T06:47:38 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-01T06:47:38 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-01T06:47:38 | mmf_cli.run: \u001b[0mUsing seed 38077325\n",
            "\u001b[32m2022-08-01T06:47:38 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-01T06:47:42 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-01T06:47:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-01T06:47:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-01T06:47:48 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.cc_full.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.cc.full/visual_bert.pretrained.cc_full.tar.gz ]\n",
            "Downloading visual_bert.pretrained.cc_full.tar.gz: 100% 415M/415M [00:12<00:00, 32.2MB/s]\n",
            "[ Starting checksum for visual_bert.pretrained.cc_full.tar.gz]\n",
            "[ Checksum successful for visual_bert.pretrained.cc_full.tar.gz]\n",
            "Unpacking visual_bert.pretrained.cc_full.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:48:09 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:48:09 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:48:09 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:48:09 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-08-01T06:48:09 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-01T06:48:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:49:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:49:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:49:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.5060, val/total_loss: 0.5060, val/misogyny_memes/accuracy: 0.7640, val/misogyny_memes/binary_f1: 0.7626, val/misogyny_memes/roc_auc: 0.8467, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 35s 544ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.846668\n",
            "\u001b[32m2022-08-01T06:50:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.5340, train/misogyny_memes/cross_entropy/avg: 0.5340, train/total_loss: 0.5340, train/total_loss/avg: 0.5340, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00005, ups: 2.50, time: 40s 227ms, time_since_start: 02m 17s 481ms, eta: 06m 09s 291ms\n",
            "\u001b[32m2022-08-01T06:50:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:50:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:50:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:50:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.4391, val/total_loss: 0.4391, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8094, val/misogyny_memes/roc_auc: 0.8870, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 31s 702ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.886968\n",
            "\u001b[32m2022-08-01T06:51:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:51:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:51:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:51:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.4150, val/total_loss: 0.4150, val/misogyny_memes/accuracy: 0.8180, val/misogyny_memes/binary_f1: 0.8209, val/misogyny_memes/roc_auc: 0.8972, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 27s 970ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.897150\n",
            "\u001b[32m2022-08-01T06:52:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.4807, train/misogyny_memes/cross_entropy/avg: 0.5073, train/total_loss: 0.4807, train/total_loss/avg: 0.5073, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 775ms, time_since_start: 04m 36s 965ms, eta: 05m 24s 565ms\n",
            "\u001b[32m2022-08-01T06:52:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:52:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:52:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:52:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.3981, val/total_loss: 0.3981, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8129, val/misogyny_memes/roc_auc: 0.9042, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 28s 055ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.904199\n",
            "\u001b[32m2022-08-01T06:53:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:53:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:53:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:54:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.6051, val/total_loss: 0.6051, val/misogyny_memes/accuracy: 0.7050, val/misogyny_memes/binary_f1: 0.7065, val/misogyny_memes/roc_auc: 0.7634, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 26s 301ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.904199\n",
            "\u001b[32m2022-08-01T06:54:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.4807, train/misogyny_memes/cross_entropy/avg: 0.4307, train/total_loss: 0.4807, train/total_loss/avg: 0.4307, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00005, ups: 2.50, time: 40s 016ms, time_since_start: 06m 51s 139ms, eta: 04m 45s 721ms\n",
            "\u001b[32m2022-08-01T06:54:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:54:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:54:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:55:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4726, val/total_loss: 0.4726, val/misogyny_memes/accuracy: 0.8070, val/misogyny_memes/binary_f1: 0.8278, val/misogyny_memes/roc_auc: 0.9015, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 24s 802ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.904199\n",
            "\u001b[32m2022-08-01T06:55:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:55:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:55:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:56:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.3891, val/total_loss: 0.3891, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8235, val/misogyny_memes/roc_auc: 0.9075, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 35s 291ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.907473\n",
            "\u001b[32m2022-08-01T06:56:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.4807, train/misogyny_memes/cross_entropy/avg: 0.4658, train/total_loss: 0.4807, train/total_loss/avg: 0.4658, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 941ms, time_since_start: 09m 10s 629ms, eta: 04m 04s 442ms\n",
            "\u001b[32m2022-08-01T06:56:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:57:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:57:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:57:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4404, val/total_loss: 0.4404, val/misogyny_memes/accuracy: 0.8330, val/misogyny_memes/binary_f1: 0.8380, val/misogyny_memes/roc_auc: 0.9063, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 26s 376ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.907473\n",
            "\u001b[32m2022-08-01T06:58:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:58:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:58:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:58:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.3962, val/total_loss: 0.3962, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8378, val/misogyny_memes/roc_auc: 0.9111, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 33s 348ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.911104\n",
            "\u001b[32m2022-08-01T06:59:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.4807, train/misogyny_memes/cross_entropy/avg: 0.4493, train/total_loss: 0.4807, train/total_loss/avg: 0.4493, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 457ms, time_since_start: 11m 29s 275ms, eta: 03m 21s 235ms\n",
            "\u001b[32m2022-08-01T06:59:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:59:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T06:59:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T06:59:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.3846, val/total_loss: 0.3846, val/misogyny_memes/accuracy: 0.8260, val/misogyny_memes/binary_f1: 0.8270, val/misogyny_memes/roc_auc: 0.9172, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 31s 546ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.917236\n",
            "\u001b[32m2022-08-01T07:00:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:00:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:00:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:01:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4062, val/total_loss: 0.4062, val/misogyny_memes/accuracy: 0.8390, val/misogyny_memes/binary_f1: 0.8500, val/misogyny_memes/roc_auc: 0.9216, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 32s 045ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.921603\n",
            "\u001b[32m2022-08-01T07:01:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.3834, train/misogyny_memes/cross_entropy/avg: 0.4003, train/total_loss: 0.3834, train/total_loss/avg: 0.4003, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 373ms, time_since_start: 13m 51s 410ms, eta: 02m 40s 645ms\n",
            "\u001b[32m2022-08-01T07:01:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:01:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:01:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:02:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.5114, val/total_loss: 0.5114, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8505, val/misogyny_memes/roc_auc: 0.9118, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 25s 405ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.921603\n",
            "\u001b[32m2022-08-01T07:02:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:02:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:02:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:03:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4455, val/total_loss: 0.4455, val/misogyny_memes/accuracy: 0.8430, val/misogyny_memes/binary_f1: 0.8465, val/misogyny_memes/roc_auc: 0.9213, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 25s 525ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.921603\n",
            "\u001b[32m2022-08-01T07:03:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.4474, train/misogyny_memes/cross_entropy/avg: 0.4071, train/total_loss: 0.4474, train/total_loss/avg: 0.4071, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 347ms, time_since_start: 16m 01s 302ms, eta: 02m 403ms\n",
            "\u001b[32m2022-08-01T07:03:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:04:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:04:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:04:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4717, val/total_loss: 0.4717, val/misogyny_memes/accuracy: 0.8430, val/misogyny_memes/binary_f1: 0.8517, val/misogyny_memes/roc_auc: 0.9232, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 31s 787ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.923160\n",
            "\u001b[32m2022-08-01T07:05:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:05:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:05:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:05:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4677, val/total_loss: 0.4677, val/misogyny_memes/accuracy: 0.8340, val/misogyny_memes/binary_f1: 0.8469, val/misogyny_memes/roc_auc: 0.9150, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 28s 339ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.923160\n",
            "\u001b[32m2022-08-01T07:06:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.3834, train/misogyny_memes/cross_entropy/avg: 0.3782, train/total_loss: 0.3834, train/total_loss/avg: 0.3782, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 235ms, time_since_start: 18m 19s 964ms, eta: 01m 20s 041ms\n",
            "\u001b[32m2022-08-01T07:06:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:06:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:06:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:06:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4950, val/total_loss: 0.4950, val/misogyny_memes/accuracy: 0.8390, val/misogyny_memes/binary_f1: 0.8429, val/misogyny_memes/roc_auc: 0.9114, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 24s 223ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.923160\n",
            "\u001b[32m2022-08-01T07:07:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:07:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:07:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:07:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.5890, val/total_loss: 0.5890, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8092, val/misogyny_memes/roc_auc: 0.9076, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 24s 277ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.923160\n",
            "\u001b[32m2022-08-01T07:08:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.3834, train/misogyny_memes/cross_entropy/avg: 0.3372, train/total_loss: 0.3834, train/total_loss/avg: 0.3372, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 667ms, time_since_start: 20m 27s 361ms, eta: 40s 460ms\n",
            "\u001b[32m2022-08-01T07:08:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:08:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:08:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:08:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.6246, val/total_loss: 0.6246, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8369, val/misogyny_memes/roc_auc: 0.9077, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 28s 372ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.923160\n",
            "\u001b[32m2022-08-01T07:09:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:09:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:09:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:09:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.6104, val/total_loss: 0.6104, val/misogyny_memes/accuracy: 0.8470, val/misogyny_memes/binary_f1: 0.8487, val/misogyny_memes/roc_auc: 0.9156, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 25s 044ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.923160\n",
            "\u001b[32m2022-08-01T07:10:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:10:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:10:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:10:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.2775, train/misogyny_memes/cross_entropy/avg: 0.3133, train/total_loss: 0.2775, train/total_loss/avg: 0.3133, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 1.92, time: 52s 778ms, time_since_start: 22m 53s 398ms, eta: 0ms\n",
            "\u001b[32m2022-08-01T07:10:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:10:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:10:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:11:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.5367, val/total_loss: 0.5367, val/misogyny_memes/accuracy: 0.8300, val/misogyny_memes/binary_f1: 0.8372, val/misogyny_memes/roc_auc: 0.9065, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 22s 102ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.923160\n",
            "\u001b[32m2022-08-01T07:11:04 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-01T07:11:04 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-01T07:11:04 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-08-01T07:11:14 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T07:11:14 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 700\n",
            "\u001b[32m2022-08-01T07:11:14 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 700\n",
            "\u001b[32m2022-08-01T07:11:14 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-01T07:11:16 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:11<00:00,  2.89it/s]\n",
            "\u001b[32m2022-08-01T07:11:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4717, val/total_loss: 0.4717, val/misogyny_memes/accuracy: 0.8430, val/misogyny_memes/binary_f1: 0.8517, val/misogyny_memes/roc_auc: 0.9232\n",
            "\u001b[32m2022-08-01T07:11:27 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 23m 38s 210ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "q0VCv3GHrddt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visual_bert_0801_8\n"
      ],
      "metadata": {
        "id": "AW9ag_8lr99E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "!mmf_run config=\"projects/visual_bert/configs/misogyny_memes/from_coco.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco \\\n",
        "        training.tensorboard=True \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        training.lr_ratio=0.7 \\\n",
        "        training.use_warmup=True \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./visualbert_0801_8 \\\n",
        "        env.tensorboard_logdir=logs/fit/visualbert_0801_8 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OquLK7i7rrJW",
        "outputId": "8ff439a0-8de3-4305-cbc8-2e0abaa43968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/misogyny_memes/from_coco.yaml\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.7\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./visualbert_0801_8\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/visualbert_0801_8\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf: \u001b[0mLogging to: ./visualbert_0801_8/train.log\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/misogyny_memes/from_coco.yaml', 'model=visual_bert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'training.lr_ratio=0.7', 'training.use_warmup=True', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./visualbert_0801_8', 'env.tensorboard_logdir=logs/fit/visualbert_0801_8'])\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf_cli.run: \u001b[0mUsing seed 54836535\n",
            "\u001b[32m2022-08-01T07:19:54 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-01T07:19:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-01T07:20:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-01T07:20:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-01T07:20:05 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.finetuned.hateful_memes_from_coco.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.finetuned.hateful_memes.from_coco/visual_bert.finetuned.hateful_memes_from_coco.tar.gz ]\n",
            "Downloading visual_bert.finetuned.hateful_memes_from_coco.tar.gz: 100% 414M/414M [00:12<00:00, 32.8MB/s]\n",
            "[ Starting checksum for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
            "[ Checksum successful for visual_bert.finetuned.hateful_memes_from_coco.tar.gz]\n",
            "Unpacking visual_bert.finetuned.hateful_memes_from_coco.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:20:26 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:20:26 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:20:26 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:20:26 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-08-01T07:20:26 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-01T07:21:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:21:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:21:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:21:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.5090, val/total_loss: 0.5090, val/misogyny_memes/accuracy: 0.7680, val/misogyny_memes/binary_f1: 0.7734, val/misogyny_memes/roc_auc: 0.8408, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 31s 380ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.840756\n",
            "\u001b[32m2022-08-01T07:22:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.5781, train/misogyny_memes/cross_entropy/avg: 0.5781, train/total_loss: 0.5781, train/total_loss/avg: 0.5781, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00005, ups: 2.56, time: 39s 339ms, time_since_start: 02m 11s 897ms, eta: 06m 01s 136ms\n",
            "\u001b[32m2022-08-01T07:22:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:22:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:22:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:22:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.4867, val/total_loss: 0.4867, val/misogyny_memes/accuracy: 0.7580, val/misogyny_memes/binary_f1: 0.7896, val/misogyny_memes/roc_auc: 0.8658, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 34s 204ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.865756\n",
            "\u001b[32m2022-08-01T07:23:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:23:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:23:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:24:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.4448, val/total_loss: 0.4448, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8071, val/misogyny_memes/roc_auc: 0.8917, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 34s 384ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.891683\n",
            "\u001b[32m2022-08-01T07:24:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.5781, train/misogyny_memes/cross_entropy/avg: 0.5845, train/total_loss: 0.5781, train/total_loss/avg: 0.5845, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00005, ups: 1.96, time: 51s 945ms, time_since_start: 04m 51s 889ms, eta: 07m 03s 878ms\n",
            "\u001b[32m2022-08-01T07:24:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:25:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:25:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:25:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.4105, val/total_loss: 0.4105, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8279, val/misogyny_memes/roc_auc: 0.8983, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 28s 984ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.898315\n",
            "\u001b[32m2022-08-01T07:27:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:27:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:27:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:27:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4219, val/total_loss: 0.4219, val/misogyny_memes/accuracy: 0.8100, val/misogyny_memes/binary_f1: 0.8013, val/misogyny_memes/roc_auc: 0.8997, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 35s 168ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.899676\n",
            "\u001b[32m2022-08-01T07:29:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.5781, train/misogyny_memes/cross_entropy/avg: 0.4957, train/total_loss: 0.5781, train/total_loss/avg: 0.4957, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00005, ups: 0.88, time: 01m 53s 223ms, time_since_start: 09m 26s 762ms, eta: 13m 28s 416ms\n",
            "\u001b[32m2022-08-01T07:29:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:29:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:29:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:30:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4110, val/total_loss: 0.4110, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8317, val/misogyny_memes/roc_auc: 0.9071, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 34s 224ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.907101\n",
            "\u001b[32m2022-08-01T07:30:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:30:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:30:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:31:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4400, val/total_loss: 0.4400, val/misogyny_memes/accuracy: 0.8060, val/misogyny_memes/binary_f1: 0.8289, val/misogyny_memes/roc_auc: 0.9069, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 24s 207ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.907101\n",
            "\u001b[32m2022-08-01T07:31:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.3182, train/misogyny_memes/cross_entropy/avg: 0.4239, train/total_loss: 0.3182, train/total_loss/avg: 0.4239, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00005, ups: 2.63, time: 38s 493ms, time_since_start: 11m 42s 982ms, eta: 03m 55s 580ms\n",
            "\u001b[32m2022-08-01T07:31:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:31:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:31:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:32:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4021, val/total_loss: 0.4021, val/misogyny_memes/accuracy: 0.8390, val/misogyny_memes/binary_f1: 0.8435, val/misogyny_memes/roc_auc: 0.9050, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 21s 719ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.907101\n",
            "\u001b[32m2022-08-01T07:33:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:33:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:33:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:33:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4140, val/total_loss: 0.4140, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8410, val/misogyny_memes/roc_auc: 0.9097, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 27s 640ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.909651\n",
            "\u001b[32m2022-08-01T07:34:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.5080, train/misogyny_memes/cross_entropy/avg: 0.4407, train/total_loss: 0.5080, train/total_loss/avg: 0.4407, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 1.75, time: 57s 237ms, time_since_start: 14m 39s 773ms, eta: 04m 51s 909ms\n",
            "\u001b[32m2022-08-01T07:34:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:34:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:34:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:35:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.3824, val/total_loss: 0.3824, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8433, val/misogyny_memes/roc_auc: 0.9133, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 29s 323ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.913349\n",
            "\u001b[32m2022-08-01T07:36:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:36:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:36:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:37:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4081, val/total_loss: 0.4081, val/misogyny_memes/accuracy: 0.8330, val/misogyny_memes/binary_f1: 0.8328, val/misogyny_memes/roc_auc: 0.9151, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 32s 999ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.915083\n",
            "\u001b[32m2022-08-01T07:38:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.3182, train/misogyny_memes/cross_entropy/avg: 0.3965, train/total_loss: 0.3182, train/total_loss/avg: 0.3965, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 1.56, time: 01m 04s 048ms, time_since_start: 18m 17s 922ms, eta: 04m 21s 316ms\n",
            "\u001b[32m2022-08-01T07:38:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:38:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:38:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:38:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.5208, val/total_loss: 0.5208, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8219, val/misogyny_memes/roc_auc: 0.9008, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 22s 762ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.915083\n",
            "\u001b[32m2022-08-01T07:39:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:39:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:39:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:39:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.5396, val/total_loss: 0.5396, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8381, val/misogyny_memes/roc_auc: 0.8981, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 28s 938ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.915083\n",
            "\u001b[32m2022-08-01T07:40:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.3182, train/misogyny_memes/cross_entropy/avg: 0.3627, train/total_loss: 0.3182, train/total_loss/avg: 0.3627, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.22, time: 45s 120ms, time_since_start: 20m 33s 654ms, eta: 02m 18s 068ms\n",
            "\u001b[32m2022-08-01T07:40:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:40:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:40:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:41:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4600, val/total_loss: 0.4600, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8360, val/misogyny_memes/roc_auc: 0.9076, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 23s 829ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.915083\n",
            "\u001b[32m2022-08-01T07:41:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:41:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:41:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:42:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.5029, val/total_loss: 0.5029, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8412, val/misogyny_memes/roc_auc: 0.9070, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 24s 257ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.915083\n",
            "\u001b[32m2022-08-01T07:43:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.2084, train/misogyny_memes/cross_entropy/avg: 0.3226, train/total_loss: 0.2084, train/total_loss/avg: 0.3226, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 0.95, time: 01m 45s 785ms, time_since_start: 23m 51s 930ms, eta: 03m 35s 803ms\n",
            "\u001b[32m2022-08-01T07:43:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:44:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:44:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:44:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.5171, val/total_loss: 0.5171, val/misogyny_memes/accuracy: 0.8310, val/misogyny_memes/binary_f1: 0.8440, val/misogyny_memes/roc_auc: 0.9132, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 24s 583ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.915083\n",
            "\u001b[32m2022-08-01T07:46:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:46:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:46:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:46:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.5469, val/total_loss: 0.5469, val/misogyny_memes/accuracy: 0.8290, val/misogyny_memes/binary_f1: 0.8388, val/misogyny_memes/roc_auc: 0.9104, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 28s 850ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.915083\n",
            "\u001b[32m2022-08-01T07:47:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.2084, train/misogyny_memes/cross_entropy/avg: 0.2909, train/total_loss: 0.2084, train/total_loss/avg: 0.2909, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.17, time: 46s 261ms, time_since_start: 27m 21s 367ms, eta: 47s 186ms\n",
            "\u001b[32m2022-08-01T07:47:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:47:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:47:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:47:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.6299, val/total_loss: 0.6299, val/misogyny_memes/accuracy: 0.8240, val/misogyny_memes/binary_f1: 0.8336, val/misogyny_memes/roc_auc: 0.9012, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 31s 811ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.915083\n",
            "\u001b[32m2022-08-01T07:48:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:48:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:48:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:49:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.5788, val/total_loss: 0.5788, val/misogyny_memes/accuracy: 0.8420, val/misogyny_memes/binary_f1: 0.8457, val/misogyny_memes/roc_auc: 0.9136, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 24s 349ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.915083\n",
            "\u001b[32m2022-08-01T07:49:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:49:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:49:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:49:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.1756, train/misogyny_memes/cross_entropy/avg: 0.2633, train/total_loss: 0.1756, train/total_loss/avg: 0.2633, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 2.00, time: 50s 486ms, time_since_start: 29m 47s 532ms, eta: 0ms\n",
            "\u001b[32m2022-08-01T07:49:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:50:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-01T07:50:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-01T07:50:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.5829, val/total_loss: 0.5829, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8386, val/misogyny_memes/roc_auc: 0.9153, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 41s 645ms, best_update: 1000, best_iteration: 1000, best_val/misogyny_memes/roc_auc: 0.915347\n",
            "\u001b[32m2022-08-01T07:50:35 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-01T07:50:35 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-01T07:50:35 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-08-01T07:50:37 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-01T07:50:37 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1000\n",
            "\u001b[32m2022-08-01T07:50:37 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1000\n",
            "\u001b[32m2022-08-01T07:50:37 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
            "\u001b[32m2022-08-01T07:50:39 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:11<00:00,  2.81it/s]\n",
            "\u001b[32m2022-08-01T07:50:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.5829, val/total_loss: 0.5829, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8386, val/misogyny_memes/roc_auc: 0.9153\n",
            "\u001b[32m2022-08-01T07:50:50 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 30m 44s 939ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "Y0N2_ZkKr4eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unimodel-sub2: VilBert"
      ],
      "metadata": {
        "id": "IM45UN7XGeNe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vilbert_0802_2\n"
      ],
      "metadata": {
        "id": "AKskaB77sqb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViLBERT Masked Conceptual Captions\n",
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/vilbert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.6 \\\n",
        "        training.use_warmup=False \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./vilbert_0802_2 \\\n",
        "        env.tensorboard_logdir=logs/fit/vilbert_0802_2 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k35Ae9Pkm-wu",
        "outputId": "bfcf4203-23de-45a6-e304-b25fc44400ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.6\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to False\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./vilbert_0802_2\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/vilbert_0802_2\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf: \u001b[0mLogging to: ./vilbert_0802_2/train.log\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/misogyny_memes/defaults.yaml', 'model=vilbert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.lr_ratio=0.6', 'training.use_warmup=False', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'env.save_dir=./vilbert_0802_2', 'env.tensorboard_logdir=logs/fit/vilbert_0802_2'])\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf_cli.run: \u001b[0mUsing seed 2812165\n",
            "\u001b[32m2022-08-02T07:19:02 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "Downloading: 100% 433/433 [00:00<00:00, 336kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.61MB/s]\n",
            "\u001b[32m2022-08-02T07:19:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Downloading: 100% 440M/440M [00:08<00:00, 53.9MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-02T07:19:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-02T07:19:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-02T07:19:25 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-02T07:19:25 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-02T07:19:25 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-08-02T07:19:25 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-02T07:23:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:25:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:25:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:25:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:25:12 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:28:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.6801, val/total_loss: 0.6801, val/misogyny_memes/accuracy: 0.5790, val/misogyny_memes/binary_f1: 0.5682, val/misogyny_memes/roc_auc: 0.6257, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 05m 30s 661ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.625711\n",
            "\u001b[32m2022-08-02T07:31:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.6312, train/misogyny_memes/cross_entropy/avg: 0.6312, train/total_loss: 0.6312, train/total_loss/avg: 0.6312, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0., ups: 0.53, time: 03m 08s 386ms, time_since_start: 12m 17s 429ms, eta: 28m 49s 385ms\n",
            "\u001b[32m2022-08-02T07:31:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:31:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:31:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:31:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:31:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:32:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.6191, val/total_loss: 0.6191, val/misogyny_memes/accuracy: 0.6760, val/misogyny_memes/binary_f1: 0.7022, val/misogyny_memes/roc_auc: 0.7362, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 58s 856ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.736170\n",
            "\u001b[32m2022-08-02T07:35:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:35:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:35:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:35:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:35:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:36:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.6030, val/total_loss: 0.6030, val/misogyny_memes/accuracy: 0.6640, val/misogyny_memes/binary_f1: 0.6989, val/misogyny_memes/roc_auc: 0.7585, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 53s 307ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.758541\n",
            "\u001b[32m2022-08-02T07:39:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.5226, train/misogyny_memes/cross_entropy/avg: 0.5769, train/total_loss: 0.5226, train/total_loss/avg: 0.5769, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00001, ups: 0.57, time: 02m 55s 791ms, time_since_start: 20m 02s 961ms, eta: 23m 54s 460ms\n",
            "\u001b[32m2022-08-02T07:39:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:39:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:39:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:39:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:39:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:40:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.6229, val/total_loss: 0.6229, val/misogyny_memes/accuracy: 0.6610, val/misogyny_memes/binary_f1: 0.7286, val/misogyny_memes/roc_auc: 0.7824, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 54s 681ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.782405\n",
            "\u001b[32m2022-08-02T07:43:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:43:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:43:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:43:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:43:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:44:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.5265, val/total_loss: 0.5265, val/misogyny_memes/accuracy: 0.7300, val/misogyny_memes/binary_f1: 0.7348, val/misogyny_memes/roc_auc: 0.8122, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 51s 798ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.812205\n",
            "\u001b[32m2022-08-02T07:46:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.6312, train/misogyny_memes/cross_entropy/avg: 0.6226, train/total_loss: 0.6312, train/total_loss/avg: 0.6226, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00001, ups: 0.78, time: 02m 08s 300ms, time_since_start: 26m 54s 799ms, eta: 15m 16s 063ms\n",
            "\u001b[32m2022-08-02T07:46:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:46:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:46:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:46:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:46:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:47:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.5407, val/total_loss: 0.5407, val/misogyny_memes/accuracy: 0.7560, val/misogyny_memes/binary_f1: 0.7540, val/misogyny_memes/roc_auc: 0.8353, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 50s 620ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.835285\n",
            "\u001b[32m2022-08-02T07:47:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:48:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:48:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:48:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:48:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:48:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.5585, val/total_loss: 0.5585, val/misogyny_memes/accuracy: 0.7380, val/misogyny_memes/binary_f1: 0.6910, val/misogyny_memes/roc_auc: 0.8577, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 53s 446ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.857741\n",
            "\u001b[32m2022-08-02T07:49:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.5226, train/misogyny_memes/cross_entropy/avg: 0.5597, train/total_loss: 0.5226, train/total_loss/avg: 0.5597, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00001, ups: 2.08, time: 48s 859ms, time_since_start: 30m 16s 156ms, eta: 04m 59s 022ms\n",
            "\u001b[32m2022-08-02T07:49:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:49:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:49:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:49:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:49:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:50:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4442, val/total_loss: 0.4442, val/misogyny_memes/accuracy: 0.8000, val/misogyny_memes/binary_f1: 0.8127, val/misogyny_memes/roc_auc: 0.8795, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 50s 177ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.879454\n",
            "\u001b[32m2022-08-02T07:51:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:51:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:51:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:51:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:51:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:52:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.5099, val/total_loss: 0.5099, val/misogyny_memes/accuracy: 0.7480, val/misogyny_memes/binary_f1: 0.7941, val/misogyny_memes/roc_auc: 0.8822, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 57s 205ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.882156\n",
            "\u001b[32m2022-08-02T07:53:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.5226, train/misogyny_memes/cross_entropy/avg: 0.5479, train/total_loss: 0.5226, train/total_loss/avg: 0.5479, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00001, ups: 2.08, time: 48s 728ms, time_since_start: 33m 40s 921ms, eta: 04m 08s 516ms\n",
            "\u001b[32m2022-08-02T07:53:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:53:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:53:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:53:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:53:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:54:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4234, val/total_loss: 0.4234, val/misogyny_memes/accuracy: 0.8060, val/misogyny_memes/binary_f1: 0.8227, val/misogyny_memes/roc_auc: 0.8909, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 57s 605ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.890902\n",
            "\u001b[32m2022-08-02T07:54:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:55:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:55:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:55:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:55:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:55:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4595, val/total_loss: 0.4595, val/misogyny_memes/accuracy: 0.8040, val/misogyny_memes/binary_f1: 0.8275, val/misogyny_memes/roc_auc: 0.8924, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 57s 786ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.892411\n",
            "\u001b[32m2022-08-02T07:56:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.5009, train/misogyny_memes/cross_entropy/avg: 0.5236, train/total_loss: 0.5009, train/total_loss/avg: 0.5236, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00002, ups: 2.04, time: 49s 094ms, time_since_start: 37m 14s 187ms, eta: 03m 20s 304ms\n",
            "\u001b[32m2022-08-02T07:56:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:56:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:56:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:56:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:56:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:57:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4673, val/total_loss: 0.4673, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8336, val/misogyny_memes/roc_auc: 0.8984, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 57s 448ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.898435\n",
            "\u001b[32m2022-08-02T07:58:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:58:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:58:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:58:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T07:58:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T07:59:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4221, val/total_loss: 0.4221, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8264, val/misogyny_memes/roc_auc: 0.8970, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 37s 952ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.898435\n",
            "\u001b[32m2022-08-02T07:59:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.5009, train/misogyny_memes/cross_entropy/avg: 0.5059, train/total_loss: 0.5009, train/total_loss/avg: 0.5059, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00002, ups: 2.08, time: 48s 736ms, time_since_start: 40m 27s 170ms, eta: 02m 29s 132ms\n",
            "\u001b[32m2022-08-02T07:59:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:00:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:00:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:00:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:00:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:00:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4195, val/total_loss: 0.4195, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8210, val/misogyny_memes/roc_auc: 0.8985, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 52s 016ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.898535\n",
            "\u001b[32m2022-08-02T08:01:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:01:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:01:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:01:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:01:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:02:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4013, val/total_loss: 0.4013, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8316, val/misogyny_memes/roc_auc: 0.9046, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 55s 346ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.904599\n",
            "\u001b[32m2022-08-02T08:03:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.4021, train/misogyny_memes/cross_entropy/avg: 0.4705, train/total_loss: 0.4021, train/total_loss/avg: 0.4705, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00002, ups: 2.08, time: 48s 843ms, time_since_start: 43m 52s 137ms, eta: 01m 39s 640ms\n",
            "\u001b[32m2022-08-02T08:03:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:03:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:03:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:03:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:03:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:03:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4281, val/total_loss: 0.4281, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8422, val/misogyny_memes/roc_auc: 0.9000, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 40s 325ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.904599\n",
            "\u001b[32m2022-08-02T08:04:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:04:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:04:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:04:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:04:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:05:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4029, val/total_loss: 0.4029, val/misogyny_memes/accuracy: 0.8290, val/misogyny_memes/binary_f1: 0.8400, val/misogyny_memes/roc_auc: 0.9011, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 36s 758ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.904599\n",
            "\u001b[32m2022-08-02T08:06:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.4021, train/misogyny_memes/cross_entropy/avg: 0.4570, train/total_loss: 0.4021, train/total_loss/avg: 0.4570, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00002, ups: 2.08, time: 48s 639ms, time_since_start: 46m 46s 594ms, eta: 49s 612ms\n",
            "\u001b[32m2022-08-02T08:06:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:06:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:06:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:06:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:06:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:06:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5282, val/total_loss: 0.5282, val/misogyny_memes/accuracy: 0.7690, val/misogyny_memes/binary_f1: 0.7354, val/misogyny_memes/roc_auc: 0.9026, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 38s 404ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.904599\n",
            "\u001b[32m2022-08-02T08:07:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:07:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:07:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:07:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:07:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:08:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.4050, val/total_loss: 0.4050, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8360, val/misogyny_memes/roc_auc: 0.9019, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 40s 865ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.904599\n",
            "\u001b[32m2022-08-02T08:09:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:09:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:09:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:09:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:09:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:09:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.3998, train/misogyny_memes/cross_entropy/avg: 0.4509, train/total_loss: 0.3998, train/total_loss/avg: 0.4509, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00003, ups: 1.33, time: 01m 15s 510ms, time_since_start: 50m 10s 111ms, eta: 0ms\n",
            "\u001b[32m2022-08-02T08:09:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:09:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:09:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:09:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:09:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:10:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.5616, val/total_loss: 0.5616, val/misogyny_memes/accuracy: 0.8010, val/misogyny_memes/binary_f1: 0.8271, val/misogyny_memes/roc_auc: 0.8936, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 36s 643ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.904599\n",
            "\u001b[32m2022-08-02T08:10:12 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-02T08:10:12 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-02T08:10:12 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:10:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:10:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:10:34 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-02T08:10:34 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 750\n",
            "\u001b[32m2022-08-02T08:10:34 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 750\n",
            "\u001b[32m2022-08-02T08:10:34 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-02T08:10:37 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:13<00:00,  2.46it/s]\n",
            "\u001b[32m2022-08-02T08:10:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4013, val/total_loss: 0.4013, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8316, val/misogyny_memes/roc_auc: 0.9046\n",
            "\u001b[32m2022-08-02T08:10:51 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 51m 25s 954ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "import torch, gc\n",
        "import os\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "z7EOg5Sp8ZLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vilbert_0802_3"
      ],
      "metadata": {
        "id": "8xUnCYyIMFO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViLBERT Masked Conceptual Captions\n",
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/vilbert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=False \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./vilbert_0802_3 \\\n",
        "        env.tensorboard_logdir=logs/fit/vilbert_0802_3 \\"
      ],
      "metadata": {
        "id": "LSJOULRSMHK0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bbdcfd-95d0-4981-98e3-3918dc096685"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.3\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to False\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./vilbert_0802_3\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/vilbert_0802_3\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf: \u001b[0mLogging to: ./vilbert_0802_3/train.log\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/misogyny_memes/defaults.yaml', 'model=vilbert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.lr_ratio=0.3', 'training.use_warmup=False', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'env.save_dir=./vilbert_0802_3', 'env.tensorboard_logdir=logs/fit/vilbert_0802_3'])\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf_cli.run: \u001b[0mUsing seed 11288953\n",
            "\u001b[32m2022-08-02T08:32:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-02T08:32:15 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-02T08:32:23 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-02T08:32:23 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-02T08:32:23 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-02T08:32:23 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-02T08:32:23 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-08-02T08:32:23 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-02T08:33:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:33:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:33:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:33:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:33:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:34:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.7127, val/total_loss: 0.7127, val/misogyny_memes/accuracy: 0.5460, val/misogyny_memes/binary_f1: 0.6315, val/misogyny_memes/roc_auc: 0.5761, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 57s 592ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.576113\n",
            "\u001b[32m2022-08-02T08:35:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.7247, train/misogyny_memes/cross_entropy/avg: 0.7247, train/total_loss: 0.7247, train/total_loss/avg: 0.7247, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0., ups: 1.82, time: 55s 386ms, time_since_start: 02m 42s 284ms, eta: 08m 28s 452ms\n",
            "\u001b[32m2022-08-02T08:35:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:35:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:35:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:35:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:35:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:36:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.6270, val/total_loss: 0.6270, val/misogyny_memes/accuracy: 0.6570, val/misogyny_memes/binary_f1: 0.6755, val/misogyny_memes/roc_auc: 0.7063, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 01m 07s 265ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.706281\n",
            "\u001b[32m2022-08-02T08:39:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:39:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:39:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:39:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:39:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:40:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.5843, val/total_loss: 0.5843, val/misogyny_memes/accuracy: 0.6890, val/misogyny_memes/binary_f1: 0.7041, val/misogyny_memes/roc_auc: 0.7606, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 01m 01s 621ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.760582\n",
            "\u001b[32m2022-08-02T08:42:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.7247, train/misogyny_memes/cross_entropy/avg: 0.7369, train/total_loss: 0.7247, train/total_loss/avg: 0.7369, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00001, ups: 0.60, time: 02m 47s 358ms, time_since_start: 10m 26s 151ms, eta: 22m 45s 646ms\n",
            "\u001b[32m2022-08-02T08:42:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:43:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:43:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:43:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:43:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:43:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.5664, val/total_loss: 0.5664, val/misogyny_memes/accuracy: 0.7180, val/misogyny_memes/binary_f1: 0.7050, val/misogyny_memes/roc_auc: 0.7870, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 59s 049ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.787024\n",
            "\u001b[32m2022-08-02T08:46:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:46:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:46:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:46:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:46:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:47:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.5556, val/total_loss: 0.5556, val/misogyny_memes/accuracy: 0.7030, val/misogyny_memes/binary_f1: 0.7374, val/misogyny_memes/roc_auc: 0.8009, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 54s 849ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.800853\n",
            "\u001b[32m2022-08-02T08:49:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.7247, train/misogyny_memes/cross_entropy/avg: 0.6800, train/total_loss: 0.7247, train/total_loss/avg: 0.6800, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00001, ups: 0.89, time: 01m 52s 698ms, time_since_start: 17m 03s 839ms, eta: 13m 24s 666ms\n",
            "\u001b[32m2022-08-02T08:49:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:49:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:49:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:49:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:49:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:50:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.5993, val/total_loss: 0.5993, val/misogyny_memes/accuracy: 0.6860, val/misogyny_memes/binary_f1: 0.6085, val/misogyny_memes/roc_auc: 0.7985, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 38s 619ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.800853\n",
            "\u001b[32m2022-08-02T08:50:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:51:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:51:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:51:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:51:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:51:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4960, val/total_loss: 0.4960, val/misogyny_memes/accuracy: 0.7670, val/misogyny_memes/binary_f1: 0.7713, val/misogyny_memes/roc_auc: 0.8475, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 52s 452ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.847545\n",
            "\u001b[32m2022-08-02T08:52:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.5662, train/misogyny_memes/cross_entropy/avg: 0.5953, train/total_loss: 0.5662, train/total_loss/avg: 0.5953, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00001, ups: 2.04, time: 49s 155ms, time_since_start: 20m 13s 048ms, eta: 05m 832ms\n",
            "\u001b[32m2022-08-02T08:52:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:52:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:52:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:52:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:52:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:53:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4786, val/total_loss: 0.4786, val/misogyny_memes/accuracy: 0.7810, val/misogyny_memes/binary_f1: 0.8050, val/misogyny_memes/roc_auc: 0.8652, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 48s 104ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.865201\n",
            "\u001b[32m2022-08-02T08:54:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:54:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:54:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:54:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:54:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:55:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.5242, val/total_loss: 0.5242, val/misogyny_memes/accuracy: 0.7780, val/misogyny_memes/binary_f1: 0.8073, val/misogyny_memes/roc_auc: 0.8734, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 01m 368ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.873430\n",
            "\u001b[32m2022-08-02T08:56:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.5662, train/misogyny_memes/cross_entropy/avg: 0.5483, train/total_loss: 0.5662, train/total_loss/avg: 0.5483, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00001, ups: 2.04, time: 49s 111ms, time_since_start: 23m 39s 576ms, eta: 04m 10s 468ms\n",
            "\u001b[32m2022-08-02T08:56:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:56:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:56:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:56:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:56:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:56:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4400, val/total_loss: 0.4400, val/misogyny_memes/accuracy: 0.7970, val/misogyny_memes/binary_f1: 0.8143, val/misogyny_memes/roc_auc: 0.8804, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 54s 322ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.880415\n",
            "\u001b[32m2022-08-02T08:57:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:57:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:57:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:57:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:57:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T08:58:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4371, val/total_loss: 0.4371, val/misogyny_memes/accuracy: 0.8030, val/misogyny_memes/binary_f1: 0.8143, val/misogyny_memes/roc_auc: 0.8801, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 38s 220ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.880415\n",
            "\u001b[32m2022-08-02T08:59:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.5040, train/misogyny_memes/cross_entropy/avg: 0.5410, train/total_loss: 0.5040, train/total_loss/avg: 0.5410, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00002, ups: 2.04, time: 49s 389ms, time_since_start: 26m 50s 803ms, eta: 03m 21s 507ms\n",
            "\u001b[32m2022-08-02T08:59:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:59:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:59:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:59:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T08:59:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:00:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4132, val/total_loss: 0.4132, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8238, val/misogyny_memes/roc_auc: 0.8946, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 56s 809ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.894633\n",
            "\u001b[32m2022-08-02T09:01:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:01:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:01:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:01:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:01:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:01:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4305, val/total_loss: 0.4305, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8228, val/misogyny_memes/roc_auc: 0.8895, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 44s 529ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.894633\n",
            "\u001b[32m2022-08-02T09:02:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.5040, train/misogyny_memes/cross_entropy/avg: 0.5208, train/total_loss: 0.5040, train/total_loss/avg: 0.5208, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00002, ups: 2.08, time: 48s 627ms, time_since_start: 30m 09s 799ms, eta: 02m 28s 798ms\n",
            "\u001b[32m2022-08-02T09:02:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:02:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:02:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:02:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:02:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:03:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4229, val/total_loss: 0.4229, val/misogyny_memes/accuracy: 0.8060, val/misogyny_memes/binary_f1: 0.8262, val/misogyny_memes/roc_auc: 0.8904, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 41s 863ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.894633\n",
            "\u001b[32m2022-08-02T09:04:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:04:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:04:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:04:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:04:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:04:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4315, val/total_loss: 0.4315, val/misogyny_memes/accuracy: 0.8350, val/misogyny_memes/binary_f1: 0.8476, val/misogyny_memes/roc_auc: 0.9020, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 53s 897ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.902018\n",
            "\u001b[32m2022-08-02T09:05:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.3997, train/misogyny_memes/cross_entropy/avg: 0.4746, train/total_loss: 0.3997, train/total_loss/avg: 0.4746, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00002, ups: 2.08, time: 48s 813ms, time_since_start: 33m 23s 001ms, eta: 01m 39s 579ms\n",
            "\u001b[32m2022-08-02T09:05:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:05:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:05:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:05:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:05:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:06:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4192, val/total_loss: 0.4192, val/misogyny_memes/accuracy: 0.8260, val/misogyny_memes/binary_f1: 0.8311, val/misogyny_memes/roc_auc: 0.9048, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 01m 287ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.904792\n",
            "\u001b[32m2022-08-02T09:07:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:07:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:07:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:07:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:07:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:08:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4075, val/total_loss: 0.4075, val/misogyny_memes/accuracy: 0.8210, val/misogyny_memes/binary_f1: 0.8280, val/misogyny_memes/roc_auc: 0.9030, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 41s 719ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.904792\n",
            "\u001b[32m2022-08-02T09:09:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.3997, train/misogyny_memes/cross_entropy/avg: 0.4423, train/total_loss: 0.3997, train/total_loss/avg: 0.4423, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00002, ups: 2.08, time: 48s 458ms, time_since_start: 36m 42s 580ms, eta: 49s 427ms\n",
            "\u001b[32m2022-08-02T09:09:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:09:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:09:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:09:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:09:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:09:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5181, val/total_loss: 0.5181, val/misogyny_memes/accuracy: 0.8120, val/misogyny_memes/binary_f1: 0.8368, val/misogyny_memes/roc_auc: 0.9030, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 50s 662ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.904792\n",
            "\u001b[32m2022-08-02T09:10:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:10:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:10:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:10:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:10:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:11:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.4369, val/total_loss: 0.4369, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8297, val/misogyny_memes/roc_auc: 0.9053, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 01m 01s 190ms, best_update: 950, best_iteration: 950, best_val/misogyny_memes/roc_auc: 0.905292\n",
            "\u001b[32m2022-08-02T09:12:35 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:12:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:12:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:12:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:12:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:13:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.3997, train/misogyny_memes/cross_entropy/avg: 0.4465, train/total_loss: 0.3997, train/total_loss/avg: 0.4465, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00003, ups: 1.27, time: 01m 19s 005ms, time_since_start: 40m 42s 140ms, eta: 0ms\n",
            "\u001b[32m2022-08-02T09:13:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:13:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:13:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:13:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:13:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:13:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.4779, val/total_loss: 0.4779, val/misogyny_memes/accuracy: 0.7840, val/misogyny_memes/binary_f1: 0.7562, val/misogyny_memes/roc_auc: 0.8997, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 42s 168ms, best_update: 950, best_iteration: 950, best_val/misogyny_memes/roc_auc: 0.905292\n",
            "\u001b[32m2022-08-02T09:13:48 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-02T09:13:48 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-02T09:13:48 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:13:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:13:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:13:54 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-02T09:13:54 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 950\n",
            "\u001b[32m2022-08-02T09:13:54 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 950\n",
            "\u001b[32m2022-08-02T09:13:54 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
            "\u001b[32m2022-08-02T09:13:58 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:14<00:00,  2.19it/s]\n",
            "\u001b[32m2022-08-02T09:14:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.4369, val/total_loss: 0.4369, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8297, val/misogyny_memes/roc_auc: 0.9053\n",
            "\u001b[32m2022-08-02T09:14:12 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 41m 49s 174ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "SafD2rQhoRHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vilbert_0802_4\n"
      ],
      "metadata": {
        "id": "ZUZtlheJMmha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViLBERT Masked Conceptual Captions\n",
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/vilbert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        checkpoint.resume=True \\\n",
        "        checkpoint.resume_best=True \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.6 \\\n",
        "        training.use_warmup=False \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./vilbert_0802_4 \\\n",
        "        env.tensorboard_logdir=logs/fit/vilbert_0802_4 \\"
      ],
      "metadata": {
        "id": "c3SlCCMmNofk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e52ca79-cb98-4f5a-f1c3-ca483f0cecbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume to True\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_best to True\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.6\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to False\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./vilbert_0802_4\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/vilbert_0802_4\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf: \u001b[0mLogging to: ./vilbert_0802_4/train.log\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/misogyny_memes/defaults.yaml', 'model=vilbert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'checkpoint.resume=True', 'checkpoint.resume_best=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.lr_ratio=0.6', 'training.use_warmup=False', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'env.save_dir=./vilbert_0802_4', 'env.tensorboard_logdir=logs/fit/vilbert_0802_4'])\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf_cli.run: \u001b[0mUsing seed 14559096\n",
            "\u001b[32m2022-08-02T09:20:14 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-02T09:20:19 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-02T09:20:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-02T09:20:27 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:20:27 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:171: UserWarning: Tried to resume but checkpoint filepath ./vilbert_0802_4/best.ckpt is not present. Trying current.ckpt, otherwise skipping.\n",
            "  ckpt_filepath, reverse_suffix\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:20:27 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:171: UserWarning: Tried to resume but checkpoint filepath ./vilbert_0802_4/best.ckpt is not present. Trying current.ckpt, otherwise skipping.\n",
            "  ckpt_filepath, reverse_suffix\n",
            "\n",
            "\u001b[32m2022-08-02T09:20:27 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-02T09:20:27 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-02T09:20:27 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-08-02T09:20:27 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-02T09:21:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:21:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:21:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:21:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:21:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:22:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.6723, val/total_loss: 0.6723, val/misogyny_memes/accuracy: 0.5840, val/misogyny_memes/binary_f1: 0.5789, val/misogyny_memes/roc_auc: 0.6226, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 01m 338ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.622609\n",
            "\u001b[32m2022-08-02T09:23:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.8511, train/misogyny_memes/cross_entropy/avg: 0.8511, train/total_loss: 0.8511, train/total_loss/avg: 0.8511, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0., ups: 2.00, time: 50s 562ms, time_since_start: 02m 40s 653ms, eta: 07m 44s 165ms\n",
            "\u001b[32m2022-08-02T09:23:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:23:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:23:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:23:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:23:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:24:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.6098, val/total_loss: 0.6098, val/misogyny_memes/accuracy: 0.6680, val/misogyny_memes/binary_f1: 0.6376, val/misogyny_memes/roc_auc: 0.7330, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 01m 03s 289ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.732960\n",
            "\u001b[32m2022-08-02T09:26:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:26:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:26:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:26:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:26:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:27:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.5857, val/total_loss: 0.5857, val/misogyny_memes/accuracy: 0.6840, val/misogyny_memes/binary_f1: 0.7074, val/misogyny_memes/roc_auc: 0.7607, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 01m 02s 579ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.760726\n",
            "\u001b[32m2022-08-02T09:30:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.5455, train/misogyny_memes/cross_entropy/avg: 0.6983, train/total_loss: 0.5455, train/total_loss/avg: 0.6983, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00001, ups: 0.66, time: 02m 31s 648ms, time_since_start: 09m 49s 317ms, eta: 20m 37s 448ms\n",
            "\u001b[32m2022-08-02T09:30:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:30:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:30:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:30:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:30:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:31:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.6626, val/total_loss: 0.6626, val/misogyny_memes/accuracy: 0.6360, val/misogyny_memes/binary_f1: 0.7209, val/misogyny_memes/roc_auc: 0.7746, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 01m 05s 218ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.774588\n",
            "\u001b[32m2022-08-02T09:33:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:34:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:34:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:34:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:34:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:34:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.5629, val/total_loss: 0.5629, val/misogyny_memes/accuracy: 0.7210, val/misogyny_memes/binary_f1: 0.7475, val/misogyny_memes/roc_auc: 0.7974, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 59s 146ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.797415\n",
            "\u001b[32m2022-08-02T09:36:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.5888, train/misogyny_memes/cross_entropy/avg: 0.6618, train/total_loss: 0.5888, train/total_loss/avg: 0.6618, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00001, ups: 0.88, time: 01m 54s 271ms, time_since_start: 16m 26s 188ms, eta: 13m 35s 896ms\n",
            "\u001b[32m2022-08-02T09:36:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:37:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:37:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:37:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:37:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:37:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.5441, val/total_loss: 0.5441, val/misogyny_memes/accuracy: 0.7170, val/misogyny_memes/binary_f1: 0.7511, val/misogyny_memes/roc_auc: 0.8216, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 56s 739ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.821583\n",
            "\u001b[32m2022-08-02T09:38:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:38:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:38:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:38:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:38:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:39:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4866, val/total_loss: 0.4866, val/misogyny_memes/accuracy: 0.7680, val/misogyny_memes/binary_f1: 0.7811, val/misogyny_memes/roc_auc: 0.8546, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 51s 743ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.854642\n",
            "\u001b[32m2022-08-02T09:40:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.5455, train/misogyny_memes/cross_entropy/avg: 0.6135, train/total_loss: 0.5455, train/total_loss/avg: 0.6135, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00001, ups: 2.04, time: 49s 623ms, time_since_start: 19m 54s 033ms, eta: 05m 03s 696ms\n",
            "\u001b[32m2022-08-02T09:40:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:40:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:40:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:40:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:40:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:41:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4589, val/total_loss: 0.4589, val/misogyny_memes/accuracy: 0.7720, val/misogyny_memes/binary_f1: 0.7849, val/misogyny_memes/roc_auc: 0.8683, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 56s 451ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.868339\n",
            "\u001b[32m2022-08-02T09:42:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:42:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:42:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:42:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:42:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:43:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4427, val/total_loss: 0.4427, val/misogyny_memes/accuracy: 0.7990, val/misogyny_memes/binary_f1: 0.8158, val/misogyny_memes/roc_auc: 0.8805, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 56s 919ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.880455\n",
            "\u001b[32m2022-08-02T09:43:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.5455, train/misogyny_memes/cross_entropy/avg: 0.5645, train/total_loss: 0.5455, train/total_loss/avg: 0.5645, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00001, ups: 2.04, time: 49s 672ms, time_since_start: 23m 26s 609ms, eta: 04m 13s 331ms\n",
            "\u001b[32m2022-08-02T09:43:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:44:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:44:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:44:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:44:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:44:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4675, val/total_loss: 0.4675, val/misogyny_memes/accuracy: 0.7920, val/misogyny_memes/binary_f1: 0.8204, val/misogyny_memes/roc_auc: 0.8788, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 40s 890ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.880455\n",
            "\u001b[32m2022-08-02T09:45:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:45:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:45:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:45:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:45:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:46:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.5030, val/total_loss: 0.5030, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8204, val/misogyny_memes/roc_auc: 0.8894, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 52s 543ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.889441\n",
            "\u001b[32m2022-08-02T09:47:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.4685, train/misogyny_memes/cross_entropy/avg: 0.5443, train/total_loss: 0.4685, train/total_loss/avg: 0.5443, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00002, ups: 2.04, time: 49s 672ms, time_since_start: 26m 39s 158ms, eta: 03m 22s 664ms\n",
            "\u001b[32m2022-08-02T09:47:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:47:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:47:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:47:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:47:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:48:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4220, val/total_loss: 0.4220, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8348, val/misogyny_memes/roc_auc: 0.8938, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 01m 04s 866ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.893848\n",
            "\u001b[32m2022-08-02T09:49:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:49:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:49:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:49:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:49:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:49:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4402, val/total_loss: 0.4402, val/misogyny_memes/accuracy: 0.8080, val/misogyny_memes/binary_f1: 0.8261, val/misogyny_memes/roc_auc: 0.9009, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 57s 733ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.900933\n",
            "\u001b[32m2022-08-02T09:50:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.4685, train/misogyny_memes/cross_entropy/avg: 0.4984, train/total_loss: 0.4685, train/total_loss/avg: 0.4984, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00002, ups: 2.04, time: 49s 139ms, time_since_start: 30m 20s 480ms, eta: 02m 30s 365ms\n",
            "\u001b[32m2022-08-02T09:50:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:51:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:51:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:51:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:51:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T09:51:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4533, val/total_loss: 0.4533, val/misogyny_memes/accuracy: 0.7950, val/misogyny_memes/binary_f1: 0.8243, val/misogyny_memes/roc_auc: 0.8981, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 42s 634ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.900933\n",
            "\u001b[32m2022-08-02T09:52:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:52:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:52:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:52:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T09:52:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "9gadV5ByNp__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vilbert_0802_5\n"
      ],
      "metadata": {
        "id": "nNGPVuUuMzv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViLBERT Masked Conceptual Captions\n",
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/vilbert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.6 \\\n",
        "        training.use_warmup=False \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./vilbert_0802_5 \\\n",
        "        env.tensorboard_logdir=logs/fit/vilbert_0802_5 \\"
      ],
      "metadata": {
        "id": "RQ1vX76OOHcu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddcbc2e9-c5f5-4769-fd12-315a6047aad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.direct\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.6\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to False\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./vilbert_0802_5\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/vilbert_0802_5\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf: \u001b[0mLogging to: ./vilbert_0802_5/train.log\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/misogyny_memes/defaults.yaml', 'model=vilbert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.lr_ratio=0.6', 'training.use_warmup=False', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'env.save_dir=./vilbert_0802_5', 'env.tensorboard_logdir=logs/fit/vilbert_0802_5'])\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf_cli.run: \u001b[0mUsing seed 3919382\n",
            "\u001b[32m2022-08-02T14:12:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-02T14:12:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-02T14:12:14 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-02T14:12:14 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-02T14:12:14 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_direct.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.direct/vilbert.finetuned.hateful_memes_direct.tar.gz ]\n",
            "Downloading vilbert.finetuned.hateful_memes_direct.tar.gz: 100% 918M/918M [00:51<00:00, 18.0MB/s]\n",
            "[ Starting checksum for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
            "[ Checksum successful for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
            "Unpacking vilbert.finetuned.hateful_memes_direct.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:13:17 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:13:17 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:13:17 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:13:17 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:13:18 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
            "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
            "Unexpected keys if any: []\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:13:18 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:13:18 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:13:18 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:13:18 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[32m2022-08-02T14:13:18 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-02T14:13:18 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-02T14:13:18 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-02T14:13:18 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-02T14:13:18 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-02T14:13:18 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-02T14:13:18 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-08-02T14:13:18 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-02T14:20:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:24:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:24:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:24:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:24:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T14:29:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 1.9602, val/total_loss: 1.9602, val/misogyny_memes/accuracy: 0.6240, val/misogyny_memes/binary_f1: 0.6148, val/misogyny_memes/roc_auc: 0.6636, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 09m 807ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.663632\n",
            "\u001b[32m2022-08-02T14:35:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.8801, train/misogyny_memes/cross_entropy/avg: 0.8801, train/total_loss: 0.8801, train/total_loss/avg: 0.8801, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0., ups: 0.28, time: 05m 54s 702ms, time_since_start: 22m 50s 278ms, eta: 54m 16s 169ms\n",
            "\u001b[32m2022-08-02T14:35:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:35:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:35:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:35:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:35:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T14:35:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 1.0991, val/total_loss: 1.0991, val/misogyny_memes/accuracy: 0.6690, val/misogyny_memes/binary_f1: 0.6612, val/misogyny_memes/roc_auc: 0.7027, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 53s 693ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.702715\n",
            "\u001b[32m2022-08-02T14:41:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:41:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:41:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:41:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:41:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T14:42:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.7406, val/total_loss: 0.7406, val/misogyny_memes/accuracy: 0.7040, val/misogyny_memes/binary_f1: 0.6910, val/misogyny_memes/roc_auc: 0.7660, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 52s 908ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.765970\n",
            "\u001b[32m2022-08-02T14:48:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.5262, train/misogyny_memes/cross_entropy/avg: 0.7031, train/total_loss: 0.5262, train/total_loss/avg: 0.7031, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00001, ups: 0.29, time: 05m 47s 936ms, time_since_start: 36m 10s 801ms, eta: 47m 19s 161ms\n",
            "\u001b[32m2022-08-02T14:48:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:48:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:48:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:48:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:48:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T14:49:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.5525, val/total_loss: 0.5525, val/misogyny_memes/accuracy: 0.7330, val/misogyny_memes/binary_f1: 0.7349, val/misogyny_memes/roc_auc: 0.7998, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 53s 704ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.799841\n",
            "\u001b[32m2022-08-02T14:55:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:55:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:55:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:55:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:55:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T14:55:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.5331, val/total_loss: 0.5331, val/misogyny_memes/accuracy: 0.7490, val/misogyny_memes/binary_f1: 0.7669, val/misogyny_memes/roc_auc: 0.8210, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 55s 258ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.821039\n",
            "\u001b[32m2022-08-02T14:59:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.5262, train/misogyny_memes/cross_entropy/avg: 0.6428, train/total_loss: 0.5262, train/total_loss/avg: 0.6428, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00001, ups: 0.44, time: 03m 46s 721ms, time_since_start: 47m 31s 454ms, eta: 26m 58s 794ms\n",
            "\u001b[32m2022-08-02T14:59:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:59:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:59:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:59:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T14:59:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:00:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.5067, val/total_loss: 0.5067, val/misogyny_memes/accuracy: 0.7420, val/misogyny_memes/binary_f1: 0.7329, val/misogyny_memes/roc_auc: 0.8349, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 51s 858ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.834916\n",
            "\u001b[32m2022-08-02T15:01:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:01:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:01:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:01:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:01:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:02:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.5061, val/total_loss: 0.5061, val/misogyny_memes/accuracy: 0.7680, val/misogyny_memes/binary_f1: 0.7603, val/misogyny_memes/roc_auc: 0.8464, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 51s 420ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.846408\n",
            "\u001b[32m2022-08-02T15:03:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.5262, train/misogyny_memes/cross_entropy/avg: 0.6419, train/total_loss: 0.5262, train/total_loss/avg: 0.6419, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00001, ups: 2.17, time: 46s 009ms, time_since_start: 50m 49s 674ms, eta: 04m 41s 577ms\n",
            "\u001b[32m2022-08-02T15:03:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:03:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:03:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:03:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:03:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:03:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4842, val/total_loss: 0.4842, val/misogyny_memes/accuracy: 0.7600, val/misogyny_memes/binary_f1: 0.7782, val/misogyny_memes/roc_auc: 0.8527, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 51s 766ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.852736\n",
            "\u001b[32m2022-08-02T15:04:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:04:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:04:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:04:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:04:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:05:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4951, val/total_loss: 0.4951, val/misogyny_memes/accuracy: 0.7640, val/misogyny_memes/binary_f1: 0.7904, val/misogyny_memes/roc_auc: 0.8552, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 51s 961ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.855174\n",
            "\u001b[32m2022-08-02T15:06:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.5654, train/misogyny_memes/cross_entropy/avg: 0.6266, train/total_loss: 0.5654, train/total_loss/avg: 0.6266, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00001, ups: 2.17, time: 46s 555ms, time_since_start: 54m 09s 584ms, eta: 03m 57s 430ms\n",
            "\u001b[32m2022-08-02T15:06:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:06:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:06:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:06:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:06:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:07:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.5060, val/total_loss: 0.5060, val/misogyny_memes/accuracy: 0.7720, val/misogyny_memes/binary_f1: 0.8031, val/misogyny_memes/roc_auc: 0.8741, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 55s 572ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.874143\n",
            "\u001b[32m2022-08-02T15:08:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:08:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:08:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:08:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:08:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:09:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4429, val/total_loss: 0.4429, val/misogyny_memes/accuracy: 0.7960, val/misogyny_memes/binary_f1: 0.8086, val/misogyny_memes/roc_auc: 0.8760, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 55s 010ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.876032\n",
            "\u001b[32m2022-08-02T15:09:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.5262, train/misogyny_memes/cross_entropy/avg: 0.5877, train/total_loss: 0.5262, train/total_loss/avg: 0.5877, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00002, ups: 2.13, time: 47s 481ms, time_since_start: 57m 34s 234ms, eta: 03m 13s 725ms\n",
            "\u001b[32m2022-08-02T15:09:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:10:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:10:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:10:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:10:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:10:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4607, val/total_loss: 0.4607, val/misogyny_memes/accuracy: 0.7980, val/misogyny_memes/binary_f1: 0.8091, val/misogyny_memes/roc_auc: 0.8765, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 53s 539ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.876513\n",
            "\u001b[32m2022-08-02T15:11:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:11:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:11:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:11:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:11:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:12:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4931, val/total_loss: 0.4931, val/misogyny_memes/accuracy: 0.7930, val/misogyny_memes/binary_f1: 0.8106, val/misogyny_memes/roc_auc: 0.8766, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 55s 405ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.876649\n",
            "\u001b[32m2022-08-02T15:13:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.5262, train/misogyny_memes/cross_entropy/avg: 0.5664, train/total_loss: 0.5262, train/total_loss/avg: 0.5664, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00002, ups: 2.17, time: 46s 922ms, time_since_start: 01h 57s 443ms, eta: 02m 23s 582ms\n",
            "\u001b[32m2022-08-02T15:13:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:13:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:13:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:13:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:13:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:14:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4851, val/total_loss: 0.4851, val/misogyny_memes/accuracy: 0.7910, val/misogyny_memes/binary_f1: 0.7825, val/misogyny_memes/roc_auc: 0.8823, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 56s 848ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.882300\n",
            "\u001b[32m2022-08-02T15:14:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:15:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:15:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:15:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:15:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:15:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4505, val/total_loss: 0.4505, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8245, val/misogyny_memes/roc_auc: 0.8886, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 01m 02s 866ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.888621\n",
            "\u001b[32m2022-08-02T15:16:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.5222, train/misogyny_memes/cross_entropy/avg: 0.5328, train/total_loss: 0.5222, train/total_loss/avg: 0.5328, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00002, ups: 2.13, time: 47s 303ms, time_since_start: 01h 04m 31s 618ms, eta: 01m 36s 498ms\n",
            "\u001b[32m2022-08-02T15:16:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:16:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:16:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:16:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:16:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:17:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4220, val/total_loss: 0.4220, val/misogyny_memes/accuracy: 0.8050, val/misogyny_memes/binary_f1: 0.8162, val/misogyny_memes/roc_auc: 0.8923, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 57s 406ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.892279\n",
            "\u001b[32m2022-08-02T15:18:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:18:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:18:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:18:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:18:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:19:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4561, val/total_loss: 0.4561, val/misogyny_memes/accuracy: 0.8000, val/misogyny_memes/binary_f1: 0.8217, val/misogyny_memes/roc_auc: 0.8934, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 55s 811ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.893376\n",
            "\u001b[32m2022-08-02T15:20:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.5222, train/misogyny_memes/cross_entropy/avg: 0.5077, train/total_loss: 0.5222, train/total_loss/avg: 0.5077, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00002, ups: 2.13, time: 47s 246ms, time_since_start: 01h 07m 59s 536ms, eta: 48s 191ms\n",
            "\u001b[32m2022-08-02T15:20:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:20:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:20:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:20:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:20:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:21:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.4398, val/total_loss: 0.4398, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8254, val/misogyny_memes/roc_auc: 0.8946, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 56s 371ms, best_update: 900, best_iteration: 900, best_val/misogyny_memes/roc_auc: 0.894645\n",
            "\u001b[32m2022-08-02T15:21:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:22:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:22:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:22:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:22:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:22:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.6268, val/total_loss: 0.6268, val/misogyny_memes/accuracy: 0.7790, val/misogyny_memes/binary_f1: 0.8154, val/misogyny_memes/roc_auc: 0.8925, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 38s 596ms, best_update: 900, best_iteration: 900, best_val/misogyny_memes/roc_auc: 0.894645\n",
            "\u001b[32m2022-08-02T15:23:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:23:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:23:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:23:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:23:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:23:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.4390, train/misogyny_memes/cross_entropy/avg: 0.4686, train/total_loss: 0.4390, train/total_loss/avg: 0.4686, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00003, ups: 1.35, time: 01m 14s 335ms, time_since_start: 01h 11m 36s 152ms, eta: 0ms\n",
            "\u001b[32m2022-08-02T15:23:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:24:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:24:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:24:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:24:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:24:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.4499, val/total_loss: 0.4499, val/misogyny_memes/accuracy: 0.8220, val/misogyny_memes/binary_f1: 0.8379, val/misogyny_memes/roc_auc: 0.9041, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 53s 706ms, best_update: 1000, best_iteration: 1000, best_val/misogyny_memes/roc_auc: 0.904103\n",
            "\u001b[32m2022-08-02T15:24:46 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-02T15:24:46 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-02T15:24:46 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:24:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-02T15:24:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-02T15:24:50 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-02T15:24:50 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1000\n",
            "\u001b[32m2022-08-02T15:24:50 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1000\n",
            "\u001b[32m2022-08-02T15:24:50 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
            "\u001b[32m2022-08-02T15:24:53 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:12<00:00,  2.61it/s]\n",
            "\u001b[32m2022-08-02T15:25:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.4499, val/total_loss: 0.4499, val/misogyny_memes/accuracy: 0.8220, val/misogyny_memes/binary_f1: 0.8379, val/misogyny_memes/roc_auc: 0.9041\n",
            "\u001b[32m2022-08-02T15:25:05 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 12m 51s 544ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "PtJVB8CFNAZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vilbert_0802_6"
      ],
      "metadata": {
        "id": "S4WEc5aQNA_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViLBERT Masked Conceptual Captions\n",
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/vilbert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.6 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./vilbert_0802_6 \\\n",
        "        env.tensorboard_logdir=logs/fit/vilbert_0802_6 \\"
      ],
      "metadata": {
        "id": "aWA8AuaDOeC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56d6ad57-7cdc-428e-dee2-6986111a30e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.direct\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.6\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./vilbert_0802_6\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/vilbert_0802_6\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf: \u001b[0mLogging to: ./vilbert_0802_6/train.log\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/misogyny_memes/defaults.yaml', 'model=vilbert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.direct', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.lr_ratio=0.6', 'training.use_warmup=True', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./vilbert_0802_6', 'env.tensorboard_logdir=logs/fit/vilbert_0802_6'])\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf_cli.run: \u001b[0mUsing seed 31621299\n",
            "\u001b[32m2022-08-03T01:55:31 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "Downloading: 100% 433/433 [00:00<00:00, 386kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.76MB/s]\n",
            "\u001b[32m2022-08-03T01:55:36 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 80.0MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-03T01:55:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-03T01:55:48 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-03T01:55:48 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_direct.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.direct/vilbert.finetuned.hateful_memes_direct.tar.gz ]\n",
            "Downloading vilbert.finetuned.hateful_memes_direct.tar.gz: 100% 918M/918M [00:36<00:00, 25.4MB/s]\n",
            "[ Starting checksum for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
            "[ Checksum successful for vilbert.finetuned.hateful_memes_direct.tar.gz]\n",
            "Unpacking vilbert.finetuned.hateful_memes_direct.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T01:56:37 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T01:56:37 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T01:56:37 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T01:56:37 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T01:56:38 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
            "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
            "Unexpected keys if any: []\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T01:56:38 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T01:56:38 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T01:56:38 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T01:56:38 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[32m2022-08-03T01:56:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-03T01:56:38 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-03T01:56:38 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-03T01:56:38 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-03T01:56:38 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-03T01:56:38 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-03T01:56:38 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-08-03T01:56:38 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-03T02:00:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:02:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:02:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:02:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:02:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:05:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 1.0530, val/total_loss: 1.0530, val/misogyny_memes/accuracy: 0.6550, val/misogyny_memes/binary_f1: 0.6820, val/misogyny_memes/roc_auc: 0.7041, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 05m 29s 945ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.704068\n",
            "\u001b[32m2022-08-03T02:09:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.5260, train/misogyny_memes/cross_entropy/avg: 0.5260, train/total_loss: 0.5260, train/total_loss/avg: 0.5260, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00001, ups: 0.49, time: 03m 26s 122ms, time_since_start: 13m 16s 660ms, eta: 31m 32s 208ms\n",
            "\u001b[32m2022-08-03T02:09:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:09:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:09:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:09:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:09:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:10:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.5576, val/total_loss: 0.5576, val/misogyny_memes/accuracy: 0.7320, val/misogyny_memes/binary_f1: 0.7428, val/misogyny_memes/roc_auc: 0.7978, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 56s 815ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.797763\n",
            "\u001b[32m2022-08-03T02:13:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:13:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:13:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:13:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:13:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:13:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.5239, val/total_loss: 0.5239, val/misogyny_memes/accuracy: 0.7470, val/misogyny_memes/binary_f1: 0.7668, val/misogyny_memes/roc_auc: 0.8276, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 51s 023ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.827555\n",
            "\u001b[32m2022-08-03T02:17:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.3883, train/misogyny_memes/cross_entropy/avg: 0.4571, train/total_loss: 0.3883, train/total_loss/avg: 0.4571, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00002, ups: 0.55, time: 03m 02s 184ms, time_since_start: 21m 13s 555ms, eta: 24m 46s 625ms\n",
            "\u001b[32m2022-08-03T02:17:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:17:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:17:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:17:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:17:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:17:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.4866, val/total_loss: 0.4866, val/misogyny_memes/accuracy: 0.7790, val/misogyny_memes/binary_f1: 0.7797, val/misogyny_memes/roc_auc: 0.8474, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 51s 695ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.847385\n",
            "\u001b[32m2022-08-03T02:20:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:21:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:21:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:21:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:21:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:21:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4624, val/total_loss: 0.4624, val/misogyny_memes/accuracy: 0.7720, val/misogyny_memes/binary_f1: 0.7841, val/misogyny_memes/roc_auc: 0.8615, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 52s 607ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.861466\n",
            "\u001b[32m2022-08-03T02:23:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.4239, train/misogyny_memes/cross_entropy/avg: 0.4461, train/total_loss: 0.4239, train/total_loss/avg: 0.4461, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00003, ups: 0.83, time: 02m 01s 158ms, time_since_start: 27m 58s 145ms, eta: 14m 25s 073ms\n",
            "\u001b[32m2022-08-03T02:23:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:23:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:23:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:23:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:23:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:24:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4770, val/total_loss: 0.4770, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8070, val/misogyny_memes/roc_auc: 0.8707, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 55s 289ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.870693\n",
            "\u001b[32m2022-08-03T02:25:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:25:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:25:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:25:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:25:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:26:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4502, val/total_loss: 0.4502, val/misogyny_memes/accuracy: 0.7980, val/misogyny_memes/binary_f1: 0.8177, val/misogyny_memes/roc_auc: 0.8848, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 47s 515ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.884762\n",
            "\u001b[32m2022-08-03T02:27:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.4239, train/misogyny_memes/cross_entropy/avg: 0.4825, train/total_loss: 0.4239, train/total_loss/avg: 0.4825, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00004, ups: 2.13, time: 47s 277ms, time_since_start: 31m 15s 702ms, eta: 04m 49s 335ms\n",
            "\u001b[32m2022-08-03T02:27:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:27:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:27:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:27:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:27:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:27:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.5128, val/total_loss: 0.5128, val/misogyny_memes/accuracy: 0.7780, val/misogyny_memes/binary_f1: 0.8128, val/misogyny_memes/roc_auc: 0.8820, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 34s 400ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.884762\n",
            "\u001b[32m2022-08-03T02:28:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:28:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:28:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:28:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:28:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:29:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4330, val/total_loss: 0.4330, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8204, val/misogyny_memes/roc_auc: 0.8910, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 52s 890ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.891042\n",
            "\u001b[32m2022-08-03T02:30:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.4239, train/misogyny_memes/cross_entropy/avg: 0.4550, train/total_loss: 0.4239, train/total_loss/avg: 0.4550, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 2.13, time: 47s 645ms, time_since_start: 34m 17s 827ms, eta: 04m 02s 992ms\n",
            "\u001b[32m2022-08-03T02:30:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:30:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:30:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:30:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:30:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:30:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4721, val/total_loss: 0.4721, val/misogyny_memes/accuracy: 0.7870, val/misogyny_memes/binary_f1: 0.8140, val/misogyny_memes/roc_auc: 0.8984, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 52s 189ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.898359\n",
            "\u001b[32m2022-08-03T02:31:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:31:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:31:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:31:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:31:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:32:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4162, val/total_loss: 0.4162, val/misogyny_memes/accuracy: 0.8210, val/misogyny_memes/binary_f1: 0.8319, val/misogyny_memes/roc_auc: 0.8987, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 53s 957ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.898748\n",
            "\u001b[32m2022-08-03T02:33:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.3883, train/misogyny_memes/cross_entropy/avg: 0.4141, train/total_loss: 0.3883, train/total_loss/avg: 0.4141, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 2.08, time: 48s 016ms, time_since_start: 37m 39s 523ms, eta: 03m 15s 908ms\n",
            "\u001b[32m2022-08-03T02:33:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:33:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:33:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:33:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:33:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:34:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4555, val/total_loss: 0.4555, val/misogyny_memes/accuracy: 0.8100, val/misogyny_memes/binary_f1: 0.8351, val/misogyny_memes/roc_auc: 0.8997, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 01m 101ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.899688\n",
            "\u001b[32m2022-08-03T02:35:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:35:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:35:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:35:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:35:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:35:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4402, val/total_loss: 0.4402, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8225, val/misogyny_memes/roc_auc: 0.8903, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 39s 422ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.899688\n",
            "\u001b[32m2022-08-03T02:36:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.4239, train/misogyny_memes/cross_entropy/avg: 0.4163, train/total_loss: 0.4239, train/total_loss/avg: 0.4163, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.13, time: 47s 754ms, time_since_start: 40m 54s 398ms, eta: 02m 26s 129ms\n",
            "\u001b[32m2022-08-03T02:36:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:36:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:36:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:36:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:36:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:37:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4925, val/total_loss: 0.4925, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8338, val/misogyny_memes/roc_auc: 0.8937, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 34s 674ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.899688\n",
            "\u001b[32m2022-08-03T02:38:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:38:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:38:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:38:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:38:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:39:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4002, val/total_loss: 0.4002, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8287, val/misogyny_memes/roc_auc: 0.9070, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 53s 740ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.906961\n",
            "\u001b[32m2022-08-03T02:39:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.3883, train/misogyny_memes/cross_entropy/avg: 0.3940, train/total_loss: 0.3883, train/total_loss/avg: 0.3940, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 1.96, time: 51s 029ms, time_since_start: 44m 01s 277ms, eta: 01m 44s 100ms\n",
            "\u001b[32m2022-08-03T02:39:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:40:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:40:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:40:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:40:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:40:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4261, val/total_loss: 0.4261, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8293, val/misogyny_memes/roc_auc: 0.9065, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 38s 843ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.906961\n",
            "\u001b[32m2022-08-03T02:41:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:41:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:41:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:41:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:41:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:41:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4660, val/total_loss: 0.4660, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8260, val/misogyny_memes/roc_auc: 0.8914, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 37s 315ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.906961\n",
            "\u001b[32m2022-08-03T02:42:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.3883, train/misogyny_memes/cross_entropy/avg: 0.3587, train/total_loss: 0.3883, train/total_loss/avg: 0.3587, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.13, time: 47s 901ms, time_since_start: 46m 53s 294ms, eta: 48s 859ms\n",
            "\u001b[32m2022-08-03T02:42:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:42:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:42:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:42:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:42:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:43:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5224, val/total_loss: 0.5224, val/misogyny_memes/accuracy: 0.8220, val/misogyny_memes/binary_f1: 0.8279, val/misogyny_memes/roc_auc: 0.8977, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 36s 734ms, best_update: 750, best_iteration: 750, best_val/misogyny_memes/roc_auc: 0.906961\n",
            "\u001b[32m2022-08-03T02:44:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:44:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:44:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:44:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:44:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:45:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.4820, val/total_loss: 0.4820, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8338, val/misogyny_memes/roc_auc: 0.9083, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 54s 140ms, best_update: 950, best_iteration: 950, best_val/misogyny_memes/roc_auc: 0.908322\n",
            "\u001b[32m2022-08-03T02:45:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:45:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:45:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:45:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:45:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:46:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.3450, train/misogyny_memes/cross_entropy/avg: 0.3432, train/total_loss: 0.3450, train/total_loss/avg: 0.3432, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 1.28, time: 01m 18s 629ms, time_since_start: 50m 30s 355ms, eta: 0ms\n",
            "\u001b[32m2022-08-03T02:46:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:46:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:46:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:46:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:46:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:46:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.4805, val/total_loss: 0.4805, val/misogyny_memes/accuracy: 0.8190, val/misogyny_memes/binary_f1: 0.8341, val/misogyny_memes/roc_auc: 0.8989, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 41s 169ms, best_update: 950, best_iteration: 950, best_val/misogyny_memes/roc_auc: 0.908322\n",
            "\u001b[32m2022-08-03T02:47:00 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-03T02:47:00 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-03T02:47:00 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:47:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T02:47:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T02:47:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-03T02:47:07 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 950\n",
            "\u001b[32m2022-08-03T02:47:07 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 950\n",
            "\u001b[32m2022-08-03T02:47:07 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 4\n",
            "\u001b[32m2022-08-03T02:47:10 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:12<00:00,  2.59it/s]\n",
            "\u001b[32m2022-08-03T02:47:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.4820, val/total_loss: 0.4820, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8338, val/misogyny_memes/roc_auc: 0.9083\n",
            "\u001b[32m2022-08-03T02:47:22 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 51m 34s 536ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "wwh0vM-4NrWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vilbert_0802_7\n"
      ],
      "metadata": {
        "id": "2dpJ8ds2NHs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViLBERT Masked Conceptual Captions\n",
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/vilbert/configs/misogyny_memes/from_cc.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        checkpoint.resume_zoo=vilbert.pretrained.cc.full \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.6 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./vilbert_0802_7 \\\n",
        "        env.tensorboard_logdir=logs/fit/vilbert_0802_7 \\"
      ],
      "metadata": {
        "id": "uUPhxEHGO70z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8055da8d-5360-4ca6-8d02-873f37c453da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/misogyny_memes/from_cc.yaml\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.pretrained.cc.full\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.6\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./vilbert_0802_7\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/vilbert_0802_7\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf: \u001b[0mLogging to: ./vilbert_0802_7/train.log\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/misogyny_memes/from_cc.yaml', 'model=vilbert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'checkpoint.resume_zoo=vilbert.pretrained.cc.full', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.lr_ratio=0.6', 'training.use_warmup=True', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./vilbert_0802_7', 'env.tensorboard_logdir=logs/fit/vilbert_0802_7'])\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf_cli.run: \u001b[0mUsing seed 20205404\n",
            "\u001b[32m2022-08-03T03:09:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-03T03:09:23 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-03T03:09:30 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-03T03:09:30 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-03T03:09:30 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:09:37 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:09:37 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:09:37 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:09:37 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:09:37 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-08-03T03:09:38 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-03T03:11:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:11:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:11:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:11:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:11:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:12:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.6833, val/total_loss: 0.6833, val/misogyny_memes/accuracy: 0.5800, val/misogyny_memes/binary_f1: 0.6410, val/misogyny_memes/roc_auc: 0.6338, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 49s 269ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.633836\n",
            "\u001b[32m2022-08-03T03:14:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.5892, train/misogyny_memes/cross_entropy/avg: 0.5892, train/total_loss: 0.5892, train/total_loss/avg: 0.5892, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00001, ups: 0.87, time: 01m 55s 543ms, time_since_start: 04m 59s 844ms, eta: 17m 40s 691ms\n",
            "\u001b[32m2022-08-03T03:14:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:14:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:14:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:14:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:14:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:15:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.6224, val/total_loss: 0.6224, val/misogyny_memes/accuracy: 0.6550, val/misogyny_memes/binary_f1: 0.6861, val/misogyny_memes/roc_auc: 0.7229, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 49s 670ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.722880\n",
            "\u001b[32m2022-08-03T03:17:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:17:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:17:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:17:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:17:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:18:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.5222, val/total_loss: 0.5222, val/misogyny_memes/accuracy: 0.7370, val/misogyny_memes/binary_f1: 0.7449, val/misogyny_memes/roc_auc: 0.8180, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 53s 713ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.818037\n",
            "\u001b[32m2022-08-03T03:20:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.4722, train/misogyny_memes/cross_entropy/avg: 0.5307, train/total_loss: 0.4722, train/total_loss/avg: 0.5307, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00002, ups: 0.87, time: 01m 55s 198ms, time_since_start: 10m 33s 881ms, eta: 15m 40s 016ms\n",
            "\u001b[32m2022-08-03T03:20:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:20:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:20:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:20:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:20:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:20:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.4968, val/total_loss: 0.4968, val/misogyny_memes/accuracy: 0.7830, val/misogyny_memes/binary_f1: 0.7879, val/misogyny_memes/roc_auc: 0.8588, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 49s 467ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.858773\n",
            "\u001b[32m2022-08-03T03:22:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:23:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:23:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:23:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:23:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:23:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4759, val/total_loss: 0.4759, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.7729, val/misogyny_memes/roc_auc: 0.8891, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 52s 986ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.889061\n",
            "\u001b[32m2022-08-03T03:25:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.4722, train/misogyny_memes/cross_entropy/avg: 0.4585, train/total_loss: 0.4722, train/total_loss/avg: 0.4585, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00003, ups: 1.01, time: 01m 39s 840ms, time_since_start: 15m 56s 840ms, eta: 11m 52s 862ms\n",
            "\u001b[32m2022-08-03T03:25:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:25:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:25:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:25:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:25:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:26:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4531, val/total_loss: 0.4531, val/misogyny_memes/accuracy: 0.7960, val/misogyny_memes/binary_f1: 0.7875, val/misogyny_memes/roc_auc: 0.8913, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 49s 458ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.891339\n",
            "\u001b[32m2022-08-03T03:27:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:27:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:27:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:27:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:27:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:27:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4630, val/total_loss: 0.4630, val/misogyny_memes/accuracy: 0.7980, val/misogyny_memes/binary_f1: 0.8020, val/misogyny_memes/roc_auc: 0.8927, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 51s 639ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.892715\n",
            "\u001b[32m2022-08-03T03:28:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.3143, train/misogyny_memes/cross_entropy/avg: 0.3910, train/total_loss: 0.3143, train/total_loss/avg: 0.3910, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00004, ups: 2.13, time: 47s 924ms, time_since_start: 19m 13s 882ms, eta: 04m 53s 298ms\n",
            "\u001b[32m2022-08-03T03:28:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:28:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:28:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:28:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:28:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:29:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4264, val/total_loss: 0.4264, val/misogyny_memes/accuracy: 0.8210, val/misogyny_memes/binary_f1: 0.8284, val/misogyny_memes/roc_auc: 0.8989, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 58s 872ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.898944\n",
            "\u001b[32m2022-08-03T03:30:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:30:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:30:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:30:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:30:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:31:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4817, val/total_loss: 0.4817, val/misogyny_memes/accuracy: 0.8100, val/misogyny_memes/binary_f1: 0.8316, val/misogyny_memes/roc_auc: 0.8946, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 36s 377ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.898944\n",
            "\u001b[32m2022-08-03T03:31:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.4682, train/misogyny_memes/cross_entropy/avg: 0.4064, train/total_loss: 0.4682, train/total_loss/avg: 0.4064, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 2.13, time: 47s 637ms, time_since_start: 22m 24s 411ms, eta: 04m 02s 950ms\n",
            "\u001b[32m2022-08-03T03:31:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:32:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:32:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:32:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:32:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:32:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4424, val/total_loss: 0.4424, val/misogyny_memes/accuracy: 0.8050, val/misogyny_memes/binary_f1: 0.8270, val/misogyny_memes/roc_auc: 0.8847, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 37s 019ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.898944\n",
            "\u001b[32m2022-08-03T03:33:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:33:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:33:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:33:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:33:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:34:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.3865, val/total_loss: 0.3865, val/misogyny_memes/accuracy: 0.8240, val/misogyny_memes/binary_f1: 0.8385, val/misogyny_memes/roc_auc: 0.9071, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 51s 666ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.907081\n",
            "\u001b[32m2022-08-03T03:35:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.3143, train/misogyny_memes/cross_entropy/avg: 0.3901, train/total_loss: 0.3143, train/total_loss/avg: 0.3901, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 2.13, time: 47s 728ms, time_since_start: 25m 30s 630ms, eta: 03m 14s 733ms\n",
            "\u001b[32m2022-08-03T03:35:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:35:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:35:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:35:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:35:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:35:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4509, val/total_loss: 0.4509, val/misogyny_memes/accuracy: 0.8430, val/misogyny_memes/binary_f1: 0.8517, val/misogyny_memes/roc_auc: 0.9178, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 55s 831ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.917788\n",
            "\u001b[32m2022-08-03T03:36:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:36:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:36:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:36:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:36:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:37:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4568, val/total_loss: 0.4568, val/misogyny_memes/accuracy: 0.8260, val/misogyny_memes/binary_f1: 0.8410, val/misogyny_memes/roc_auc: 0.9112, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 41s 557ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.917788\n",
            "\u001b[32m2022-08-03T03:38:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.3143, train/misogyny_memes/cross_entropy/avg: 0.3555, train/total_loss: 0.3143, train/total_loss/avg: 0.3555, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.13, time: 47s 484ms, time_since_start: 28m 43s 019ms, eta: 02m 25s 301ms\n",
            "\u001b[32m2022-08-03T03:38:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:38:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:38:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:38:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:38:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:39:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4274, val/total_loss: 0.4274, val/misogyny_memes/accuracy: 0.8220, val/misogyny_memes/binary_f1: 0.8248, val/misogyny_memes/roc_auc: 0.9105, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 46s 559ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.917788\n",
            "\u001b[32m2022-08-03T03:39:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:39:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:39:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:39:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:39:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:40:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4414, val/total_loss: 0.4414, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8276, val/misogyny_memes/roc_auc: 0.9101, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 39s 071ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.917788\n",
            "\u001b[32m2022-08-03T03:41:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.3143, train/misogyny_memes/cross_entropy/avg: 0.3623, train/total_loss: 0.3143, train/total_loss/avg: 0.3623, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 2.13, time: 47s 324ms, time_since_start: 31m 43s 531ms, eta: 01m 36s 542ms\n",
            "\u001b[32m2022-08-03T03:41:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:41:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:41:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:41:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:41:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:42:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4998, val/total_loss: 0.4998, val/misogyny_memes/accuracy: 0.8070, val/misogyny_memes/binary_f1: 0.8311, val/misogyny_memes/roc_auc: 0.9046, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 49s 320ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.917788\n",
            "\u001b[32m2022-08-03T03:42:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:43:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:43:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:43:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:43:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:43:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.5195, val/total_loss: 0.5195, val/misogyny_memes/accuracy: 0.7880, val/misogyny_memes/binary_f1: 0.8221, val/misogyny_memes/roc_auc: 0.9169, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 47s 057ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.917788\n",
            "\u001b[32m2022-08-03T03:44:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.3143, train/misogyny_memes/cross_entropy/avg: 0.3334, train/total_loss: 0.3143, train/total_loss/avg: 0.3334, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.13, time: 47s 608ms, time_since_start: 34m 55s 100ms, eta: 48s 560ms\n",
            "\u001b[32m2022-08-03T03:44:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:44:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:44:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:44:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:44:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:45:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5592, val/total_loss: 0.5592, val/misogyny_memes/accuracy: 0.8290, val/misogyny_memes/binary_f1: 0.8397, val/misogyny_memes/roc_auc: 0.9080, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 48s 216ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.917788\n",
            "\u001b[32m2022-08-03T03:46:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:46:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:46:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:46:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:46:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:46:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.5743, val/total_loss: 0.5743, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8417, val/misogyny_memes/roc_auc: 0.9099, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 49s 899ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.917788\n",
            "\u001b[32m2022-08-03T03:47:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:47:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:47:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:47:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:47:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:48:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.3084, train/misogyny_memes/cross_entropy/avg: 0.3055, train/total_loss: 0.3084, train/total_loss/avg: 0.3055, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 1.41, time: 01m 11s 527ms, time_since_start: 38m 32s 386ms, eta: 0ms\n",
            "\u001b[32m2022-08-03T03:48:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:48:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:48:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:48:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:48:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:48:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.8269, val/total_loss: 0.8269, val/misogyny_memes/accuracy: 0.8000, val/misogyny_memes/binary_f1: 0.8285, val/misogyny_memes/roc_auc: 0.8864, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 40s 695ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.917788\n",
            "\u001b[32m2022-08-03T03:48:44 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-03T03:48:44 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-03T03:48:44 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:49:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:49:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T03:49:05 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-03T03:49:05 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 600\n",
            "\u001b[32m2022-08-03T03:49:05 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 600\n",
            "\u001b[32m2022-08-03T03:49:05 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-03T03:49:11 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:12<00:00,  2.56it/s]\n",
            "\u001b[32m2022-08-03T03:49:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4509, val/total_loss: 0.4509, val/misogyny_memes/accuracy: 0.8430, val/misogyny_memes/binary_f1: 0.8517, val/misogyny_memes/roc_auc: 0.9178\n",
            "\u001b[32m2022-08-03T03:49:23 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 39m 53s 255ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_J1EVP9SNr8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vilbert_0802_8\n"
      ],
      "metadata": {
        "id": "gs2EER4vNN4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViLBERT Masked Conceptual Captions\n",
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/vilbert/configs/misogyny_memes/from_cc.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.6 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./vilbert_0802_8 \\\n",
        "        env.tensorboard_logdir=logs/fit/vilbert_0802_8 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SDzMNqVvw0x",
        "outputId": "58dbaa4b-b296-4ff9-e2c5-bce41cffd7ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/misogyny_memes/from_cc.yaml\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.6\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./vilbert_0802_8\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/vilbert_0802_8\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf: \u001b[0mLogging to: ./vilbert_0802_8/train.log\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/misogyny_memes/from_cc.yaml', 'model=vilbert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.lr_ratio=0.6', 'training.use_warmup=True', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./vilbert_0802_8', 'env.tensorboard_logdir=logs/fit/vilbert_0802_8'])\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf_cli.run: \u001b[0mUsing seed 53288081\n",
            "\u001b[32m2022-08-03T03:58:53 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-03T03:58:57 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-03T03:59:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-03T03:59:04 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-03T03:59:04 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.finetuned.hateful_memes_from_cc.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.finetuned.hateful_memes.from_cc_original/vilbert.finetuned.hateful_memes_from_cc.tar.gz ]\n",
            "Downloading vilbert.finetuned.hateful_memes_from_cc.tar.gz: 100% 918M/918M [00:18<00:00, 49.4MB/s]\n",
            "[ Starting checksum for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
            "[ Checksum successful for vilbert.finetuned.hateful_memes_from_cc.tar.gz]\n",
            "Unpacking vilbert.finetuned.hateful_memes_from_cc.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:59:35 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:59:35 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:59:35 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T03:59:35 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:35 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-08-03T03:59:36 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-03T04:00:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:00:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:00:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:00:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:00:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:01:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.6125, val/total_loss: 0.6125, val/misogyny_memes/accuracy: 0.6560, val/misogyny_memes/binary_f1: 0.6560, val/misogyny_memes/roc_auc: 0.7353, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 59s 049ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.735261\n",
            "\u001b[32m2022-08-03T04:02:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.6376, train/misogyny_memes/cross_entropy/avg: 0.6376, train/total_loss: 0.6376, train/total_loss/avg: 0.6376, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00001, ups: 1.64, time: 01m 01s 545ms, time_since_start: 03m 21s 164ms, eta: 09m 24s 989ms\n",
            "\u001b[32m2022-08-03T04:02:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:02:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:02:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:02:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:02:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:03:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.5388, val/total_loss: 0.5388, val/misogyny_memes/accuracy: 0.7240, val/misogyny_memes/binary_f1: 0.7172, val/misogyny_memes/roc_auc: 0.8052, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 52s 995ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.805248\n",
            "\u001b[32m2022-08-03T04:06:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:06:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:06:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:06:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:06:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:07:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.5577, val/total_loss: 0.5577, val/misogyny_memes/accuracy: 0.7460, val/misogyny_memes/binary_f1: 0.7196, val/misogyny_memes/roc_auc: 0.8368, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 57s 628ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.836790\n",
            "\u001b[32m2022-08-03T04:10:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.5066, train/misogyny_memes/cross_entropy/avg: 0.5721, train/total_loss: 0.5066, train/total_loss/avg: 0.5721, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00002, ups: 0.55, time: 03m 02s 205ms, time_since_start: 11m 16s 275ms, eta: 24m 46s 794ms\n",
            "\u001b[32m2022-08-03T04:10:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:10:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:10:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:10:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:10:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:11:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.4900, val/total_loss: 0.4900, val/misogyny_memes/accuracy: 0.7480, val/misogyny_memes/binary_f1: 0.7872, val/misogyny_memes/roc_auc: 0.8779, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 54s 693ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.877861\n",
            "\u001b[32m2022-08-03T04:14:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:14:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:14:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:14:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:14:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:15:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4427, val/total_loss: 0.4427, val/misogyny_memes/accuracy: 0.7780, val/misogyny_memes/binary_f1: 0.7762, val/misogyny_memes/roc_auc: 0.8787, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 58s 438ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.878702\n",
            "\u001b[32m2022-08-03T04:17:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.5066, train/misogyny_memes/cross_entropy/avg: 0.5082, train/total_loss: 0.5066, train/total_loss/avg: 0.5082, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00003, ups: 0.81, time: 02m 03s 447ms, time_since_start: 18m 12s 441ms, eta: 14m 41s 416ms\n",
            "\u001b[32m2022-08-03T04:17:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:17:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:17:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:17:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:17:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:18:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4430, val/total_loss: 0.4430, val/misogyny_memes/accuracy: 0.8090, val/misogyny_memes/binary_f1: 0.8103, val/misogyny_memes/roc_auc: 0.8907, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 49s 881ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.890730\n",
            "\u001b[32m2022-08-03T04:18:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:19:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:19:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:19:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:19:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:19:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4482, val/total_loss: 0.4482, val/misogyny_memes/accuracy: 0.8090, val/misogyny_memes/binary_f1: 0.8162, val/misogyny_memes/roc_auc: 0.8814, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 39s 328ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.890730\n",
            "\u001b[32m2022-08-03T04:20:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.3804, train/misogyny_memes/cross_entropy/avg: 0.4632, train/total_loss: 0.3804, train/total_loss/avg: 0.4632, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00004, ups: 2.13, time: 47s 570ms, time_since_start: 21m 16s 482ms, eta: 04m 51s 132ms\n",
            "\u001b[32m2022-08-03T04:20:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:20:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:20:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:20:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:20:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:21:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.5217, val/total_loss: 0.5217, val/misogyny_memes/accuracy: 0.7860, val/misogyny_memes/binary_f1: 0.8136, val/misogyny_memes/roc_auc: 0.8695, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 39s 265ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.890730\n",
            "\u001b[32m2022-08-03T04:21:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:22:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:22:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:22:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:22:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:22:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4515, val/total_loss: 0.4515, val/misogyny_memes/accuracy: 0.7900, val/misogyny_memes/binary_f1: 0.8125, val/misogyny_memes/roc_auc: 0.8870, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 39s 472ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.890730\n",
            "\u001b[32m2022-08-03T04:23:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.4953, train/misogyny_memes/cross_entropy/avg: 0.4696, train/total_loss: 0.4953, train/total_loss/avg: 0.4696, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 2.08, time: 48s 114ms, time_since_start: 24m 11s 067ms, eta: 04m 05s 381ms\n",
            "\u001b[32m2022-08-03T04:23:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:23:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:23:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:23:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:23:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:24:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4061, val/total_loss: 0.4061, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8241, val/misogyny_memes/roc_auc: 0.8976, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 47s 891ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.897559\n",
            "\u001b[32m2022-08-03T04:24:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:25:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:25:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:25:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:25:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:25:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4362, val/total_loss: 0.4362, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.7709, val/misogyny_memes/roc_auc: 0.8875, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 41s 056ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.897559\n",
            "\u001b[32m2022-08-03T04:26:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.3804, train/misogyny_memes/cross_entropy/avg: 0.4265, train/total_loss: 0.3804, train/total_loss/avg: 0.4265, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 2.08, time: 48s 260ms, time_since_start: 27m 18s 190ms, eta: 03m 16s 902ms\n",
            "\u001b[32m2022-08-03T04:26:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:26:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:26:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:26:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:26:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:27:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.5340, val/total_loss: 0.5340, val/misogyny_memes/accuracy: 0.7950, val/misogyny_memes/binary_f1: 0.8228, val/misogyny_memes/roc_auc: 0.8952, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 39s 102ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.897559\n",
            "\u001b[32m2022-08-03T04:27:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:28:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:28:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:28:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:28:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:28:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4368, val/total_loss: 0.4368, val/misogyny_memes/accuracy: 0.8100, val/misogyny_memes/binary_f1: 0.8177, val/misogyny_memes/roc_auc: 0.8962, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 41s 816ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.897559\n",
            "\u001b[32m2022-08-03T04:29:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.3804, train/misogyny_memes/cross_entropy/avg: 0.4007, train/total_loss: 0.3804, train/total_loss/avg: 0.4007, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 2.13, time: 47s 867ms, time_since_start: 30m 14s 990ms, eta: 02m 26s 474ms\n",
            "\u001b[32m2022-08-03T04:29:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:29:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:29:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:29:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:29:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:29:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4837, val/total_loss: 0.4837, val/misogyny_memes/accuracy: 0.7900, val/misogyny_memes/binary_f1: 0.8164, val/misogyny_memes/roc_auc: 0.8891, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 36s 315ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.897559\n",
            "\u001b[32m2022-08-03T04:30:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:30:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:30:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:30:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:30:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:31:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.5392, val/total_loss: 0.5392, val/misogyny_memes/accuracy: 0.7920, val/misogyny_memes/binary_f1: 0.8182, val/misogyny_memes/roc_auc: 0.8843, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 38s 042ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.897559\n",
            "\u001b[32m2022-08-03T04:32:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.3282, train/misogyny_memes/cross_entropy/avg: 0.3729, train/total_loss: 0.3282, train/total_loss/avg: 0.3729, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 2.08, time: 48s 088ms, time_since_start: 33m 05s 301ms, eta: 01m 38s 099ms\n",
            "\u001b[32m2022-08-03T04:32:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:32:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:32:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:32:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:32:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:33:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.5443, val/total_loss: 0.5443, val/misogyny_memes/accuracy: 0.8080, val/misogyny_memes/binary_f1: 0.8289, val/misogyny_memes/roc_auc: 0.9017, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 53s 484ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.901678\n",
            "\u001b[32m2022-08-03T04:33:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:34:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:34:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:34:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:34:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:34:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.4378, val/total_loss: 0.4378, val/misogyny_memes/accuracy: 0.7970, val/misogyny_memes/binary_f1: 0.7901, val/misogyny_memes/roc_auc: 0.8965, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 38s 631ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.901678\n",
            "\u001b[32m2022-08-03T04:35:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.3282, train/misogyny_memes/cross_entropy/avg: 0.3454, train/total_loss: 0.3282, train/total_loss/avg: 0.3454, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 2.08, time: 48s 101ms, time_since_start: 36m 13s 775ms, eta: 49s 063ms\n",
            "\u001b[32m2022-08-03T04:35:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:35:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:35:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:35:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:35:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:35:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5795, val/total_loss: 0.5795, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8253, val/misogyny_memes/roc_auc: 0.9017, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 39s 468ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.901678\n",
            "\u001b[32m2022-08-03T04:36:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:36:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:36:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:36:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:36:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:37:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.6085, val/total_loss: 0.6085, val/misogyny_memes/accuracy: 0.7990, val/misogyny_memes/binary_f1: 0.8251, val/misogyny_memes/roc_auc: 0.8904, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 40s 459ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.901678\n",
            "\u001b[32m2022-08-03T04:38:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:38:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:38:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:38:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:38:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:38:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.2457, train/misogyny_memes/cross_entropy/avg: 0.3244, train/total_loss: 0.2457, train/total_loss/avg: 0.3244, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 1.30, time: 01m 17s 133ms, time_since_start: 39m 38s 663ms, eta: 0ms\n",
            "\u001b[32m2022-08-03T04:38:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:38:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:38:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:38:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:38:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:39:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.5308, val/total_loss: 0.5308, val/misogyny_memes/accuracy: 0.8090, val/misogyny_memes/binary_f1: 0.8126, val/misogyny_memes/roc_auc: 0.8941, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 38s 906ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.901678\n",
            "\u001b[32m2022-08-03T04:39:22 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-03T04:39:22 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-03T04:39:22 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:39:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:39:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:39:46 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-03T04:39:46 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 800\n",
            "\u001b[32m2022-08-03T04:39:46 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 800\n",
            "\u001b[32m2022-08-03T04:39:46 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-03T04:39:52 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:12<00:00,  2.50it/s]\n",
            "\u001b[32m2022-08-03T04:40:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.5443, val/total_loss: 0.5443, val/misogyny_memes/accuracy: 0.8080, val/misogyny_memes/binary_f1: 0.8289, val/misogyny_memes/roc_auc: 0.9017\n",
            "\u001b[32m2022-08-03T04:40:05 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 41m 700ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "9MQQO-1HNtqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# unimodel-sub3: MMBT"
      ],
      "metadata": {
        "id": "Ft6HYq63Hvdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mmbt_0801_1"
      ],
      "metadata": {
        "id": "etFVTCrKR7NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/mmbt/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"mmbt\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=False \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./mmbt_0801_1 \\\n",
        "        env.tensorboard_logdir=logs/fit/mmbt_0801_1 \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7JZg5swwV1x",
        "outputId": "a3346104-7182-4616-cc06-ae599c1b8ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmbt/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 50\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.3\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to False\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./mmbt_0801_1\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/mmbt_0801_1\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf: \u001b[0mLogging to: ./mmbt_0801_1/train.log\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmbt/configs/misogyny_memes/defaults.yaml', 'model=mmbt', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'training.checkpoint_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'training.lr_ratio=0.3', 'training.use_warmup=False', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'env.save_dir=./mmbt_0801_1', 'env.tensorboard_logdir=logs/fit/mmbt_0801_1'])\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf_cli.run: \u001b[0mUsing seed 33136880\n",
            "\u001b[32m2022-08-03T04:49:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-03T04:49:37 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 79.2MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
            "100% 230M/230M [00:00<00:00, 267MB/s]\n",
            "\u001b[32m2022-08-03T04:49:54 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-03T04:49:54 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-03T04:49:54 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-03T04:49:54 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n",
            "  (model): MMBTForClassification(\n",
            "    (bert): MMBTBase(\n",
            "      (mmbt): MMBTModel(\n",
            "        (transformer): BertModelJit(\n",
            "          (embeddings): BertEmbeddingsJit(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoderJit(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "        (modal_encoder): ModalEmbeddings(\n",
            "          (encoder): ResNet152ImageEncoder(\n",
            "            (model): Sequential(\n",
            "              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "              (4): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (5): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (6): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (8): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (9): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (10): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (11): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (12): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (13): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (14): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (15): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (16): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (17): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (18): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (19): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (20): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (21): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (22): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (23): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (24): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (25): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (26): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (27): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (28): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (29): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (30): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (31): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (32): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (33): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (34): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (35): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (7): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "          )\n",
            "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-03T04:49:54 | mmf.utils.general: \u001b[0mTotal Parameters: 169793346. Trained Parameters: 169793346\n",
            "\u001b[32m2022-08-03T04:49:54 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-03T04:50:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:50:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:50:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:51:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:51:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:51:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:51:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.7434, val/total_loss: 0.7434, val/misogyny_memes/accuracy: 0.5150, val/misogyny_memes/binary_f1: 0.6786, val/misogyny_memes/roc_auc: 0.4711, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 42s 657ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.471066\n",
            "\u001b[32m2022-08-03T04:52:31 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:52:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:52:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:52:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.6736, train/misogyny_memes/cross_entropy/avg: 0.6736, train/total_loss: 0.6736, train/total_loss/avg: 0.6736, max mem: 11667.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0., ups: 1.54, time: 01m 05s 220ms, time_since_start: 02m 58s 404ms, eta: 09m 58s 726ms\n",
            "\u001b[32m2022-08-03T04:52:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:53:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:53:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:53:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.6959, val/total_loss: 0.6959, val/misogyny_memes/accuracy: 0.4980, val/misogyny_memes/binary_f1: 0.2417, val/misogyny_memes/roc_auc: 0.5205, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 39s 607ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.520504\n",
            "\u001b[32m2022-08-03T04:54:16 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:54:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:54:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:54:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:54:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:54:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:55:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.6906, val/total_loss: 0.6906, val/misogyny_memes/accuracy: 0.5190, val/misogyny_memes/binary_f1: 0.2745, val/misogyny_memes/roc_auc: 0.5823, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 39s 507ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.582346\n",
            "\u001b[32m2022-08-03T04:56:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:56:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:56:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:56:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.6425, train/misogyny_memes/cross_entropy/avg: 0.6581, train/total_loss: 0.6425, train/total_loss/avg: 0.6581, max mem: 11667.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00001, ups: 1.47, time: 01m 08s 353ms, time_since_start: 06m 32s 686ms, eta: 09m 17s 761ms\n",
            "\u001b[32m2022-08-03T04:56:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:56:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:56:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:57:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.6650, val/total_loss: 0.6650, val/misogyny_memes/accuracy: 0.6030, val/misogyny_memes/binary_f1: 0.6842, val/misogyny_memes/roc_auc: 0.6745, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 45s 438ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.674516\n",
            "\u001b[32m2022-08-03T04:57:56 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:57:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:57:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:58:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:58:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:58:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:58:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.5963, val/total_loss: 0.5963, val/misogyny_memes/accuracy: 0.6870, val/misogyny_memes/binary_f1: 0.7077, val/misogyny_memes/roc_auc: 0.7565, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 41s 976ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.756519\n",
            "\u001b[32m2022-08-03T04:59:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:59:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T04:59:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T04:59:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.6425, train/misogyny_memes/cross_entropy/avg: 0.6458, train/total_loss: 0.6425, train/total_loss/avg: 0.6458, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00001, ups: 1.64, time: 01m 01s 909ms, time_since_start: 10m 05s 237ms, eta: 07m 22s 033ms\n",
            "\u001b[32m2022-08-03T04:59:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:00:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:00:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:00:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.5904, val/total_loss: 0.5904, val/misogyny_memes/accuracy: 0.7170, val/misogyny_memes/binary_f1: 0.6947, val/misogyny_memes/roc_auc: 0.7980, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 39s 709ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.798023\n",
            "\u001b[32m2022-08-03T05:01:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:01:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:01:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:01:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:01:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:01:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:02:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.5282, val/total_loss: 0.5282, val/misogyny_memes/accuracy: 0.7440, val/misogyny_memes/binary_f1: 0.7259, val/misogyny_memes/roc_auc: 0.8346, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 43s 512ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.834604\n",
            "\u001b[32m2022-08-03T05:03:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:03:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:03:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:03:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.6214, train/misogyny_memes/cross_entropy/avg: 0.5940, train/total_loss: 0.6214, train/total_loss/avg: 0.5940, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00001, ups: 1.72, time: 58s 324ms, time_since_start: 13m 32s 278ms, eta: 05m 56s 947ms\n",
            "\u001b[32m2022-08-03T05:03:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:03:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:03:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:04:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4987, val/total_loss: 0.4987, val/misogyny_memes/accuracy: 0.7550, val/misogyny_memes/binary_f1: 0.7340, val/misogyny_memes/roc_auc: 0.8591, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 37s 217ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.859081\n",
            "\u001b[32m2022-08-03T05:04:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:04:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:04:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:05:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:05:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:05:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:05:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.5232, val/total_loss: 0.5232, val/misogyny_memes/accuracy: 0.7490, val/misogyny_memes/binary_f1: 0.7853, val/misogyny_memes/roc_auc: 0.8617, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 37s 786ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.861703\n",
            "\u001b[32m2022-08-03T05:06:28 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:06:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:06:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:06:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.6214, train/misogyny_memes/cross_entropy/avg: 0.5832, train/total_loss: 0.6214, train/total_loss/avg: 0.5832, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00001, ups: 1.72, time: 58s 594ms, time_since_start: 16m 48s 712ms, eta: 04m 58s 831ms\n",
            "\u001b[32m2022-08-03T05:06:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:06:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:06:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:07:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4465, val/total_loss: 0.4465, val/misogyny_memes/accuracy: 0.7980, val/misogyny_memes/binary_f1: 0.7964, val/misogyny_memes/roc_auc: 0.8824, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 39s 836ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.882364\n",
            "\u001b[32m2022-08-03T05:08:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:08:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:08:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:08:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:08:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:08:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:09:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4316, val/total_loss: 0.4316, val/misogyny_memes/accuracy: 0.8090, val/misogyny_memes/binary_f1: 0.8200, val/misogyny_memes/roc_auc: 0.8903, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 37s 525ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.890334\n",
            "\u001b[32m2022-08-03T05:09:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:09:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:09:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:10:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.5401, train/misogyny_memes/cross_entropy/avg: 0.5295, train/total_loss: 0.5401, train/total_loss/avg: 0.5295, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00002, ups: 1.64, time: 01m 01s 567ms, time_since_start: 20m 10s 319ms, eta: 04m 11s 196ms\n",
            "\u001b[32m2022-08-03T05:10:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:10:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:10:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:10:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4419, val/total_loss: 0.4419, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8242, val/misogyny_memes/roc_auc: 0.8954, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 41s 950ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.895368\n",
            "\u001b[32m2022-08-03T05:11:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:11:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:11:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:11:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:11:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:11:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:12:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.4479, val/total_loss: 0.4479, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8306, val/misogyny_memes/roc_auc: 0.8980, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 42s 905ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.897971\n",
            "\u001b[32m2022-08-03T05:13:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:13:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:13:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:13:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.5401, train/misogyny_memes/cross_entropy/avg: 0.4765, train/total_loss: 0.5401, train/total_loss/avg: 0.4765, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00002, ups: 1.61, time: 01m 02s 243ms, time_since_start: 23m 39s 092ms, eta: 03m 10s 465ms\n",
            "\u001b[32m2022-08-03T05:13:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:13:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:13:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:14:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4808, val/total_loss: 0.4808, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8222, val/misogyny_memes/roc_auc: 0.8939, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 30s 277ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.897971\n",
            "\u001b[32m2022-08-03T05:14:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:14:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:14:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:15:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:15:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:15:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:15:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4885, val/total_loss: 0.4885, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8298, val/misogyny_memes/roc_auc: 0.8959, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 28s 604ms, best_update: 650, best_iteration: 650, best_val/misogyny_memes/roc_auc: 0.897971\n",
            "\u001b[32m2022-08-03T05:16:16 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:16:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:16:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:16:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.4387, train/misogyny_memes/cross_entropy/avg: 0.4516, train/total_loss: 0.4387, train/total_loss/avg: 0.4516, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00002, ups: 1.61, time: 01m 02s 448ms, time_since_start: 26m 40s 929ms, eta: 02m 07s 395ms\n",
            "\u001b[32m2022-08-03T05:16:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:16:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:16:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:17:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4204, val/total_loss: 0.4204, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8356, val/misogyny_memes/roc_auc: 0.9031, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 44s 074ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.903143\n",
            "\u001b[32m2022-08-03T05:18:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:18:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:18:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:18:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:18:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:18:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:18:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.5392, val/total_loss: 0.5392, val/misogyny_memes/accuracy: 0.8080, val/misogyny_memes/binary_f1: 0.8004, val/misogyny_memes/roc_auc: 0.9004, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 26s 415ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.903143\n",
            "\u001b[32m2022-08-03T05:19:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:19:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:19:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:19:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.4387, train/misogyny_memes/cross_entropy/avg: 0.4184, train/total_loss: 0.4387, train/total_loss/avg: 0.4184, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00002, ups: 1.67, time: 01m 316ms, time_since_start: 29m 52s 227ms, eta: 01m 01s 522ms\n",
            "\u001b[32m2022-08-03T05:19:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:19:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:19:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:20:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5550, val/total_loss: 0.5550, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8133, val/misogyny_memes/roc_auc: 0.8985, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 28s 145ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.903143\n",
            "\u001b[32m2022-08-03T05:20:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:20:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:20:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:21:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:21:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:21:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "K2qeEygNTSNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mmbt_0801_2"
      ],
      "metadata": {
        "id": "35TfKtXxS94Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/mmbt/configs/misogyny_memes/with_features.yaml\" \\\n",
        "        model=\"mmbt\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=False \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./mmbt_0801_2 \\\n",
        "        env.tensorboard_logdir=logs/fit/mmbt_0801_2 \\"
      ],
      "metadata": {
        "id": "QsqcjZx7S8-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a73f17-4f8a-4cbd-ce31-265bcc5ea8d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmbt/configs/misogyny_memes/with_features.yaml\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 50\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.3\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to False\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./mmbt_0801_2\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/mmbt_0801_2\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf: \u001b[0mLogging to: ./mmbt_0801_2/train.log\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmbt/configs/misogyny_memes/with_features.yaml', 'model=mmbt', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'training.checkpoint_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.lr_ratio=0.3', 'training.use_warmup=False', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'env.save_dir=./mmbt_0801_2', 'env.tensorboard_logdir=logs/fit/mmbt_0801_2'])\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf_cli.run: \u001b[0mUsing seed 41263863\n",
            "\u001b[32m2022-08-03T05:30:41 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-03T05:30:44 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/detectron/vmb_weights.tar.gz to /root/.cache/torch/mmf/data/models/detectron.vmb_weights/vmb_weights.tar.gz ]\n",
            "Downloading vmb_weights.tar.gz: 100% 15.5M/15.5M [00:01<00:00, 13.6MB/s]\n",
            "[ Starting checksum for vmb_weights.tar.gz]\n",
            "[ Checksum successful for vmb_weights.tar.gz]\n",
            "Unpacking vmb_weights.tar.gz\n",
            "\u001b[32m2022-08-03T05:30:56 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-03T05:30:56 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-03T05:30:56 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-03T05:30:56 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n",
            "  (model): MMBTForClassification(\n",
            "    (bert): MMBTBase(\n",
            "      (mmbt): MMBTModel(\n",
            "        (transformer): BertModelJit(\n",
            "          (embeddings): BertEmbeddingsJit(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoderJit(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "        (modal_encoder): ModalEmbeddings(\n",
            "          (encoder): FinetuneFasterRcnnFpnFc7(\n",
            "            (lc): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "          )\n",
            "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-03T05:30:56 | mmf.utils.general: \u001b[0mTotal Parameters: 115845890. Trained Parameters: 115845890\n",
            "\u001b[32m2022-08-03T05:30:56 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-03T05:32:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:32:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:32:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:32:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:32:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:32:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:32:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.7322, val/total_loss: 0.7322, val/misogyny_memes/accuracy: 0.4840, val/misogyny_memes/binary_f1: 0.4647, val/misogyny_memes/roc_auc: 0.4997, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 33s 833ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.499682\n",
            "\u001b[32m2022-08-03T05:35:10 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:35:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:35:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:35:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.6653, train/misogyny_memes/cross_entropy/avg: 0.6653, train/total_loss: 0.6653, train/total_loss/avg: 0.6653, max mem: 9388.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0., ups: 0.68, time: 02m 27s 046ms, time_since_start: 04m 28s 418ms, eta: 22m 29s 891ms\n",
            "\u001b[32m2022-08-03T05:35:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:35:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:35:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:35:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.6612, val/total_loss: 0.6612, val/misogyny_memes/accuracy: 0.5960, val/misogyny_memes/binary_f1: 0.6868, val/misogyny_memes/roc_auc: 0.6932, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 33s 961ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.693232\n",
            "\u001b[32m2022-08-03T05:38:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:38:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:38:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:39:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:39:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:39:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:39:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.6102, val/total_loss: 0.6102, val/misogyny_memes/accuracy: 0.6690, val/misogyny_memes/binary_f1: 0.6268, val/misogyny_memes/roc_auc: 0.7300, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 33s 205ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.729997\n",
            "\u001b[32m2022-08-03T05:42:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:42:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:42:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:42:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.5440, train/misogyny_memes/cross_entropy/avg: 0.6047, train/total_loss: 0.5440, train/total_loss/avg: 0.6047, max mem: 9388.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00001, ups: 0.55, time: 03m 03s 522ms, time_since_start: 11m 41s 290ms, eta: 24m 57s 543ms\n",
            "\u001b[32m2022-08-03T05:42:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:42:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:42:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:43:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.6134, val/total_loss: 0.6134, val/misogyny_memes/accuracy: 0.6710, val/misogyny_memes/binary_f1: 0.5851, val/misogyny_memes/roc_auc: 0.7444, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 35s 744ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.744359\n",
            "\u001b[32m2022-08-03T05:46:03 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:46:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:46:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:46:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:46:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:46:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:46:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.5907, val/total_loss: 0.5907, val/misogyny_memes/accuracy: 0.6750, val/misogyny_memes/binary_f1: 0.5891, val/misogyny_memes/roc_auc: 0.7587, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 34s 186ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.758701\n",
            "\u001b[32m2022-08-03T05:48:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:48:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:48:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:49:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.5440, train/misogyny_memes/cross_entropy/avg: 0.5701, train/total_loss: 0.5440, train/total_loss/avg: 0.5701, max mem: 9388.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00001, ups: 0.81, time: 02m 04s 014ms, time_since_start: 18m 04s 365ms, eta: 14m 45s 464ms\n",
            "\u001b[32m2022-08-03T05:49:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:49:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:49:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:49:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.5772, val/total_loss: 0.5772, val/misogyny_memes/accuracy: 0.6860, val/misogyny_memes/binary_f1: 0.6432, val/misogyny_memes/roc_auc: 0.7662, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 28s 297ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.766226\n",
            "\u001b[32m2022-08-03T05:50:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:50:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:50:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:50:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:50:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T05:50:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T05:50:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.5838, val/total_loss: 0.5838, val/misogyny_memes/accuracy: 0.7050, val/misogyny_memes/binary_f1: 0.6492, val/misogyny_memes/roc_auc: 0.7828, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 34s 739ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.782849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7-5-8oDaTVrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mmbt_0801_3"
      ],
      "metadata": {
        "id": "o1XMmpZrTNsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/mmbt/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"mmbt\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        training.lr_ratio=0.6 \\\n",
        "        training.use_warmup=False \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        env.save_dir=./mmbt_0801_3 \\\n",
        "        env.tensorboard_logdir=logs/fit/mmbt_0801_3 \\"
      ],
      "metadata": {
        "id": "p74voTjbTyuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "oGSgotTyTt0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mmbt_0801_4"
      ],
      "metadata": {
        "id": "oDjvMBx8TvAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/mmbt/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"mmbt\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        checkpoint.resume_zoo=mmbt.hateful_memes.images \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./mmbt_0801_4 \\\n",
        "        env.tensorboard_logdir=logs/fit/mmbt_0801_4 \\"
      ],
      "metadata": {
        "id": "YZxKqVCaShP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b7adef-45c7-4c8a-8ee3-a576f1033cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmbt/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to mmbt.hateful_memes.images\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 50\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.3\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./mmbt_0801_4\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/mmbt_0801_4\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf: \u001b[0mLogging to: ./mmbt_0801_4/train.log\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmbt/configs/misogyny_memes/defaults.yaml', 'model=mmbt', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'checkpoint.resume_zoo=mmbt.hateful_memes.images', 'training.checkpoint_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'training.lr_ratio=0.3', 'training.use_warmup=True', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./mmbt_0801_4', 'env.tensorboard_logdir=logs/fit/mmbt_0801_4'])\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf_cli.run: \u001b[0mUsing seed 58918976\n",
            "\u001b[32m2022-08-05T03:20:58 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "Downloading: 100% 433/433 [00:00<00:00, 425kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 894kB/s] \n",
            "\u001b[32m2022-08-05T03:21:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 80.7MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
            "100% 230M/230M [00:02<00:00, 98.6MB/s]\n",
            "\u001b[32m2022-08-05T03:21:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-05T03:21:22 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-05T03:21:23 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/mmbt/mmbt.finetuned.hateful_memes_images_2020_08_20.tar.gz to /root/.cache/torch/mmf/data/models/mmbt.hateful_memes.images/mmbt.finetuned.hateful_memes_images.tar.gz ]\n",
            "Downloading mmbt.finetuned.hateful_memes_images.tar.gz: 100% 630M/630M [00:27<00:00, 23.3MB/s]\n",
            "[ Starting checksum for mmbt.finetuned.hateful_memes_images.tar.gz]\n",
            "[ Checksum successful for mmbt.finetuned.hateful_memes_images.tar.gz]\n",
            "Unpacking mmbt.finetuned.hateful_memes_images.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:00 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:00 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:00 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:00 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:00 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.mmbt.transformer.embeddings.position_ids'] in the checkpoint.\n",
            "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
            "Unexpected keys if any: []\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:01 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:01 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
            "  \"'optimizer' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:01 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:01 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
            "  \"'lr_scheduler' key is not present in the \"\n",
            "\n",
            "\u001b[32m2022-08-05T03:22:01 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-05T03:22:01 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-05T03:22:01 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-05T03:22:01 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-05T03:22:01 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-05T03:22:01 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n",
            "  (model): MMBTForClassification(\n",
            "    (bert): MMBTBase(\n",
            "      (mmbt): MMBTModel(\n",
            "        (transformer): BertModelJit(\n",
            "          (embeddings): BertEmbeddingsJit(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoderJit(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "        (modal_encoder): ModalEmbeddings(\n",
            "          (encoder): ResNet152ImageEncoder(\n",
            "            (model): Sequential(\n",
            "              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "              (4): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (5): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (6): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (8): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (9): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (10): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (11): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (12): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (13): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (14): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (15): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (16): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (17): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (18): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (19): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (20): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (21): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (22): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (23): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (24): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (25): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (26): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (27): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (28): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (29): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (30): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (31): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (32): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (33): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (34): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (35): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (7): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "          )\n",
            "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-05T03:22:01 | mmf.utils.general: \u001b[0mTotal Parameters: 169793346. Trained Parameters: 169793346\n",
            "\u001b[32m2022-08-05T03:22:01 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-05T03:22:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:22:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:27:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:27:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:27:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:27:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.7221, val/total_loss: 0.7221, val/misogyny_memes/accuracy: 0.6550, val/misogyny_memes/binary_f1: 0.6717, val/misogyny_memes/roc_auc: 0.7043, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 37s 584ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.704312\n",
            "\u001b[32m2022-08-05T03:28:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:28:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:28:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:28:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.5252, train/misogyny_memes/cross_entropy/avg: 0.5252, train/total_loss: 0.5252, train/total_loss/avg: 0.5252, max mem: 11667.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00001, ups: 1.69, time: 59s 923ms, time_since_start: 07m 26s 080ms, eta: 09m 10s 098ms\n",
            "\u001b[32m2022-08-05T03:28:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:28:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:28:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:29:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.5758, val/total_loss: 0.5758, val/misogyny_memes/accuracy: 0.6880, val/misogyny_memes/binary_f1: 0.7258, val/misogyny_memes/roc_auc: 0.7784, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 34s 619ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.778386\n",
            "\u001b[32m2022-08-05T03:30:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:30:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:30:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:30:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:30:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:30:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:30:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.5191, val/total_loss: 0.5191, val/misogyny_memes/accuracy: 0.7330, val/misogyny_memes/binary_f1: 0.7474, val/misogyny_memes/roc_auc: 0.8224, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 35s 094ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.822412\n",
            "\u001b[32m2022-08-05T03:31:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:31:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:31:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:31:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.5084, train/misogyny_memes/cross_entropy/avg: 0.5168, train/total_loss: 0.5084, train/total_loss/avg: 0.5168, max mem: 11667.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00002, ups: 1.72, time: 58s 886ms, time_since_start: 10m 35s 122ms, eta: 08m 510ms\n",
            "\u001b[32m2022-08-05T03:31:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:32:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:32:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:32:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.4868, val/total_loss: 0.4868, val/misogyny_memes/accuracy: 0.7730, val/misogyny_memes/binary_f1: 0.7904, val/misogyny_memes/roc_auc: 0.8605, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 34s 150ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.860546\n",
            "\u001b[32m2022-08-05T03:33:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:33:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:33:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:33:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:33:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:33:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:34:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4497, val/total_loss: 0.4497, val/misogyny_memes/accuracy: 0.7840, val/misogyny_memes/binary_f1: 0.7919, val/misogyny_memes/roc_auc: 0.8726, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 36s 962ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.872582\n",
            "\u001b[32m2022-08-05T03:34:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:34:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:34:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:35:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.5084, train/misogyny_memes/cross_entropy/avg: 0.4253, train/total_loss: 0.5084, train/total_loss/avg: 0.4253, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00003, ups: 1.72, time: 58s 822ms, time_since_start: 13m 45s 305ms, eta: 06m 59s 990ms\n",
            "\u001b[32m2022-08-05T03:35:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:35:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:35:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:35:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4659, val/total_loss: 0.4659, val/misogyny_memes/accuracy: 0.8000, val/misogyny_memes/binary_f1: 0.8084, val/misogyny_memes/roc_auc: 0.8838, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 34s 397ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.883789\n",
            "\u001b[32m2022-08-05T03:36:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:36:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:36:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:36:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:36:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:36:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:37:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4905, val/total_loss: 0.4905, val/misogyny_memes/accuracy: 0.7930, val/misogyny_memes/binary_f1: 0.7757, val/misogyny_memes/roc_auc: 0.8796, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 28s 210ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.883789\n",
            "\u001b[32m2022-08-05T03:37:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:37:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:37:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:38:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.2424, train/misogyny_memes/cross_entropy/avg: 0.3608, train/total_loss: 0.2424, train/total_loss/avg: 0.3608, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00004, ups: 1.59, time: 01m 03s 211ms, time_since_start: 16m 49s 654ms, eta: 06m 26s 852ms\n",
            "\u001b[32m2022-08-05T03:38:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:38:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:38:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:38:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4555, val/total_loss: 0.4555, val/misogyny_memes/accuracy: 0.7970, val/misogyny_memes/binary_f1: 0.8000, val/misogyny_memes/roc_auc: 0.8831, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 24s 527ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.883789\n",
            "\u001b[32m2022-08-05T03:39:20 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:39:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:39:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:39:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:39:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:39:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:40:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4890, val/total_loss: 0.4890, val/misogyny_memes/accuracy: 0.7680, val/misogyny_memes/binary_f1: 0.7993, val/misogyny_memes/roc_auc: 0.8800, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 25s 787ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.883789\n",
            "\u001b[32m2022-08-05T03:40:44 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:40:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:40:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:41:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.4640, train/misogyny_memes/cross_entropy/avg: 0.3814, train/total_loss: 0.4640, train/total_loss/avg: 0.3814, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 1.67, time: 01m 390ms, time_since_start: 19m 38s 806ms, eta: 05m 07s 990ms\n",
            "\u001b[32m2022-08-05T03:41:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:41:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:41:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:41:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.5150, val/total_loss: 0.5150, val/misogyny_memes/accuracy: 0.7930, val/misogyny_memes/binary_f1: 0.7832, val/misogyny_memes/roc_auc: 0.8848, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 36s 816ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.884846\n",
            "\u001b[32m2022-08-05T03:42:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:42:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:42:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:42:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:42:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:42:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:43:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4732, val/total_loss: 0.4732, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8320, val/misogyny_memes/roc_auc: 0.9034, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 38s 821ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:44:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:44:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:44:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:44:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.2424, train/misogyny_memes/cross_entropy/avg: 0.3489, train/total_loss: 0.2424, train/total_loss/avg: 0.3489, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 1.64, time: 01m 01s 090ms, time_since_start: 22m 59s 927ms, eta: 04m 09s 251ms\n",
            "\u001b[32m2022-08-05T03:44:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:44:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:44:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:44:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.5824, val/total_loss: 0.5824, val/misogyny_memes/accuracy: 0.7960, val/misogyny_memes/binary_f1: 0.7976, val/misogyny_memes/roc_auc: 0.8788, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 24s 649ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:45:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:45:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:45:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:45:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:45:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:45:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:46:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.6618, val/total_loss: 0.6618, val/misogyny_memes/accuracy: 0.7990, val/misogyny_memes/binary_f1: 0.8084, val/misogyny_memes/roc_auc: 0.8797, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 24s 478ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:46:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:46:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:46:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:47:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.2424, train/misogyny_memes/cross_entropy/avg: 0.3197, train/total_loss: 0.2424, train/total_loss/avg: 0.3197, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 1.64, time: 01m 01s 411ms, time_since_start: 25m 49s 130ms, eta: 03m 07s 918ms\n",
            "\u001b[32m2022-08-05T03:47:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:47:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:47:20 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:47:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.5407, val/total_loss: 0.5407, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8121, val/misogyny_memes/roc_auc: 0.8907, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 24s 643ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:48:19 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:48:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:48:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:48:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:48:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:48:48 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:49:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.4670, val/total_loss: 0.4670, val/misogyny_memes/accuracy: 0.8090, val/misogyny_memes/binary_f1: 0.8049, val/misogyny_memes/roc_auc: 0.8876, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 27s 142ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:49:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:49:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:49:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:50:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.1861, train/misogyny_memes/cross_entropy/avg: 0.2971, train/total_loss: 0.1861, train/total_loss/avg: 0.2971, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 1.72, time: 58s 539ms, time_since_start: 28m 42s 725ms, eta: 01m 59s 420ms\n",
            "\u001b[32m2022-08-05T03:50:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:50:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:50:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:50:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.5970, val/total_loss: 0.5970, val/misogyny_memes/accuracy: 0.7820, val/misogyny_memes/binary_f1: 0.7970, val/misogyny_memes/roc_auc: 0.8765, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 28s 566ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:51:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:51:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:51:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:51:32 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:51:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:51:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:51:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.6001, val/total_loss: 0.6001, val/misogyny_memes/accuracy: 0.8040, val/misogyny_memes/binary_f1: 0.8168, val/misogyny_memes/roc_auc: 0.8829, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 24s 503ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:52:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:52:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:52:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:52:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.1861, train/misogyny_memes/cross_entropy/avg: 0.2708, train/total_loss: 0.1861, train/total_loss/avg: 0.2708, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 1.72, time: 58s 670ms, time_since_start: 31m 33s 126ms, eta: 59s 843ms\n",
            "\u001b[32m2022-08-05T03:52:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:53:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:53:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:53:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.5694, val/total_loss: 0.5694, val/misogyny_memes/accuracy: 0.7990, val/misogyny_memes/binary_f1: 0.7934, val/misogyny_memes/roc_auc: 0.8942, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 24s 842ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:54:03 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:54:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:54:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:54:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:54:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:54:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:54:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.5663, val/total_loss: 0.5663, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8232, val/misogyny_memes/roc_auc: 0.8891, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 25s 871ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:55:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:55:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:55:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:55:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.1861, train/misogyny_memes/cross_entropy/avg: 0.2730, train/total_loss: 0.1861, train/total_loss/avg: 0.2730, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 1.72, time: 58s 573ms, time_since_start: 34m 26s 526ms, eta: 0ms\n",
            "\u001b[32m2022-08-05T03:55:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:56:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:56:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:56:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.5937, val/total_loss: 0.5937, val/misogyny_memes/accuracy: 0.7920, val/misogyny_memes/binary_f1: 0.7734, val/misogyny_memes/roc_auc: 0.8952, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 26s 415ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.903387\n",
            "\u001b[32m2022-08-05T03:56:15 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-05T03:56:15 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-05T03:56:15 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:56:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T03:56:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T03:56:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-05T03:56:21 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 550\n",
            "\u001b[32m2022-08-05T03:56:21 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 550\n",
            "\u001b[32m2022-08-05T03:56:21 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 2\n",
            "\u001b[32m2022-08-05T03:56:24 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:09<00:00,  3.47it/s]\n",
            "\u001b[32m2022-08-05T03:56:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4732, val/total_loss: 0.4732, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8320, val/misogyny_memes/roc_auc: 0.9034\n",
            "\u001b[32m2022-08-05T03:56:33 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 35m 10s 998ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "WzVdwqrEUfGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mmbt_0801_5"
      ],
      "metadata": {
        "id": "ot5FowU3BlZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/mmbt/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"mmbt\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        checkpoint.resume=True \\\n",
        "        checkpoint.resume_best=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=1000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./mmbt_0801_5 \\\n",
        "        env.tensorboard_logdir=logs/fit/mmbt_0801_5 \\"
      ],
      "metadata": {
        "id": "6AzDaHzvDzf-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "603c9950-2ec4-4192-9cfb-6508dbc311cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmbt/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume to True\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_best to True\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 50\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 1000\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.3\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./mmbt_0801_5\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/mmbt_0801_5\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf: \u001b[0mLogging to: ./mmbt_0801_5/train.log\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmbt/configs/misogyny_memes/defaults.yaml', 'model=mmbt', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'checkpoint.resume=True', 'checkpoint.resume_best=True', 'training.checkpoint_interval=50', 'training.evaluation_interval=50', 'training.max_updates=1000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'training.lr_ratio=0.3', 'training.use_warmup=True', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./mmbt_0801_5', 'env.tensorboard_logdir=logs/fit/mmbt_0801_5'])\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf_cli.run: \u001b[0mUsing seed 5090913\n",
            "\u001b[32m2022-08-05T04:10:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-05T04:10:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "\u001b[32m2022-08-05T04:10:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-05T04:10:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:10:20 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:171: UserWarning: Tried to resume but checkpoint filepath ./mmbt_0801_5/best.ckpt is not present. Trying current.ckpt, otherwise skipping.\n",
            "  ckpt_filepath, reverse_suffix\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:10:20 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:171: UserWarning: Tried to resume but checkpoint filepath ./mmbt_0801_5/best.ckpt is not present. Trying current.ckpt, otherwise skipping.\n",
            "  ckpt_filepath, reverse_suffix\n",
            "\n",
            "\u001b[32m2022-08-05T04:10:20 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-05T04:10:20 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n",
            "  (model): MMBTForClassification(\n",
            "    (bert): MMBTBase(\n",
            "      (mmbt): MMBTModel(\n",
            "        (transformer): BertModelJit(\n",
            "          (embeddings): BertEmbeddingsJit(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoderJit(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "        (modal_encoder): ModalEmbeddings(\n",
            "          (encoder): ResNet152ImageEncoder(\n",
            "            (model): Sequential(\n",
            "              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "              (4): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (5): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (6): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (8): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (9): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (10): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (11): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (12): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (13): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (14): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (15): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (16): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (17): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (18): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (19): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (20): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (21): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (22): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (23): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (24): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (25): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (26): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (27): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (28): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (29): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (30): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (31): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (32): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (33): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (34): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (35): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (7): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "          )\n",
            "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-05T04:10:20 | mmf.utils.general: \u001b[0mTotal Parameters: 169793346. Trained Parameters: 169793346\n",
            "\u001b[32m2022-08-05T04:10:20 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-05T04:11:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:11:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:11:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:11:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:11:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:11:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:11:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/1000, val/misogyny_memes/cross_entropy: 0.6860, val/total_loss: 0.6860, val/misogyny_memes/accuracy: 0.5300, val/misogyny_memes/binary_f1: 0.4125, val/misogyny_memes/roc_auc: 0.5857, num_updates: 50, epoch: 1, iterations: 50, max_updates: 1000, val_time: 38s 450ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.585720\n",
            "\u001b[32m2022-08-05T04:12:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:12:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:12:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:12:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, train/misogyny_memes/cross_entropy: 0.7281, train/misogyny_memes/cross_entropy/avg: 0.7281, train/total_loss: 0.7281, train/total_loss/avg: 0.7281, max mem: 11667.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 1000, lr: 0.00001, ups: 1.72, time: 58s 627ms, time_since_start: 02m 35s 516ms, eta: 08m 58s 197ms\n",
            "\u001b[32m2022-08-05T04:12:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:13:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:13:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:13:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/1000, val/misogyny_memes/cross_entropy: 0.6644, val/total_loss: 0.6644, val/misogyny_memes/accuracy: 0.6230, val/misogyny_memes/binary_f1: 0.5085, val/misogyny_memes/roc_auc: 0.6919, num_updates: 100, epoch: 1, iterations: 100, max_updates: 1000, val_time: 33s 284ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.691908\n",
            "\u001b[32m2022-08-05T04:14:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:14:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:14:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:14:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:14:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:14:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:15:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/1000, val/misogyny_memes/cross_entropy: 0.5924, val/total_loss: 0.5924, val/misogyny_memes/accuracy: 0.7180, val/misogyny_memes/binary_f1: 0.7531, val/misogyny_memes/roc_auc: 0.7753, num_updates: 150, epoch: 1, iterations: 150, max_updates: 1000, val_time: 38s 452ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.775344\n",
            "\u001b[32m2022-08-05T04:15:55 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:15:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:15:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:16:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, train/misogyny_memes/cross_entropy: 0.6249, train/misogyny_memes/cross_entropy/avg: 0.6765, train/total_loss: 0.6249, train/total_loss/avg: 0.6765, max mem: 11667.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 1000, lr: 0.00002, ups: 1.72, time: 58s 465ms, time_since_start: 05m 49s 902ms, eta: 07m 57s 081ms\n",
            "\u001b[32m2022-08-05T04:16:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:16:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:16:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:16:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/1000, val/misogyny_memes/cross_entropy: 0.5561, val/total_loss: 0.5561, val/misogyny_memes/accuracy: 0.7250, val/misogyny_memes/binary_f1: 0.6746, val/misogyny_memes/roc_auc: 0.8316, num_updates: 200, epoch: 1, iterations: 200, max_updates: 1000, val_time: 37s 819ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.831646\n",
            "\u001b[32m2022-08-05T04:17:32 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:17:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:17:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:17:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:18:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:18:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:18:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/1000, val/misogyny_memes/cross_entropy: 0.4872, val/total_loss: 0.4872, val/misogyny_memes/accuracy: 0.7830, val/misogyny_memes/binary_f1: 0.8095, val/misogyny_memes/roc_auc: 0.8792, num_updates: 250, epoch: 1, iterations: 250, max_updates: 1000, val_time: 41s 475ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.879222\n",
            "\u001b[32m2022-08-05T04:19:16 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:19:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:19:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:19:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, train/misogyny_memes/cross_entropy: 0.6249, train/misogyny_memes/cross_entropy/avg: 0.6316, train/total_loss: 0.6249, train/total_loss/avg: 0.6316, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 1000, lr: 0.00003, ups: 1.64, time: 01m 01s 496ms, time_since_start: 09m 13s 109ms, eta: 07m 19s 086ms\n",
            "\u001b[32m2022-08-05T04:19:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:19:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:19:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:20:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/1000, val/misogyny_memes/cross_entropy: 0.4621, val/total_loss: 0.4621, val/misogyny_memes/accuracy: 0.8000, val/misogyny_memes/binary_f1: 0.8058, val/misogyny_memes/roc_auc: 0.8829, num_updates: 300, epoch: 2, iterations: 300, max_updates: 1000, val_time: 37s 225ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.882913\n",
            "\u001b[32m2022-08-05T04:20:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:20:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:20:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:21:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:21:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:21:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:21:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/1000, val/misogyny_memes/cross_entropy: 0.4674, val/total_loss: 0.4674, val/misogyny_memes/accuracy: 0.7960, val/misogyny_memes/binary_f1: 0.7914, val/misogyny_memes/roc_auc: 0.8857, num_updates: 350, epoch: 2, iterations: 350, max_updates: 1000, val_time: 38s 150ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.885675\n",
            "\u001b[32m2022-08-05T04:22:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:22:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:22:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:22:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, train/misogyny_memes/cross_entropy: 0.5419, train/misogyny_memes/cross_entropy/avg: 0.5529, train/total_loss: 0.5419, train/total_loss/avg: 0.5529, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 1000, lr: 0.00004, ups: 1.67, time: 01m 887ms, time_since_start: 12m 30s 507ms, eta: 06m 12s 634ms\n",
            "\u001b[32m2022-08-05T04:22:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:23:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:23:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:23:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/1000, val/misogyny_memes/cross_entropy: 0.4979, val/total_loss: 0.4979, val/misogyny_memes/accuracy: 0.7850, val/misogyny_memes/binary_f1: 0.8022, val/misogyny_memes/roc_auc: 0.8835, num_updates: 400, epoch: 2, iterations: 400, max_updates: 1000, val_time: 27s 106ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.885675\n",
            "\u001b[32m2022-08-05T04:24:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:24:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:24:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:24:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:24:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:24:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:24:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/1000, val/misogyny_memes/cross_entropy: 0.4467, val/total_loss: 0.4467, val/misogyny_memes/accuracy: 0.7990, val/misogyny_memes/binary_f1: 0.8161, val/misogyny_memes/roc_auc: 0.8891, num_updates: 450, epoch: 2, iterations: 450, max_updates: 1000, val_time: 36s 474ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.889105\n",
            "\u001b[32m2022-08-05T04:25:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:25:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:25:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:25:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, train/misogyny_memes/cross_entropy: 0.5419, train/misogyny_memes/cross_entropy/avg: 0.5271, train/total_loss: 0.5419, train/total_loss/avg: 0.5271, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 1000, lr: 0.00005, ups: 1.72, time: 58s 659ms, time_since_start: 15m 34s 556ms, eta: 04m 59s 162ms\n",
            "\u001b[32m2022-08-05T04:25:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:26:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:26:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:26:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/1000, val/misogyny_memes/cross_entropy: 0.4603, val/total_loss: 0.4603, val/misogyny_memes/accuracy: 0.7810, val/misogyny_memes/binary_f1: 0.7985, val/misogyny_memes/roc_auc: 0.8681, num_updates: 500, epoch: 2, iterations: 500, max_updates: 1000, val_time: 27s 908ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.889105\n",
            "\u001b[32m2022-08-05T04:27:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:27:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:27:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:27:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:27:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:27:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:28:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/1000, val/misogyny_memes/cross_entropy: 0.4470, val/total_loss: 0.4470, val/misogyny_memes/accuracy: 0.8180, val/misogyny_memes/binary_f1: 0.8286, val/misogyny_memes/roc_auc: 0.8997, num_updates: 550, epoch: 2, iterations: 550, max_updates: 1000, val_time: 34s 386ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.899668\n",
            "\u001b[32m2022-08-05T04:28:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:28:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:28:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:29:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, train/misogyny_memes/cross_entropy: 0.4242, train/misogyny_memes/cross_entropy/avg: 0.4625, train/total_loss: 0.4242, train/total_loss/avg: 0.4625, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 1000, lr: 0.00005, ups: 1.67, time: 01m 383ms, time_since_start: 18m 39s 696ms, eta: 04m 06s 364ms\n",
            "\u001b[32m2022-08-05T04:29:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:29:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:29:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:29:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/1000, val/misogyny_memes/cross_entropy: 0.4965, val/total_loss: 0.4965, val/misogyny_memes/accuracy: 0.7900, val/misogyny_memes/binary_f1: 0.7752, val/misogyny_memes/roc_auc: 0.8927, num_updates: 600, epoch: 3, iterations: 600, max_updates: 1000, val_time: 27s 213ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.899668\n",
            "\u001b[32m2022-08-05T04:30:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:30:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:30:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:30:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:30:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:30:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:30:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/1000, val/misogyny_memes/cross_entropy: 0.5159, val/total_loss: 0.5159, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8199, val/misogyny_memes/roc_auc: 0.8911, num_updates: 650, epoch: 3, iterations: 650, max_updates: 1000, val_time: 26s 735ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.899668\n",
            "\u001b[32m2022-08-05T04:31:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:31:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:31:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:31:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, train/misogyny_memes/cross_entropy: 0.4242, train/misogyny_memes/cross_entropy/avg: 0.4270, train/total_loss: 0.4242, train/total_loss/avg: 0.4270, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 1000, lr: 0.00005, ups: 1.67, time: 01m 799ms, time_since_start: 21m 36s 053ms, eta: 03m 06s 047ms\n",
            "\u001b[32m2022-08-05T04:31:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:32:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:32:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:32:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/1000, val/misogyny_memes/cross_entropy: 0.4477, val/total_loss: 0.4477, val/misogyny_memes/accuracy: 0.8190, val/misogyny_memes/binary_f1: 0.8297, val/misogyny_memes/roc_auc: 0.8937, num_updates: 700, epoch: 3, iterations: 700, max_updates: 1000, val_time: 28s 405ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.899668\n",
            "\u001b[32m2022-08-05T04:33:08 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:33:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:33:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:33:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:33:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:33:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:34:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/1000, val/misogyny_memes/cross_entropy: 0.5301, val/total_loss: 0.5301, val/misogyny_memes/accuracy: 0.7970, val/misogyny_memes/binary_f1: 0.8143, val/misogyny_memes/roc_auc: 0.8928, num_updates: 750, epoch: 3, iterations: 750, max_updates: 1000, val_time: 32s 420ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.899668\n",
            "\u001b[32m2022-08-05T04:34:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:34:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:34:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:35:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, train/misogyny_memes/cross_entropy: 0.3314, train/misogyny_memes/cross_entropy/avg: 0.4151, train/total_loss: 0.3314, train/total_loss/avg: 0.4151, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 1000, lr: 0.00005, ups: 1.61, time: 01m 02s 351ms, time_since_start: 24m 43s 679ms, eta: 02m 07s 198ms\n",
            "\u001b[32m2022-08-05T04:35:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:35:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:35:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:35:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4875, val/total_loss: 0.4875, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8193, val/misogyny_memes/roc_auc: 0.9000, num_updates: 800, epoch: 3, iterations: 800, max_updates: 1000, val_time: 37s 863ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.900040\n",
            "\u001b[32m2022-08-05T04:36:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:36:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:36:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:36:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:36:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:36:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:37:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/1000, val/misogyny_memes/cross_entropy: 0.5623, val/total_loss: 0.5623, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8173, val/misogyny_memes/roc_auc: 0.8925, num_updates: 850, epoch: 4, iterations: 850, max_updates: 1000, val_time: 25s 359ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.900040\n",
            "\u001b[32m2022-08-05T04:37:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:37:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:37:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:38:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, train/misogyny_memes/cross_entropy: 0.3314, train/misogyny_memes/cross_entropy/avg: 0.3814, train/total_loss: 0.3314, train/total_loss/avg: 0.3814, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 1000, lr: 0.00005, ups: 1.72, time: 58s 836ms, time_since_start: 27m 45s 983ms, eta: 01m 013ms\n",
            "\u001b[32m2022-08-05T04:38:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:38:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:38:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:38:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/1000, val/misogyny_memes/cross_entropy: 0.7754, val/total_loss: 0.7754, val/misogyny_memes/accuracy: 0.7750, val/misogyny_memes/binary_f1: 0.8075, val/misogyny_memes/roc_auc: 0.8868, num_updates: 900, epoch: 4, iterations: 900, max_updates: 1000, val_time: 24s 143ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.900040\n",
            "\u001b[32m2022-08-05T04:39:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:39:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:39:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:39:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:39:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:39:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:39:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/1000, val/misogyny_memes/cross_entropy: 0.6879, val/total_loss: 0.6879, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8296, val/misogyny_memes/roc_auc: 0.8950, num_updates: 950, epoch: 4, iterations: 950, max_updates: 1000, val_time: 26s 991ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.900040\n",
            "\u001b[32m2022-08-05T04:40:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:40:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:40:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:40:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, train/misogyny_memes/cross_entropy: 0.3166, train/misogyny_memes/cross_entropy/avg: 0.3610, train/total_loss: 0.3166, train/total_loss/avg: 0.3610, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 1000, lr: 0.00005, ups: 1.72, time: 58s 681ms, time_since_start: 30m 34s 892ms, eta: 0ms\n",
            "\u001b[32m2022-08-05T04:40:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:41:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:41:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:41:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/1000, val/misogyny_memes/cross_entropy: 0.6180, val/total_loss: 0.6180, val/misogyny_memes/accuracy: 0.8070, val/misogyny_memes/binary_f1: 0.7996, val/misogyny_memes/roc_auc: 0.8970, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 1000, val_time: 26s 043ms, best_update: 800, best_iteration: 800, best_val/misogyny_memes/roc_auc: 0.900040\n",
            "\u001b[32m2022-08-05T04:41:22 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-05T04:41:22 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-05T04:41:22 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:41:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T04:41:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T04:41:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-05T04:41:25 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 800\n",
            "\u001b[32m2022-08-05T04:41:25 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 800\n",
            "\u001b[32m2022-08-05T04:41:25 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-05T04:41:30 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:09<00:00,  3.48it/s]\n",
            "\u001b[32m2022-08-05T04:41:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/1000, val/misogyny_memes/cross_entropy: 0.4875, val/total_loss: 0.4875, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8193, val/misogyny_memes/roc_auc: 0.9000\n",
            "\u001b[32m2022-08-05T04:41:39 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 31m 18s 676ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MVh9A2NvEH9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Unimodels"
      ],
      "metadata": {
        "id": "bJmZNxMzi-Uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## visual_bert 7"
      ],
      "metadata": {
        "id": "7jr0cE81jGqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/visual_bert/configs/misogyny_memes/from_coco.yaml\" \\\n",
        "        model=\"visual_bert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        checkpoint.resume_zoo=visual_bert.pretrained.cc.full \\\n",
        "        training.tensorboard=True \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=3000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        training.lr_ratio=0.7 \\\n",
        "        training.use_warmup=True \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./visualbert_final \\\n",
        "        env.tensorboard_logdir=logs/fit/visualbert_final \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE7qqZgMjGBf",
        "outputId": "edb63157-2586-4140-ba13-c6346cd1c4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/misogyny_memes/from_coco.yaml\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.pretrained.cc.full\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 3000\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.7\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./visualbert_final\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/visualbert_final\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf: \u001b[0mLogging to: ./visualbert_final/train.log\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/misogyny_memes/from_coco.yaml', 'model=visual_bert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'checkpoint.resume_zoo=visual_bert.pretrained.cc.full', 'training.tensorboard=True', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=3000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'training.lr_ratio=0.7', 'training.use_warmup=True', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./visualbert_final', 'env.tensorboard_logdir=logs/fit/visualbert_final'])\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf_cli.run: \u001b[0mUsing seed 49744038\n",
            "\u001b[32m2022-08-03T16:44:49 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "Downloading: 100% 433/433 [00:00<00:00, 454kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 945kB/s] \n",
            "\u001b[32m2022-08-03T16:44:56 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Downloading: 100% 440M/440M [00:08<00:00, 52.6MB/s]\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-03T16:45:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-03T16:45:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-03T16:45:13 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/visual_bert/visual_bert.pretrained.cc_full.tar.gz to /root/.cache/torch/mmf/data/models/visual_bert.pretrained.cc.full/visual_bert.pretrained.cc_full.tar.gz ]\n",
            "Downloading visual_bert.pretrained.cc_full.tar.gz: 100% 415M/415M [00:34<00:00, 11.9MB/s]\n",
            "[ Starting checksum for visual_bert.pretrained.cc_full.tar.gz]\n",
            "[ Checksum successful for visual_bert.pretrained.cc_full.tar.gz]\n",
            "Unpacking visual_bert.pretrained.cc_full.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T16:45:58 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T16:45:58 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T16:45:58 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T16:45:58 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.trainers.mmf_trainer: \u001b[0mVisualBERT(\n",
            "  (model): VisualBERTForClassification(\n",
            "    (bert): VisualBERTBase(\n",
            "      (embeddings): BertVisioLinguisticEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (token_type_embeddings_visual): Embedding(2, 768)\n",
            "        (position_embeddings_visual): Embedding(512, 768)\n",
            "        (projection): Linear(in_features=2048, out_features=768, bias=True)\n",
            "      )\n",
            "      (encoder): BertEncoderJit(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayerJit(\n",
            "            (attention): BertAttentionJit(\n",
            "              (self): BertSelfAttentionJit(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.utils.general: \u001b[0mTotal Parameters: 112044290. Trained Parameters: 112044290\n",
            "\u001b[32m2022-08-03T16:45:58 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-03T16:52:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T16:56:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T16:56:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:00:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/3000, val/misogyny_memes/cross_entropy: 0.5589, val/total_loss: 0.5589, val/misogyny_memes/accuracy: 0.7090, val/misogyny_memes/binary_f1: 0.6540, val/misogyny_memes/roc_auc: 0.8252, num_updates: 50, epoch: 1, iterations: 50, max_updates: 3000, val_time: 08m 20s 627ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.825182\n",
            "\u001b[32m2022-08-03T17:06:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, train/misogyny_memes/cross_entropy: 0.4153, train/misogyny_memes/cross_entropy/avg: 0.4153, train/total_loss: 0.4153, train/total_loss/avg: 0.4153, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 3000, lr: 0.00005, ups: 0.29, time: 05m 43s 893ms, time_since_start: 21m 23s 826ms, eta: 02h 49m 32s 365ms\n",
            "\u001b[32m2022-08-03T17:06:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:06:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:06:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, val/misogyny_memes/cross_entropy: 0.4969, val/total_loss: 0.4969, val/misogyny_memes/accuracy: 0.7710, val/misogyny_memes/binary_f1: 0.8038, val/misogyny_memes/roc_auc: 0.8741, num_updates: 100, epoch: 1, iterations: 100, max_updates: 3000, val_time: 37s 385ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.874139\n",
            "\u001b[32m2022-08-03T17:12:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:12:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:12:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:13:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/3000, val/misogyny_memes/cross_entropy: 0.4276, val/total_loss: 0.4276, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8270, val/misogyny_memes/roc_auc: 0.9032, num_updates: 150, epoch: 1, iterations: 150, max_updates: 3000, val_time: 31s 306ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.903215\n",
            "\u001b[32m2022-08-03T17:19:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, train/misogyny_memes/cross_entropy: 0.3625, train/misogyny_memes/cross_entropy/avg: 0.3889, train/total_loss: 0.3625, train/total_loss/avg: 0.3889, max mem: 9201.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 3000, lr: 0.00005, ups: 0.29, time: 05m 47s 593ms, time_since_start: 33m 52s 793ms, eta: 02h 45m 27s 268ms\n",
            "\u001b[32m2022-08-03T17:19:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:19:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:19:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:19:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, val/misogyny_memes/cross_entropy: 0.4829, val/total_loss: 0.4829, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8173, val/misogyny_memes/roc_auc: 0.8962, num_updates: 200, epoch: 1, iterations: 200, max_updates: 3000, val_time: 25s 460ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.903215\n",
            "\u001b[32m2022-08-03T17:25:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:25:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:25:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:25:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/3000, val/misogyny_memes/cross_entropy: 0.4230, val/total_loss: 0.4230, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8172, val/misogyny_memes/roc_auc: 0.8932, num_updates: 250, epoch: 1, iterations: 250, max_updates: 3000, val_time: 21s 001ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.903215\n",
            "\u001b[32m2022-08-03T17:29:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, train/misogyny_memes/cross_entropy: 0.4153, train/misogyny_memes/cross_entropy/avg: 0.4727, train/total_loss: 0.4153, train/total_loss/avg: 0.4727, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 3000, lr: 0.00005, ups: 0.44, time: 03m 46s 890ms, time_since_start: 44m 09s 470ms, eta: 01h 44m 08s 565ms\n",
            "\u001b[32m2022-08-03T17:29:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:29:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:29:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:29:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, val/misogyny_memes/cross_entropy: 0.4488, val/total_loss: 0.4488, val/misogyny_memes/accuracy: 0.8330, val/misogyny_memes/binary_f1: 0.8408, val/misogyny_memes/roc_auc: 0.9102, num_updates: 300, epoch: 2, iterations: 300, max_updates: 3000, val_time: 28s 729ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.910187\n",
            "\u001b[32m2022-08-03T17:30:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:30:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:30:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:30:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/3000, val/misogyny_memes/cross_entropy: 0.4152, val/total_loss: 0.4152, val/misogyny_memes/accuracy: 0.8080, val/misogyny_memes/binary_f1: 0.8154, val/misogyny_memes/roc_auc: 0.8949, num_updates: 350, epoch: 2, iterations: 350, max_updates: 3000, val_time: 22s 505ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.910187\n",
            "\u001b[32m2022-08-03T17:31:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, train/misogyny_memes/cross_entropy: 0.4153, train/misogyny_memes/cross_entropy/avg: 0.4605, train/total_loss: 0.4153, train/total_loss/avg: 0.4605, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 262ms, time_since_start: 46m 19s 348ms, eta: 17m 21s 240ms\n",
            "\u001b[32m2022-08-03T17:31:31 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:31:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:31:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:31:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, val/misogyny_memes/cross_entropy: 0.4001, val/total_loss: 0.4001, val/misogyny_memes/accuracy: 0.8260, val/misogyny_memes/binary_f1: 0.8349, val/misogyny_memes/roc_auc: 0.9082, num_updates: 400, epoch: 2, iterations: 400, max_updates: 3000, val_time: 24s 001ms, best_update: 300, best_iteration: 300, best_val/misogyny_memes/roc_auc: 0.910187\n",
            "\u001b[32m2022-08-03T17:32:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:32:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:32:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:33:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/3000, val/misogyny_memes/cross_entropy: 0.4207, val/total_loss: 0.4207, val/misogyny_memes/accuracy: 0.8240, val/misogyny_memes/binary_f1: 0.8361, val/misogyny_memes/roc_auc: 0.9122, num_updates: 450, epoch: 2, iterations: 450, max_updates: 3000, val_time: 31s 736ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.912173\n",
            "\u001b[32m2022-08-03T17:33:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, train/misogyny_memes/cross_entropy: 0.4153, train/misogyny_memes/cross_entropy/avg: 0.4235, train/total_loss: 0.4153, train/total_loss/avg: 0.4235, max mem: 9201.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 492ms, time_since_start: 48m 33s 791ms, eta: 16m 47s 068ms\n",
            "\u001b[32m2022-08-03T17:33:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:33:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:33:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:34:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, val/misogyny_memes/cross_entropy: 0.4021, val/total_loss: 0.4021, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8241, val/misogyny_memes/roc_auc: 0.9141, num_updates: 500, epoch: 2, iterations: 500, max_updates: 3000, val_time: 31s 284ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.914114\n",
            "\u001b[32m2022-08-03T17:34:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:35:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:35:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:35:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/3000, val/misogyny_memes/cross_entropy: 0.4137, val/total_loss: 0.4137, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8468, val/misogyny_memes/roc_auc: 0.9180, num_updates: 550, epoch: 2, iterations: 550, max_updates: 3000, val_time: 35s 681ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.918037\n",
            "\u001b[32m2022-08-03T17:36:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, train/misogyny_memes/cross_entropy: 0.3625, train/misogyny_memes/cross_entropy/avg: 0.4111, train/total_loss: 0.3625, train/total_loss/avg: 0.4111, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 713ms, time_since_start: 50m 59s 857ms, eta: 16m 12s 197ms\n",
            "\u001b[32m2022-08-03T17:36:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:36:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:36:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:36:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, val/misogyny_memes/cross_entropy: 0.7108, val/total_loss: 0.7108, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8075, val/misogyny_memes/roc_auc: 0.8914, num_updates: 600, epoch: 3, iterations: 600, max_updates: 3000, val_time: 23s 863ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.918037\n",
            "\u001b[32m2022-08-03T17:37:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:37:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:37:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:37:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/3000, val/misogyny_memes/cross_entropy: 0.5351, val/total_loss: 0.5351, val/misogyny_memes/accuracy: 0.7970, val/misogyny_memes/binary_f1: 0.8258, val/misogyny_memes/roc_auc: 0.9032, num_updates: 650, epoch: 3, iterations: 650, max_updates: 3000, val_time: 24s 692ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.918037\n",
            "\u001b[32m2022-08-03T17:38:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, train/misogyny_memes/cross_entropy: 0.3625, train/misogyny_memes/cross_entropy/avg: 0.3870, train/total_loss: 0.3625, train/total_loss/avg: 0.3870, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 302ms, time_since_start: 53m 07s 124ms, eta: 15m 22s 043ms\n",
            "\u001b[32m2022-08-03T17:38:19 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:38:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:38:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:38:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, val/misogyny_memes/cross_entropy: 0.5016, val/total_loss: 0.5016, val/misogyny_memes/accuracy: 0.8300, val/misogyny_memes/binary_f1: 0.8286, val/misogyny_memes/roc_auc: 0.9190, num_updates: 700, epoch: 3, iterations: 700, max_updates: 3000, val_time: 35s 067ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:39:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:39:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:39:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:40:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/3000, val/misogyny_memes/cross_entropy: 0.5036, val/total_loss: 0.5036, val/misogyny_memes/accuracy: 0.8470, val/misogyny_memes/binary_f1: 0.8566, val/misogyny_memes/roc_auc: 0.9087, num_updates: 750, epoch: 3, iterations: 750, max_updates: 3000, val_time: 26s 159ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:40:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, train/misogyny_memes/cross_entropy: 0.3491, train/misogyny_memes/cross_entropy/avg: 0.3535, train/total_loss: 0.3491, train/total_loss/avg: 0.3535, max mem: 9201.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 164ms, time_since_start: 55m 27s 333ms, eta: 14m 38s 850ms\n",
            "\u001b[32m2022-08-03T17:40:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:40:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:40:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:41:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, val/misogyny_memes/cross_entropy: 0.5655, val/total_loss: 0.5655, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8405, val/misogyny_memes/roc_auc: 0.9063, num_updates: 800, epoch: 3, iterations: 800, max_updates: 3000, val_time: 24s 106ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:41:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:41:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:41:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:42:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/3000, val/misogyny_memes/cross_entropy: 0.5230, val/total_loss: 0.5230, val/misogyny_memes/accuracy: 0.8410, val/misogyny_memes/binary_f1: 0.8430, val/misogyny_memes/roc_auc: 0.9087, num_updates: 850, epoch: 4, iterations: 850, max_updates: 3000, val_time: 26s 134ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:42:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, train/misogyny_memes/cross_entropy: 0.3491, train/misogyny_memes/cross_entropy/avg: 0.3268, train/total_loss: 0.3491, train/total_loss/avg: 0.3268, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 653ms, time_since_start: 57m 36s 976ms, eta: 14m 09s 376ms\n",
            "\u001b[32m2022-08-03T17:42:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:42:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:42:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:43:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, val/misogyny_memes/cross_entropy: 0.6242, val/total_loss: 0.6242, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8470, val/misogyny_memes/roc_auc: 0.9093, num_updates: 900, epoch: 4, iterations: 900, max_updates: 3000, val_time: 27s 702ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:43:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:44:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:44:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:44:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/3000, val/misogyny_memes/cross_entropy: 0.7084, val/total_loss: 0.7084, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8382, val/misogyny_memes/roc_auc: 0.8996, num_updates: 950, epoch: 4, iterations: 950, max_updates: 3000, val_time: 23s 576ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:44:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:44:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:44:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:45:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, train/misogyny_memes/cross_entropy: 0.2756, train/misogyny_memes/cross_entropy/avg: 0.2971, train/total_loss: 0.2756, train/total_loss/avg: 0.2971, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 3000, lr: 0.00005, ups: 1.89, time: 53s 915ms, time_since_start: 01h 01s 595ms, eta: 18m 19s 880ms\n",
            "\u001b[32m2022-08-03T17:45:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:45:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:45:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:45:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, val/misogyny_memes/cross_entropy: 0.5705, val/total_loss: 0.5705, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8378, val/misogyny_memes/roc_auc: 0.9087, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 3000, val_time: 22s 540ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:46:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:46:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:46:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:46:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/3000, val/misogyny_memes/cross_entropy: 0.6180, val/total_loss: 0.6180, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8290, val/misogyny_memes/roc_auc: 0.9053, num_updates: 1050, epoch: 4, iterations: 1050, max_updates: 3000, val_time: 23s 028ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:47:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, train/misogyny_memes/cross_entropy: 0.2756, train/misogyny_memes/cross_entropy/avg: 0.2813, train/total_loss: 0.2756, train/total_loss/avg: 0.2813, max mem: 9201.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 391ms, time_since_start: 01h 02m 06s 090ms, eta: 12m 43s 412ms\n",
            "\u001b[32m2022-08-03T17:47:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:47:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:47:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:47:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, val/misogyny_memes/cross_entropy: 0.5744, val/total_loss: 0.5744, val/misogyny_memes/accuracy: 0.8210, val/misogyny_memes/binary_f1: 0.8186, val/misogyny_memes/roc_auc: 0.9089, num_updates: 1100, epoch: 4, iterations: 1100, max_updates: 3000, val_time: 27s 221ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:48:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:48:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:48:36 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:48:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/3000, val/misogyny_memes/cross_entropy: 0.6259, val/total_loss: 0.6259, val/misogyny_memes/accuracy: 0.8350, val/misogyny_memes/binary_f1: 0.8424, val/misogyny_memes/roc_auc: 0.9043, num_updates: 1150, epoch: 5, iterations: 1150, max_updates: 3000, val_time: 24s 005ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:49:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, train/misogyny_memes/cross_entropy: 0.2426, train/misogyny_memes/cross_entropy/avg: 0.2658, train/total_loss: 0.2426, train/total_loss/avg: 0.2658, max mem: 9201.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 615ms, time_since_start: 01h 04m 16s 983ms, eta: 12m 07s 332ms\n",
            "\u001b[32m2022-08-03T17:49:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:49:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:49:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:49:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, val/misogyny_memes/cross_entropy: 0.6805, val/total_loss: 0.6805, val/misogyny_memes/accuracy: 0.8390, val/misogyny_memes/binary_f1: 0.8456, val/misogyny_memes/roc_auc: 0.9059, num_updates: 1200, epoch: 5, iterations: 1200, max_updates: 3000, val_time: 20s 786ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:50:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:50:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:50:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:50:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/3000, val/misogyny_memes/cross_entropy: 0.6563, val/total_loss: 0.6563, val/misogyny_memes/accuracy: 0.8420, val/misogyny_memes/binary_f1: 0.8521, val/misogyny_memes/roc_auc: 0.9047, num_updates: 1250, epoch: 5, iterations: 1250, max_updates: 3000, val_time: 25s 535ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:51:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, train/misogyny_memes/cross_entropy: 0.2426, train/misogyny_memes/cross_entropy/avg: 0.2573, train/total_loss: 0.2426, train/total_loss/avg: 0.2573, max mem: 9201.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 536ms, time_since_start: 01h 06m 22s 273ms, eta: 11m 25s 562ms\n",
            "\u001b[32m2022-08-03T17:51:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:51:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:51:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:51:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, val/misogyny_memes/cross_entropy: 0.7409, val/total_loss: 0.7409, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8273, val/misogyny_memes/roc_auc: 0.9055, num_updates: 1300, epoch: 5, iterations: 1300, max_updates: 3000, val_time: 23s 109ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:52:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:52:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:52:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:53:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/3000, val/misogyny_memes/cross_entropy: 0.8585, val/total_loss: 0.8585, val/misogyny_memes/accuracy: 0.8420, val/misogyny_memes/binary_f1: 0.8532, val/misogyny_memes/roc_auc: 0.8926, num_updates: 1350, epoch: 5, iterations: 1350, max_updates: 3000, val_time: 25s 170ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:53:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, train/misogyny_memes/cross_entropy: 0.1552, train/misogyny_memes/cross_entropy/avg: 0.2446, train/total_loss: 0.1552, train/total_loss/avg: 0.2446, max mem: 9201.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 429ms, time_since_start: 01h 08m 29s 278ms, eta: 10m 43s 493ms\n",
            "\u001b[32m2022-08-03T17:53:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:53:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:53:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:54:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, val/misogyny_memes/cross_entropy: 0.7441, val/total_loss: 0.7441, val/misogyny_memes/accuracy: 0.8410, val/misogyny_memes/binary_f1: 0.8521, val/misogyny_memes/roc_auc: 0.8922, num_updates: 1400, epoch: 5, iterations: 1400, max_updates: 3000, val_time: 20s 601ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:54:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:54:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:54:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:55:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/3000, val/misogyny_memes/cross_entropy: 0.6402, val/total_loss: 0.6402, val/misogyny_memes/accuracy: 0.8380, val/misogyny_memes/binary_f1: 0.8503, val/misogyny_memes/roc_auc: 0.9065, num_updates: 1450, epoch: 6, iterations: 1450, max_updates: 3000, val_time: 21s 084ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:55:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, train/misogyny_memes/cross_entropy: 0.1552, train/misogyny_memes/cross_entropy/avg: 0.2297, train/total_loss: 0.1552, train/total_loss/avg: 0.2297, max mem: 9201.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 674ms, time_since_start: 01h 10m 30s 219ms, eta: 10m 07s 024ms\n",
            "\u001b[32m2022-08-03T17:55:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:55:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:55:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:56:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/misogyny_memes/cross_entropy: 0.8504, val/total_loss: 0.8504, val/misogyny_memes/accuracy: 0.8460, val/misogyny_memes/binary_f1: 0.8544, val/misogyny_memes/roc_auc: 0.8983, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 3000, val_time: 23s 668ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:56:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:56:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:56:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:57:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/3000, val/misogyny_memes/cross_entropy: 0.6870, val/total_loss: 0.6870, val/misogyny_memes/accuracy: 0.8470, val/misogyny_memes/binary_f1: 0.8496, val/misogyny_memes/roc_auc: 0.9139, num_updates: 1550, epoch: 6, iterations: 1550, max_updates: 3000, val_time: 21s 396ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:57:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, train/misogyny_memes/cross_entropy: 0.1227, train/misogyny_memes/cross_entropy/avg: 0.2164, train/total_loss: 0.1227, train/total_loss/avg: 0.2164, max mem: 9201.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 318ms, time_since_start: 01h 12m 34s 100ms, eta: 09m 21s 470ms\n",
            "\u001b[32m2022-08-03T17:57:45 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:57:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:57:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:58:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, val/misogyny_memes/cross_entropy: 0.7605, val/total_loss: 0.7605, val/misogyny_memes/accuracy: 0.8310, val/misogyny_memes/binary_f1: 0.8281, val/misogyny_memes/roc_auc: 0.8998, num_updates: 1600, epoch: 6, iterations: 1600, max_updates: 3000, val_time: 24s 937ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:58:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:59:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T17:59:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T17:59:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/3000, val/misogyny_memes/cross_entropy: 0.7315, val/total_loss: 0.7315, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8406, val/misogyny_memes/roc_auc: 0.9078, num_updates: 1650, epoch: 6, iterations: 1650, max_updates: 3000, val_time: 25s 659ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T17:59:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, train/misogyny_memes/cross_entropy: 0.1227, train/misogyny_memes/cross_entropy/avg: 0.2050, train/total_loss: 0.1227, train/total_loss/avg: 0.2050, max mem: 9201.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 375ms, time_since_start: 01h 14m 43s 642ms, eta: 08m 42s 121ms\n",
            "\u001b[32m2022-08-03T17:59:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:00:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:00:06 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:00:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, val/misogyny_memes/cross_entropy: 0.7148, val/total_loss: 0.7148, val/misogyny_memes/accuracy: 0.8460, val/misogyny_memes/binary_f1: 0.8536, val/misogyny_memes/roc_auc: 0.9029, num_updates: 1700, epoch: 7, iterations: 1700, max_updates: 3000, val_time: 21s 626ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:00:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:01:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:01:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:01:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/3000, val/misogyny_memes/cross_entropy: 1.0130, val/total_loss: 1.0130, val/misogyny_memes/accuracy: 0.8300, val/misogyny_memes/binary_f1: 0.8307, val/misogyny_memes/roc_auc: 0.8879, num_updates: 1750, epoch: 7, iterations: 1750, max_updates: 3000, val_time: 22s 943ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:01:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, train/misogyny_memes/cross_entropy: 0.1227, train/misogyny_memes/cross_entropy/avg: 0.2038, train/total_loss: 0.1227, train/total_loss/avg: 0.2038, max mem: 9201.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 334ms, time_since_start: 01h 16m 47s 338ms, eta: 08m 01s 460ms\n",
            "\u001b[32m2022-08-03T18:01:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:02:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:02:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:02:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, val/misogyny_memes/cross_entropy: 0.7162, val/total_loss: 0.7162, val/misogyny_memes/accuracy: 0.8310, val/misogyny_memes/binary_f1: 0.8288, val/misogyny_memes/roc_auc: 0.9026, num_updates: 1800, epoch: 7, iterations: 1800, max_updates: 3000, val_time: 26s 870ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:03:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:03:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:03:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:03:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/3000, val/misogyny_memes/cross_entropy: 0.8515, val/total_loss: 0.8515, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8214, val/misogyny_memes/roc_auc: 0.9019, num_updates: 1850, epoch: 7, iterations: 1850, max_updates: 3000, val_time: 25s 518ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:04:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, train/misogyny_memes/cross_entropy: 0.1227, train/misogyny_memes/cross_entropy/avg: 0.1990, train/total_loss: 0.1227, train/total_loss/avg: 0.1990, max mem: 9201.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 3000, lr: 0.00005, ups: 2.50, time: 40s 163ms, time_since_start: 01h 18m 59s 753ms, eta: 07m 30s 633ms\n",
            "\u001b[32m2022-08-03T18:04:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:04:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:04:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:04:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, val/misogyny_memes/cross_entropy: 0.7712, val/total_loss: 0.7712, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8303, val/misogyny_memes/roc_auc: 0.9088, num_updates: 1900, epoch: 7, iterations: 1900, max_updates: 3000, val_time: 23s 780ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:05:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:05:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:05:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:05:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/3000, val/misogyny_memes/cross_entropy: 0.7651, val/total_loss: 0.7651, val/misogyny_memes/accuracy: 0.8400, val/misogyny_memes/binary_f1: 0.8524, val/misogyny_memes/roc_auc: 0.9103, num_updates: 1950, epoch: 7, iterations: 1950, max_updates: 3000, val_time: 28s 233ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:06:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:06:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:06:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:06:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, train/misogyny_memes/cross_entropy: 0.1187, train/misogyny_memes/cross_entropy/avg: 0.1894, train/total_loss: 0.1187, train/total_loss/avg: 0.1894, max mem: 9201.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 3000, lr: 0.00005, ups: 1.79, time: 56s 108ms, time_since_start: 01h 21m 27s 549ms, eta: 09m 32s 310ms\n",
            "\u001b[32m2022-08-03T18:06:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:06:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:06:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:07:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, val/misogyny_memes/cross_entropy: 0.7448, val/total_loss: 0.7448, val/misogyny_memes/accuracy: 0.8380, val/misogyny_memes/binary_f1: 0.8514, val/misogyny_memes/roc_auc: 0.9133, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 3000, val_time: 22s 361ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:07:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:07:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:07:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:08:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2050/3000, val/misogyny_memes/cross_entropy: 0.6966, val/total_loss: 0.6966, val/misogyny_memes/accuracy: 0.8370, val/misogyny_memes/binary_f1: 0.8458, val/misogyny_memes/roc_auc: 0.9135, num_updates: 2050, epoch: 8, iterations: 2050, max_updates: 3000, val_time: 22s 803ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:08:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, train/misogyny_memes/cross_entropy: 0.1136, train/misogyny_memes/cross_entropy/avg: 0.1845, train/total_loss: 0.1136, train/total_loss/avg: 0.1845, max mem: 9201.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 750ms, time_since_start: 01h 23m 32s 002ms, eta: 06m 04s 913ms\n",
            "\u001b[32m2022-08-03T18:08:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:08:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:08:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:09:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, val/misogyny_memes/cross_entropy: 0.9048, val/total_loss: 0.9048, val/misogyny_memes/accuracy: 0.8380, val/misogyny_memes/binary_f1: 0.8436, val/misogyny_memes/roc_auc: 0.9095, num_updates: 2100, epoch: 8, iterations: 2100, max_updates: 3000, val_time: 24s 488ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:09:47 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:09:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:09:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:10:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2150/3000, val/misogyny_memes/cross_entropy: 0.8639, val/total_loss: 0.8639, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8432, val/misogyny_memes/roc_auc: 0.9087, num_updates: 2150, epoch: 8, iterations: 2150, max_updates: 3000, val_time: 24s 454ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:10:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, train/misogyny_memes/cross_entropy: 0.1136, train/misogyny_memes/cross_entropy/avg: 0.1765, train/total_loss: 0.1136, train/total_loss/avg: 0.1765, max mem: 9201.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 445ms, time_since_start: 01h 25m 39s 822ms, eta: 05m 21s 878ms\n",
            "\u001b[32m2022-08-03T18:10:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:11:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:11:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:11:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, val/misogyny_memes/cross_entropy: 0.7930, val/total_loss: 0.7930, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8220, val/misogyny_memes/roc_auc: 0.8983, num_updates: 2200, epoch: 8, iterations: 2200, max_updates: 3000, val_time: 22s 512ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:11:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:12:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:12:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:12:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2250/3000, val/misogyny_memes/cross_entropy: 0.6974, val/total_loss: 0.6974, val/misogyny_memes/accuracy: 0.8370, val/misogyny_memes/binary_f1: 0.8410, val/misogyny_memes/roc_auc: 0.9126, num_updates: 2250, epoch: 8, iterations: 2250, max_updates: 3000, val_time: 24s 895ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:12:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, train/misogyny_memes/cross_entropy: 0.0957, train/misogyny_memes/cross_entropy/avg: 0.1703, train/total_loss: 0.0957, train/total_loss/avg: 0.1703, max mem: 9201.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 635ms, time_since_start: 01h 27m 46s 473ms, eta: 04m 42s 999ms\n",
            "\u001b[32m2022-08-03T18:12:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:13:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:13:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:13:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, val/misogyny_memes/cross_entropy: 0.7232, val/total_loss: 0.7232, val/misogyny_memes/accuracy: 0.8310, val/misogyny_memes/binary_f1: 0.8380, val/misogyny_memes/roc_auc: 0.9081, num_updates: 2300, epoch: 9, iterations: 2300, max_updates: 3000, val_time: 24s 627ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:14:02 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:14:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:14:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:14:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2350/3000, val/misogyny_memes/cross_entropy: 0.8233, val/total_loss: 0.8233, val/misogyny_memes/accuracy: 0.8260, val/misogyny_memes/binary_f1: 0.8257, val/misogyny_memes/roc_auc: 0.9031, num_updates: 2350, epoch: 9, iterations: 2350, max_updates: 3000, val_time: 23s 390ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:15:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, train/misogyny_memes/cross_entropy: 0.0862, train/misogyny_memes/cross_entropy/avg: 0.1641, train/total_loss: 0.0862, train/total_loss/avg: 0.1641, max mem: 9201.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 346ms, time_since_start: 01h 29m 53s 549ms, eta: 04m 802ms\n",
            "\u001b[32m2022-08-03T18:15:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:15:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:15:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:15:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, val/misogyny_memes/cross_entropy: 0.9649, val/total_loss: 0.9649, val/misogyny_memes/accuracy: 0.8060, val/misogyny_memes/binary_f1: 0.7909, val/misogyny_memes/roc_auc: 0.8908, num_updates: 2400, epoch: 9, iterations: 2400, max_updates: 3000, val_time: 27s 300ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:16:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:16:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:16:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:16:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2450/3000, val/misogyny_memes/cross_entropy: 0.8405, val/total_loss: 0.8405, val/misogyny_memes/accuracy: 0.8210, val/misogyny_memes/binary_f1: 0.8141, val/misogyny_memes/roc_auc: 0.8970, num_updates: 2450, epoch: 9, iterations: 2450, max_updates: 3000, val_time: 22s 924ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:17:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, train/misogyny_memes/cross_entropy: 0.0794, train/misogyny_memes/cross_entropy/avg: 0.1598, train/total_loss: 0.0794, train/total_loss/avg: 0.1598, max mem: 9201.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 305ms, time_since_start: 01h 32m 02s 814ms, eta: 03m 20s 457ms\n",
            "\u001b[32m2022-08-03T18:17:14 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:17:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:17:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:17:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, val/misogyny_memes/cross_entropy: 0.6020, val/total_loss: 0.6020, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8303, val/misogyny_memes/roc_auc: 0.9060, num_updates: 2500, epoch: 9, iterations: 2500, max_updates: 3000, val_time: 23s 226ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:18:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:18:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:18:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:18:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2550/3000, val/misogyny_memes/cross_entropy: 0.9097, val/total_loss: 0.9097, val/misogyny_memes/accuracy: 0.8210, val/misogyny_memes/binary_f1: 0.8344, val/misogyny_memes/roc_auc: 0.9041, num_updates: 2550, epoch: 10, iterations: 2550, max_updates: 3000, val_time: 24s 376ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:19:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, train/misogyny_memes/cross_entropy: 0.0794, train/misogyny_memes/cross_entropy/avg: 0.1570, train/total_loss: 0.0794, train/total_loss/avg: 0.1570, max mem: 9201.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 637ms, time_since_start: 01h 34m 09s 602ms, eta: 02m 41s 719ms\n",
            "\u001b[32m2022-08-03T18:19:21 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:19:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:19:32 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:19:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, val/misogyny_memes/cross_entropy: 0.7002, val/total_loss: 0.7002, val/misogyny_memes/accuracy: 0.8310, val/misogyny_memes/binary_f1: 0.8361, val/misogyny_memes/roc_auc: 0.9042, num_updates: 2600, epoch: 10, iterations: 2600, max_updates: 3000, val_time: 23s 048ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:20:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:20:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:20:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:20:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2650/3000, val/misogyny_memes/cross_entropy: 0.9427, val/total_loss: 0.9427, val/misogyny_memes/accuracy: 0.8400, val/misogyny_memes/binary_f1: 0.8470, val/misogyny_memes/roc_auc: 0.9091, num_updates: 2650, epoch: 10, iterations: 2650, max_updates: 3000, val_time: 23s 943ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:21:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, train/misogyny_memes/cross_entropy: 0.0551, train/misogyny_memes/cross_entropy/avg: 0.1512, train/total_loss: 0.0551, train/total_loss/avg: 0.1512, max mem: 9201.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 565ms, time_since_start: 01h 36m 15s 457ms, eta: 02m 01s 070ms\n",
            "\u001b[32m2022-08-03T18:21:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:21:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:21:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:21:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, val/misogyny_memes/cross_entropy: 1.0317, val/total_loss: 1.0317, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8370, val/misogyny_memes/roc_auc: 0.9058, num_updates: 2700, epoch: 10, iterations: 2700, max_updates: 3000, val_time: 27s 255ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:22:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:22:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:22:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:22:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2750/3000, val/misogyny_memes/cross_entropy: 0.9008, val/total_loss: 0.9008, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8453, val/misogyny_memes/roc_auc: 0.9062, num_updates: 2750, epoch: 10, iterations: 2750, max_updates: 3000, val_time: 25s 749ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:23:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, train/misogyny_memes/cross_entropy: 0.0336, train/misogyny_memes/cross_entropy/avg: 0.1462, train/total_loss: 0.0336, train/total_loss/avg: 0.1462, max mem: 9201.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 613ms, time_since_start: 01h 38m 27s 389ms, eta: 01m 20s 810ms\n",
            "\u001b[32m2022-08-03T18:23:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:23:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:23:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:24:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, val/misogyny_memes/cross_entropy: 0.8544, val/total_loss: 0.8544, val/misogyny_memes/accuracy: 0.8440, val/misogyny_memes/binary_f1: 0.8564, val/misogyny_memes/roc_auc: 0.9089, num_updates: 2800, epoch: 10, iterations: 2800, max_updates: 3000, val_time: 23s 179ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:24:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:24:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:24:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:25:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2850/3000, val/misogyny_memes/cross_entropy: 0.9624, val/total_loss: 0.9624, val/misogyny_memes/accuracy: 0.8380, val/misogyny_memes/binary_f1: 0.8457, val/misogyny_memes/roc_auc: 0.9083, num_updates: 2850, epoch: 11, iterations: 2850, max_updates: 3000, val_time: 20s 834ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:25:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, train/misogyny_memes/cross_entropy: 0.0296, train/misogyny_memes/cross_entropy/avg: 0.1411, train/total_loss: 0.0296, train/total_loss/avg: 0.1411, max mem: 9201.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 3000, lr: 0.00005, ups: 2.56, time: 39s 495ms, time_since_start: 01h 40m 30s 480ms, eta: 40s 285ms\n",
            "\u001b[32m2022-08-03T18:25:42 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:25:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:25:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:26:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, val/misogyny_memes/cross_entropy: 1.0492, val/total_loss: 1.0492, val/misogyny_memes/accuracy: 0.8420, val/misogyny_memes/binary_f1: 0.8457, val/misogyny_memes/roc_auc: 0.9063, num_updates: 2900, epoch: 11, iterations: 2900, max_updates: 3000, val_time: 24s 332ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:26:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:26:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:26:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:27:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2950/3000, val/misogyny_memes/cross_entropy: 0.9747, val/total_loss: 0.9747, val/misogyny_memes/accuracy: 0.8340, val/misogyny_memes/binary_f1: 0.8494, val/misogyny_memes/roc_auc: 0.9051, num_updates: 2950, epoch: 11, iterations: 2950, max_updates: 3000, val_time: 20s 789ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:27:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:27:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:27:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:27:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, train/misogyny_memes/cross_entropy: 0.0227, train/misogyny_memes/cross_entropy/avg: 0.1365, train/total_loss: 0.0227, train/total_loss/avg: 0.1365, max mem: 9201.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 3000, lr: 0.00005, ups: 1.96, time: 51s 826ms, time_since_start: 01h 42m 46s 890ms, eta: 0ms\n",
            "\u001b[32m2022-08-03T18:27:58 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:28:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:28:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:28:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, val/misogyny_memes/cross_entropy: 0.7236, val/total_loss: 0.7236, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8433, val/misogyny_memes/roc_auc: 0.9141, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 3000, val_time: 22s 004ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.919033\n",
            "\u001b[32m2022-08-03T18:28:21 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-03T18:28:21 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-03T18:28:21 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-08-03T18:28:32 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-03T18:28:32 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 700\n",
            "\u001b[32m2022-08-03T18:28:32 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 700\n",
            "\u001b[32m2022-08-03T18:28:32 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-03T18:28:34 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:11<00:00,  2.84it/s]\n",
            "\u001b[32m2022-08-03T18:28:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, val/misogyny_memes/cross_entropy: 0.5016, val/total_loss: 0.5016, val/misogyny_memes/accuracy: 0.8300, val/misogyny_memes/binary_f1: 0.8286, val/misogyny_memes/roc_auc: 0.9190\n",
            "\u001b[32m2022-08-03T18:28:45 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 43m 33s 890ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "import torch, gc\n",
        "import os\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Yur0ue65i9cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vilbert 7"
      ],
      "metadata": {
        "id": "MPxAfVjJjKjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ViLBERT Masked Conceptual Captions\n",
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/vilbert/configs/misogyny_memes/from_cc.yaml\" \\\n",
        "        model=\"vilbert\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        checkpoint.resume_zoo=vilbert.pretrained.cc.full \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=3000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "        dataset_config.misogyny_memes.features.test[0]=$feats_dir \\\n",
        "        training.lr_ratio=0.6 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./vilbert_final \\\n",
        "        env.tensorboard_logdir=logs/fit/vilbert_final \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vM7t_44jNQN",
        "outputId": "70c7362f-c0db-4e0b-8cff-03e466029c66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/misogyny_memes/from_cc.yaml\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.pretrained.cc.full\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 3000\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.6\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./vilbert_final\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/vilbert_final\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf: \u001b[0mLogging to: ./vilbert_final/train.log\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/misogyny_memes/from_cc.yaml', 'model=vilbert', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'checkpoint.resume_zoo=vilbert.pretrained.cc.full', 'training.evaluation_interval=50', 'training.evaluation_interval=50', 'training.max_updates=3000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'training.lr_ratio=0.6', 'training.use_warmup=True', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./vilbert_final', 'env.tensorboard_logdir=logs/fit/vilbert_final'])\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf_cli.run: \u001b[0mUsing seed 11741867\n",
            "\u001b[32m2022-08-03T18:29:11 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-03T18:29:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-03T18:29:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-03T18:29:28 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-03T18:29:29 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/models/vilbert/vilbert.pretrained.cc_full.tar.gz to /root/.cache/torch/mmf/data/models/vilbert.pretrained.cc.full/vilbert.pretrained.cc_full.tar.gz ]\n",
            "Downloading vilbert.pretrained.cc_full.tar.gz: 100% 1.01G/1.01G [00:44<00:00, 22.7MB/s]\n",
            "[ Starting checksum for vilbert.pretrained.cc_full.tar.gz]\n",
            "[ Checksum successful for vilbert.pretrained.cc_full.tar.gz]\n",
            "Unpacking vilbert.pretrained.cc_full.tar.gz\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:30:29 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:30:29 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:30:29 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:30:29 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.weight from model.bert.v_embeddings.image_embeddings.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_embeddings.bias from model.bert.v_embeddings.image_embeddings.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.weight from model.bert.v_embeddings.image_location_embeddings.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.image_location_embeddings.bias from model.bert.v_embeddings.image_location_embeddings.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.weight from model.bert.v_embeddings.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_embeddings.LayerNorm.bias from model.bert.v_embeddings.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.weight from model.bert.encoder.v_layer.0.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.query.bias from model.bert.encoder.v_layer.0.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.weight from model.bert.encoder.v_layer.0.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.key.bias from model.bert.encoder.v_layer.0.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.weight from model.bert.encoder.v_layer.0.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.self.value.bias from model.bert.encoder.v_layer.0.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.weight from model.bert.encoder.v_layer.0.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.dense.bias from model.bert.encoder.v_layer.0.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.0.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.0.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.weight from model.bert.encoder.v_layer.0.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.intermediate.dense.bias from model.bert.encoder.v_layer.0.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.weight from model.bert.encoder.v_layer.0.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.dense.bias from model.bert.encoder.v_layer.0.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.weight from model.bert.encoder.v_layer.0.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.0.output.LayerNorm.bias from model.bert.encoder.v_layer.0.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.weight from model.bert.encoder.v_layer.1.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.query.bias from model.bert.encoder.v_layer.1.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.weight from model.bert.encoder.v_layer.1.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.key.bias from model.bert.encoder.v_layer.1.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.weight from model.bert.encoder.v_layer.1.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.self.value.bias from model.bert.encoder.v_layer.1.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.weight from model.bert.encoder.v_layer.1.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.dense.bias from model.bert.encoder.v_layer.1.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.1.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.1.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.weight from model.bert.encoder.v_layer.1.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.intermediate.dense.bias from model.bert.encoder.v_layer.1.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.weight from model.bert.encoder.v_layer.1.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.dense.bias from model.bert.encoder.v_layer.1.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.weight from model.bert.encoder.v_layer.1.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.1.output.LayerNorm.bias from model.bert.encoder.v_layer.1.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.weight from model.bert.encoder.v_layer.2.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.query.bias from model.bert.encoder.v_layer.2.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.weight from model.bert.encoder.v_layer.2.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.key.bias from model.bert.encoder.v_layer.2.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.weight from model.bert.encoder.v_layer.2.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.self.value.bias from model.bert.encoder.v_layer.2.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.weight from model.bert.encoder.v_layer.2.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.dense.bias from model.bert.encoder.v_layer.2.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.2.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.2.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.weight from model.bert.encoder.v_layer.2.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.intermediate.dense.bias from model.bert.encoder.v_layer.2.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.weight from model.bert.encoder.v_layer.2.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.dense.bias from model.bert.encoder.v_layer.2.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.weight from model.bert.encoder.v_layer.2.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.2.output.LayerNorm.bias from model.bert.encoder.v_layer.2.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.weight from model.bert.encoder.v_layer.3.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.query.bias from model.bert.encoder.v_layer.3.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.weight from model.bert.encoder.v_layer.3.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.key.bias from model.bert.encoder.v_layer.3.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.weight from model.bert.encoder.v_layer.3.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.self.value.bias from model.bert.encoder.v_layer.3.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.weight from model.bert.encoder.v_layer.3.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.dense.bias from model.bert.encoder.v_layer.3.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.3.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.3.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.weight from model.bert.encoder.v_layer.3.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.intermediate.dense.bias from model.bert.encoder.v_layer.3.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.weight from model.bert.encoder.v_layer.3.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.dense.bias from model.bert.encoder.v_layer.3.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.weight from model.bert.encoder.v_layer.3.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.3.output.LayerNorm.bias from model.bert.encoder.v_layer.3.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.weight from model.bert.encoder.v_layer.4.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.query.bias from model.bert.encoder.v_layer.4.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.weight from model.bert.encoder.v_layer.4.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.key.bias from model.bert.encoder.v_layer.4.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.weight from model.bert.encoder.v_layer.4.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.self.value.bias from model.bert.encoder.v_layer.4.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.weight from model.bert.encoder.v_layer.4.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.dense.bias from model.bert.encoder.v_layer.4.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.4.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.4.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.weight from model.bert.encoder.v_layer.4.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.intermediate.dense.bias from model.bert.encoder.v_layer.4.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.weight from model.bert.encoder.v_layer.4.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.dense.bias from model.bert.encoder.v_layer.4.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.weight from model.bert.encoder.v_layer.4.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.4.output.LayerNorm.bias from model.bert.encoder.v_layer.4.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.weight from model.bert.encoder.v_layer.5.attention.self.query.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.query.bias from model.bert.encoder.v_layer.5.attention.self.query.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.weight from model.bert.encoder.v_layer.5.attention.self.key.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.key.bias from model.bert.encoder.v_layer.5.attention.self.key.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.weight from model.bert.encoder.v_layer.5.attention.self.value.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.self.value.bias from model.bert.encoder.v_layer.5.attention.self.value.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.weight from model.bert.encoder.v_layer.5.attention.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.dense.bias from model.bert.encoder.v_layer.5.attention.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight from model.bert.encoder.v_layer.5.attention.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias from model.bert.encoder.v_layer.5.attention.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.weight from model.bert.encoder.v_layer.5.intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.intermediate.dense.bias from model.bert.encoder.v_layer.5.intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.weight from model.bert.encoder.v_layer.5.output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.dense.bias from model.bert.encoder.v_layer.5.output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.weight from model.bert.encoder.v_layer.5.output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.v_layer.5.output.LayerNorm.bias from model.bert.encoder.v_layer.5.output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.weight from model.bert.encoder.c_layer.0.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query1.bias from model.bert.encoder.c_layer.0.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.weight from model.bert.encoder.c_layer.0.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key1.bias from model.bert.encoder.c_layer.0.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.weight from model.bert.encoder.c_layer.0.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value1.bias from model.bert.encoder.c_layer.0.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.weight from model.bert.encoder.c_layer.0.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.query2.bias from model.bert.encoder.c_layer.0.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.weight from model.bert.encoder.c_layer.0.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.key2.bias from model.bert.encoder.c_layer.0.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.weight from model.bert.encoder.c_layer.0.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biattention.value2.bias from model.bert.encoder.c_layer.0.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.weight from model.bert.encoder.c_layer.0.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense1.bias from model.bert.encoder.c_layer.0.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.weight from model.bert.encoder.c_layer.0.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense1.bias from model.bert.encoder.c_layer.0.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.weight from model.bert.encoder.c_layer.0.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.dense2.bias from model.bert.encoder.c_layer.0.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.0.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.weight from model.bert.encoder.c_layer.0.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.biOutput.q_dense2.bias from model.bert.encoder.c_layer.0.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.weight from model.bert.encoder.c_layer.0.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_intermediate.dense.bias from model.bert.encoder.c_layer.0.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.weight from model.bert.encoder.c_layer.0.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.dense.bias from model.bert.encoder.c_layer.0.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.weight from model.bert.encoder.c_layer.0.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.v_output.LayerNorm.bias from model.bert.encoder.c_layer.0.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.weight from model.bert.encoder.c_layer.0.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_intermediate.dense.bias from model.bert.encoder.c_layer.0.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.weight from model.bert.encoder.c_layer.0.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.dense.bias from model.bert.encoder.c_layer.0.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.weight from model.bert.encoder.c_layer.0.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.0.t_output.LayerNorm.bias from model.bert.encoder.c_layer.0.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.weight from model.bert.encoder.c_layer.1.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query1.bias from model.bert.encoder.c_layer.1.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.weight from model.bert.encoder.c_layer.1.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key1.bias from model.bert.encoder.c_layer.1.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.weight from model.bert.encoder.c_layer.1.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value1.bias from model.bert.encoder.c_layer.1.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.weight from model.bert.encoder.c_layer.1.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.query2.bias from model.bert.encoder.c_layer.1.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.weight from model.bert.encoder.c_layer.1.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.key2.bias from model.bert.encoder.c_layer.1.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.weight from model.bert.encoder.c_layer.1.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biattention.value2.bias from model.bert.encoder.c_layer.1.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.weight from model.bert.encoder.c_layer.1.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense1.bias from model.bert.encoder.c_layer.1.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.weight from model.bert.encoder.c_layer.1.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense1.bias from model.bert.encoder.c_layer.1.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.weight from model.bert.encoder.c_layer.1.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.dense2.bias from model.bert.encoder.c_layer.1.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.1.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.weight from model.bert.encoder.c_layer.1.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.biOutput.q_dense2.bias from model.bert.encoder.c_layer.1.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.weight from model.bert.encoder.c_layer.1.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_intermediate.dense.bias from model.bert.encoder.c_layer.1.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.weight from model.bert.encoder.c_layer.1.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.dense.bias from model.bert.encoder.c_layer.1.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.weight from model.bert.encoder.c_layer.1.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.v_output.LayerNorm.bias from model.bert.encoder.c_layer.1.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.weight from model.bert.encoder.c_layer.1.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_intermediate.dense.bias from model.bert.encoder.c_layer.1.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.weight from model.bert.encoder.c_layer.1.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.dense.bias from model.bert.encoder.c_layer.1.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.weight from model.bert.encoder.c_layer.1.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.1.t_output.LayerNorm.bias from model.bert.encoder.c_layer.1.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.weight from model.bert.encoder.c_layer.2.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query1.bias from model.bert.encoder.c_layer.2.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.weight from model.bert.encoder.c_layer.2.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key1.bias from model.bert.encoder.c_layer.2.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.weight from model.bert.encoder.c_layer.2.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value1.bias from model.bert.encoder.c_layer.2.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.weight from model.bert.encoder.c_layer.2.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.query2.bias from model.bert.encoder.c_layer.2.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.weight from model.bert.encoder.c_layer.2.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.key2.bias from model.bert.encoder.c_layer.2.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.weight from model.bert.encoder.c_layer.2.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biattention.value2.bias from model.bert.encoder.c_layer.2.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.weight from model.bert.encoder.c_layer.2.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense1.bias from model.bert.encoder.c_layer.2.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.weight from model.bert.encoder.c_layer.2.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense1.bias from model.bert.encoder.c_layer.2.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.weight from model.bert.encoder.c_layer.2.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.dense2.bias from model.bert.encoder.c_layer.2.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.2.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.weight from model.bert.encoder.c_layer.2.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.biOutput.q_dense2.bias from model.bert.encoder.c_layer.2.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.weight from model.bert.encoder.c_layer.2.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_intermediate.dense.bias from model.bert.encoder.c_layer.2.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.weight from model.bert.encoder.c_layer.2.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.dense.bias from model.bert.encoder.c_layer.2.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.weight from model.bert.encoder.c_layer.2.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.v_output.LayerNorm.bias from model.bert.encoder.c_layer.2.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.weight from model.bert.encoder.c_layer.2.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_intermediate.dense.bias from model.bert.encoder.c_layer.2.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.weight from model.bert.encoder.c_layer.2.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.dense.bias from model.bert.encoder.c_layer.2.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.weight from model.bert.encoder.c_layer.2.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.2.t_output.LayerNorm.bias from model.bert.encoder.c_layer.2.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.weight from model.bert.encoder.c_layer.3.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query1.bias from model.bert.encoder.c_layer.3.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.weight from model.bert.encoder.c_layer.3.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key1.bias from model.bert.encoder.c_layer.3.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.weight from model.bert.encoder.c_layer.3.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value1.bias from model.bert.encoder.c_layer.3.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.weight from model.bert.encoder.c_layer.3.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.query2.bias from model.bert.encoder.c_layer.3.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.weight from model.bert.encoder.c_layer.3.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.key2.bias from model.bert.encoder.c_layer.3.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.weight from model.bert.encoder.c_layer.3.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biattention.value2.bias from model.bert.encoder.c_layer.3.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.weight from model.bert.encoder.c_layer.3.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense1.bias from model.bert.encoder.c_layer.3.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.weight from model.bert.encoder.c_layer.3.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense1.bias from model.bert.encoder.c_layer.3.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.weight from model.bert.encoder.c_layer.3.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.dense2.bias from model.bert.encoder.c_layer.3.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.3.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.weight from model.bert.encoder.c_layer.3.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.biOutput.q_dense2.bias from model.bert.encoder.c_layer.3.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.weight from model.bert.encoder.c_layer.3.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_intermediate.dense.bias from model.bert.encoder.c_layer.3.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.weight from model.bert.encoder.c_layer.3.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.dense.bias from model.bert.encoder.c_layer.3.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.weight from model.bert.encoder.c_layer.3.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.v_output.LayerNorm.bias from model.bert.encoder.c_layer.3.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.weight from model.bert.encoder.c_layer.3.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_intermediate.dense.bias from model.bert.encoder.c_layer.3.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.weight from model.bert.encoder.c_layer.3.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.dense.bias from model.bert.encoder.c_layer.3.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.weight from model.bert.encoder.c_layer.3.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.3.t_output.LayerNorm.bias from model.bert.encoder.c_layer.3.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.weight from model.bert.encoder.c_layer.4.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query1.bias from model.bert.encoder.c_layer.4.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.weight from model.bert.encoder.c_layer.4.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key1.bias from model.bert.encoder.c_layer.4.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.weight from model.bert.encoder.c_layer.4.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value1.bias from model.bert.encoder.c_layer.4.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.weight from model.bert.encoder.c_layer.4.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.query2.bias from model.bert.encoder.c_layer.4.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.weight from model.bert.encoder.c_layer.4.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.key2.bias from model.bert.encoder.c_layer.4.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.weight from model.bert.encoder.c_layer.4.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biattention.value2.bias from model.bert.encoder.c_layer.4.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.weight from model.bert.encoder.c_layer.4.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense1.bias from model.bert.encoder.c_layer.4.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.weight from model.bert.encoder.c_layer.4.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense1.bias from model.bert.encoder.c_layer.4.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.weight from model.bert.encoder.c_layer.4.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.dense2.bias from model.bert.encoder.c_layer.4.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.4.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.weight from model.bert.encoder.c_layer.4.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.biOutput.q_dense2.bias from model.bert.encoder.c_layer.4.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.weight from model.bert.encoder.c_layer.4.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_intermediate.dense.bias from model.bert.encoder.c_layer.4.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.weight from model.bert.encoder.c_layer.4.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.dense.bias from model.bert.encoder.c_layer.4.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.weight from model.bert.encoder.c_layer.4.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.v_output.LayerNorm.bias from model.bert.encoder.c_layer.4.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.weight from model.bert.encoder.c_layer.4.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_intermediate.dense.bias from model.bert.encoder.c_layer.4.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.weight from model.bert.encoder.c_layer.4.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.dense.bias from model.bert.encoder.c_layer.4.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.weight from model.bert.encoder.c_layer.4.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.4.t_output.LayerNorm.bias from model.bert.encoder.c_layer.4.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.weight from model.bert.encoder.c_layer.5.biattention.query1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query1.bias from model.bert.encoder.c_layer.5.biattention.query1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.weight from model.bert.encoder.c_layer.5.biattention.key1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key1.bias from model.bert.encoder.c_layer.5.biattention.key1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.weight from model.bert.encoder.c_layer.5.biattention.value1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value1.bias from model.bert.encoder.c_layer.5.biattention.value1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.weight from model.bert.encoder.c_layer.5.biattention.query2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.query2.bias from model.bert.encoder.c_layer.5.biattention.query2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.weight from model.bert.encoder.c_layer.5.biattention.key2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.key2.bias from model.bert.encoder.c_layer.5.biattention.key2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.weight from model.bert.encoder.c_layer.5.biattention.value2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biattention.value2.bias from model.bert.encoder.c_layer.5.biattention.value2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.weight from model.bert.encoder.c_layer.5.biOutput.dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense1.bias from model.bert.encoder.c_layer.5.biOutput.dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.weight from model.bert.encoder.c_layer.5.biOutput.q_dense1.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense1.bias from model.bert.encoder.c_layer.5.biOutput.q_dense1.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.weight from model.bert.encoder.c_layer.5.biOutput.dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.dense2.bias from model.bert.encoder.c_layer.5.biOutput.dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias from model.bert.encoder.c_layer.5.biOutput.LayerNorm2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.weight from model.bert.encoder.c_layer.5.biOutput.q_dense2.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.biOutput.q_dense2.bias from model.bert.encoder.c_layer.5.biOutput.q_dense2.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.weight from model.bert.encoder.c_layer.5.v_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_intermediate.dense.bias from model.bert.encoder.c_layer.5.v_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.weight from model.bert.encoder.c_layer.5.v_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.dense.bias from model.bert.encoder.c_layer.5.v_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.weight from model.bert.encoder.c_layer.5.v_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.v_output.LayerNorm.bias from model.bert.encoder.c_layer.5.v_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.weight from model.bert.encoder.c_layer.5.t_intermediate.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_intermediate.dense.bias from model.bert.encoder.c_layer.5.t_intermediate.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.weight from model.bert.encoder.c_layer.5.t_output.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.dense.bias from model.bert.encoder.c_layer.5.t_output.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.weight from model.bert.encoder.c_layer.5.t_output.LayerNorm.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.c_layer.5.t_output.LayerNorm.bias from model.bert.encoder.c_layer.5.t_output.LayerNorm.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.weight from model.bert.t_pooler.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.t_pooler.dense.bias from model.bert.t_pooler.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.weight from model.bert.v_pooler.dense.weight\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.v_pooler.dense.bias from model.bert.v_pooler.dense.bias\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.trainers.mmf_trainer: \u001b[0mViLBERT(\n",
            "  (model): ViLBERTForClassification(\n",
            "    (bert): ViLBERTBase(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (v_embeddings): BertImageFeatureEmbeddings(\n",
            "        (image_embeddings): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "        (image_location_embeddings): Linear(in_features=5, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (v_layer): ModuleList(\n",
            "          (0): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertImageLayer(\n",
            "            (attention): BertImageAttention(\n",
            "              (self): BertImageSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertImageSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (c_layer): ModuleList(\n",
            "          (0): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertConnectionLayer(\n",
            "            (biattention): BertBiAttention(\n",
            "              (query1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (key1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (value1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (query2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (key2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (value2): Linear(in_features=768, out_features=1024, bias=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (biOutput): BertBiOutput(\n",
            "              (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm1): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (q_dropout1): Dropout(p=0.1, inplace=False)\n",
            "              (dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (LayerNorm2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout2): Dropout(p=0.1, inplace=False)\n",
            "              (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n",
            "              (q_dropout2): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (v_intermediate): BertImageIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (v_output): BertImageOutput(\n",
            "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (t_intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (t_output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (t_pooler): BertTextPooler(\n",
            "        (dense): Linear(in_features=768, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (v_pooler): BertImagePooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=1024, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.utils.general: \u001b[0mTotal Parameters: 247780354. Trained Parameters: 247780354\n",
            "\u001b[32m2022-08-03T18:30:29 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-03T18:31:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:31:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:31:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:31:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:31:31 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:32:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/3000, val/misogyny_memes/cross_entropy: 0.6658, val/total_loss: 0.6658, val/misogyny_memes/accuracy: 0.5760, val/misogyny_memes/binary_f1: 0.5556, val/misogyny_memes/roc_auc: 0.6275, num_updates: 50, epoch: 1, iterations: 50, max_updates: 3000, val_time: 54s 694ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.627544\n",
            "\u001b[32m2022-08-03T18:33:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, train/misogyny_memes/cross_entropy: 0.6471, train/misogyny_memes/cross_entropy/avg: 0.6471, train/total_loss: 0.6471, train/total_loss/avg: 0.6471, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 3000, lr: 0.00001, ups: 2.04, time: 49s 649ms, time_since_start: 03m 34s 281ms, eta: 24m 28s 626ms\n",
            "\u001b[32m2022-08-03T18:33:03 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:33:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:33:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:33:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:33:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:34:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, val/misogyny_memes/cross_entropy: 0.5882, val/total_loss: 0.5882, val/misogyny_memes/accuracy: 0.6830, val/misogyny_memes/binary_f1: 0.6688, val/misogyny_memes/roc_auc: 0.7509, num_updates: 100, epoch: 1, iterations: 100, max_updates: 3000, val_time: 57s 763ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.750864\n",
            "\u001b[32m2022-08-03T18:35:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:35:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:35:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:35:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:35:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:36:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/3000, val/misogyny_memes/cross_entropy: 0.4926, val/total_loss: 0.4926, val/misogyny_memes/accuracy: 0.7620, val/misogyny_memes/binary_f1: 0.7601, val/misogyny_memes/roc_auc: 0.8496, num_updates: 150, epoch: 1, iterations: 150, max_updates: 3000, val_time: 01m 11s 837ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.849586\n",
            "\u001b[32m2022-08-03T18:37:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, train/misogyny_memes/cross_entropy: 0.4455, train/misogyny_memes/cross_entropy/avg: 0.5463, train/total_loss: 0.4455, train/total_loss/avg: 0.5463, max mem: 10794.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 3000, lr: 0.00002, ups: 1.28, time: 01m 18s 410ms, time_since_start: 08m 14s 058ms, eta: 37m 19s 418ms\n",
            "\u001b[32m2022-08-03T18:37:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:37:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:37:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:37:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:37:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:38:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, val/misogyny_memes/cross_entropy: 0.4683, val/total_loss: 0.4683, val/misogyny_memes/accuracy: 0.7870, val/misogyny_memes/binary_f1: 0.7922, val/misogyny_memes/roc_auc: 0.8646, num_updates: 200, epoch: 1, iterations: 200, max_updates: 3000, val_time: 01m 11s 385ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.864588\n",
            "\u001b[32m2022-08-03T18:40:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:40:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:40:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:40:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:40:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:41:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/3000, val/misogyny_memes/cross_entropy: 0.4431, val/total_loss: 0.4431, val/misogyny_memes/accuracy: 0.8010, val/misogyny_memes/binary_f1: 0.7984, val/misogyny_memes/roc_auc: 0.8818, num_updates: 250, epoch: 1, iterations: 250, max_updates: 3000, val_time: 01m 09s 721ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.881768\n",
            "\u001b[32m2022-08-03T18:42:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, train/misogyny_memes/cross_entropy: 0.4455, train/misogyny_memes/cross_entropy/avg: 0.5084, train/total_loss: 0.4455, train/total_loss/avg: 0.5084, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 3000, lr: 0.00003, ups: 1.28, time: 01m 18s 813ms, time_since_start: 13m 19s 650ms, eta: 36m 10s 535ms\n",
            "\u001b[32m2022-08-03T18:42:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:43:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:43:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:43:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:43:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:43:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, val/misogyny_memes/cross_entropy: 0.4769, val/total_loss: 0.4769, val/misogyny_memes/accuracy: 0.7970, val/misogyny_memes/binary_f1: 0.8146, val/misogyny_memes/roc_auc: 0.8793, num_updates: 300, epoch: 2, iterations: 300, max_updates: 3000, val_time: 47s 705ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.881768\n",
            "\u001b[32m2022-08-03T18:44:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:44:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:44:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:44:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:44:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:45:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/3000, val/misogyny_memes/cross_entropy: 0.4430, val/total_loss: 0.4430, val/misogyny_memes/accuracy: 0.8030, val/misogyny_memes/binary_f1: 0.7979, val/misogyny_memes/roc_auc: 0.8944, num_updates: 350, epoch: 2, iterations: 350, max_updates: 3000, val_time: 59s 557ms, best_update: 350, best_iteration: 350, best_val/misogyny_memes/roc_auc: 0.894357\n",
            "\u001b[32m2022-08-03T18:46:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, train/misogyny_memes/cross_entropy: 0.4326, train/misogyny_memes/cross_entropy/avg: 0.4677, train/total_loss: 0.4326, train/total_loss/avg: 0.4677, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 3000, lr: 0.00004, ups: 2.08, time: 48s 395ms, time_since_start: 16m 44s 267ms, eta: 21m 23s 462ms\n",
            "\u001b[32m2022-08-03T18:46:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:46:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:46:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:46:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:46:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:47:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, val/misogyny_memes/cross_entropy: 0.4667, val/total_loss: 0.4667, val/misogyny_memes/accuracy: 0.8010, val/misogyny_memes/binary_f1: 0.7872, val/misogyny_memes/roc_auc: 0.8992, num_updates: 400, epoch: 2, iterations: 400, max_updates: 3000, val_time: 01m 12s 111ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.899192\n",
            "\u001b[32m2022-08-03T18:48:13 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:48:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:48:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:48:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:48:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:49:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/3000, val/misogyny_memes/cross_entropy: 0.4011, val/total_loss: 0.4011, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8242, val/misogyny_memes/roc_auc: 0.9005, num_updates: 450, epoch: 2, iterations: 450, max_updates: 3000, val_time: 01m 07s 905ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.900493\n",
            "\u001b[32m2022-08-03T18:50:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, train/misogyny_memes/cross_entropy: 0.4326, train/misogyny_memes/cross_entropy/avg: 0.4328, train/total_loss: 0.4326, train/total_loss/avg: 0.4328, max mem: 10794.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 3000, lr: 0.00005, ups: 2.04, time: 49s 313ms, time_since_start: 20m 41s 955ms, eta: 20m 57s 499ms\n",
            "\u001b[32m2022-08-03T18:50:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:50:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:50:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:50:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:50:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:50:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, val/misogyny_memes/cross_entropy: 0.4116, val/total_loss: 0.4116, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8402, val/misogyny_memes/roc_auc: 0.9003, num_updates: 500, epoch: 2, iterations: 500, max_updates: 3000, val_time: 44s 159ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.900493\n",
            "\u001b[32m2022-08-03T18:52:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:52:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:52:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:52:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:52:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:53:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/3000, val/misogyny_memes/cross_entropy: 0.4488, val/total_loss: 0.4488, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8164, val/misogyny_memes/roc_auc: 0.8963, num_updates: 550, epoch: 2, iterations: 550, max_updates: 3000, val_time: 40s 763ms, best_update: 450, best_iteration: 450, best_val/misogyny_memes/roc_auc: 0.900493\n",
            "\u001b[32m2022-08-03T18:54:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, train/misogyny_memes/cross_entropy: 0.3456, train/misogyny_memes/cross_entropy/avg: 0.4055, train/total_loss: 0.3456, train/total_loss/avg: 0.4055, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 3000, lr: 0.00005, ups: 1.43, time: 01m 10s 441ms, time_since_start: 25m 969ms, eta: 28m 44s 413ms\n",
            "\u001b[32m2022-08-03T18:54:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:54:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:54:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:54:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:54:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:55:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, val/misogyny_memes/cross_entropy: 0.4607, val/total_loss: 0.4607, val/misogyny_memes/accuracy: 0.8380, val/misogyny_memes/binary_f1: 0.8399, val/misogyny_memes/roc_auc: 0.9061, num_updates: 600, epoch: 3, iterations: 600, max_updates: 3000, val_time: 01m 09s 398ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.906105\n",
            "\u001b[32m2022-08-03T18:56:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:56:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:56:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:56:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:56:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:57:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/3000, val/misogyny_memes/cross_entropy: 0.4404, val/total_loss: 0.4404, val/misogyny_memes/accuracy: 0.8260, val/misogyny_memes/binary_f1: 0.8349, val/misogyny_memes/roc_auc: 0.8973, num_updates: 650, epoch: 3, iterations: 650, max_updates: 3000, val_time: 40s 907ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.906105\n",
            "\u001b[32m2022-08-03T18:58:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, train/misogyny_memes/cross_entropy: 0.3456, train/misogyny_memes/cross_entropy/avg: 0.3682, train/total_loss: 0.3456, train/total_loss/avg: 0.3682, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 3000, lr: 0.00005, ups: 1.19, time: 01m 24s 442ms, time_since_start: 29m 20s 509ms, eta: 33m 01s 014ms\n",
            "\u001b[32m2022-08-03T18:58:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:59:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:59:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:59:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T18:59:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T18:59:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, val/misogyny_memes/cross_entropy: 0.6124, val/total_loss: 0.6124, val/misogyny_memes/accuracy: 0.7930, val/misogyny_memes/binary_f1: 0.8214, val/misogyny_memes/roc_auc: 0.8881, num_updates: 700, epoch: 3, iterations: 700, max_updates: 3000, val_time: 43s 192ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.906105\n",
            "\u001b[32m2022-08-03T19:01:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:01:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:01:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:01:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:01:59 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:02:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/3000, val/misogyny_memes/cross_entropy: 0.4590, val/total_loss: 0.4590, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8447, val/misogyny_memes/roc_auc: 0.8978, num_updates: 750, epoch: 3, iterations: 750, max_updates: 3000, val_time: 46s 961ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.906105\n",
            "\u001b[32m2022-08-03T19:06:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, train/misogyny_memes/cross_entropy: 0.3456, train/misogyny_memes/cross_entropy/avg: 0.3783, train/total_loss: 0.3456, train/total_loss/avg: 0.3783, max mem: 10794.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 3000, lr: 0.00005, ups: 0.45, time: 03m 41s 527ms, time_since_start: 36m 46s 261ms, eta: 01h 22m 51s 070ms\n",
            "\u001b[32m2022-08-03T19:06:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:06:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:06:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:06:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:06:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:07:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, val/misogyny_memes/cross_entropy: 0.4545, val/total_loss: 0.4545, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8153, val/misogyny_memes/roc_auc: 0.9005, num_updates: 800, epoch: 3, iterations: 800, max_updates: 3000, val_time: 48s 928ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.906105\n",
            "\u001b[32m2022-08-03T19:10:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:10:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:10:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:10:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:10:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:11:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/3000, val/misogyny_memes/cross_entropy: 0.4277, val/total_loss: 0.4277, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8407, val/misogyny_memes/roc_auc: 0.9101, num_updates: 850, epoch: 4, iterations: 850, max_updates: 3000, val_time: 51s 482ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.910099\n",
            "\u001b[32m2022-08-03T19:13:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, train/misogyny_memes/cross_entropy: 0.3456, train/misogyny_memes/cross_entropy/avg: 0.3543, train/total_loss: 0.3456, train/total_loss/avg: 0.3543, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 3000, lr: 0.00005, ups: 0.86, time: 01m 56s 209ms, time_since_start: 43m 58s 077ms, eta: 41m 29s 213ms\n",
            "\u001b[32m2022-08-03T19:13:27 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:13:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:13:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:13:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:13:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:14:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, val/misogyny_memes/cross_entropy: 0.5368, val/total_loss: 0.5368, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8453, val/misogyny_memes/roc_auc: 0.9007, num_updates: 900, epoch: 4, iterations: 900, max_updates: 3000, val_time: 43s 586ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.910099\n",
            "\u001b[32m2022-08-03T19:16:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:17:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:17:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:17:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:17:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:17:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/3000, val/misogyny_memes/cross_entropy: 0.5728, val/total_loss: 0.5728, val/misogyny_memes/accuracy: 0.8430, val/misogyny_memes/binary_f1: 0.8515, val/misogyny_memes/roc_auc: 0.9041, num_updates: 950, epoch: 4, iterations: 950, max_updates: 3000, val_time: 45s 124ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.910099\n",
            "\u001b[32m2022-08-03T19:21:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:21:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:21:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:21:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:21:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:22:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, train/misogyny_memes/cross_entropy: 0.2933, train/misogyny_memes/cross_entropy/avg: 0.3324, train/total_loss: 0.2933, train/total_loss/avg: 0.3324, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 3000, lr: 0.00005, ups: 0.37, time: 04m 28s 559ms, time_since_start: 52m 35s 087ms, eta: 01h 31m 18s 622ms\n",
            "\u001b[32m2022-08-03T19:22:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:22:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:22:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:22:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:22:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:22:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, val/misogyny_memes/cross_entropy: 0.5231, val/total_loss: 0.5231, val/misogyny_memes/accuracy: 0.8380, val/misogyny_memes/binary_f1: 0.8506, val/misogyny_memes/roc_auc: 0.9030, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 3000, val_time: 40s 965ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.910099\n",
            "\u001b[32m2022-08-03T19:26:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:26:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:26:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:26:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:26:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:27:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/3000, val/misogyny_memes/cross_entropy: 0.4941, val/total_loss: 0.4941, val/misogyny_memes/accuracy: 0.8250, val/misogyny_memes/binary_f1: 0.8372, val/misogyny_memes/roc_auc: 0.9030, num_updates: 1050, epoch: 4, iterations: 1050, max_updates: 3000, val_time: 43s 944ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.910099\n",
            "\u001b[32m2022-08-03T19:31:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, train/misogyny_memes/cross_entropy: 0.2933, train/misogyny_memes/cross_entropy/avg: 0.3162, train/total_loss: 0.2933, train/total_loss/avg: 0.3162, max mem: 10794.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 3000, lr: 0.00005, ups: 0.43, time: 03m 53s 207ms, time_since_start: 01h 01m 48s 838ms, eta: 01h 15m 19s 561ms\n",
            "\u001b[32m2022-08-03T19:31:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:31:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:31:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:31:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:31:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:32:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, val/misogyny_memes/cross_entropy: 0.5105, val/total_loss: 0.5105, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8314, val/misogyny_memes/roc_auc: 0.9049, num_updates: 1100, epoch: 4, iterations: 1100, max_updates: 3000, val_time: 43s 313ms, best_update: 850, best_iteration: 850, best_val/misogyny_memes/roc_auc: 0.910099\n",
            "\u001b[32m2022-08-03T19:35:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:35:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:35:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:35:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:35:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:36:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/3000, val/misogyny_memes/cross_entropy: 0.5548, val/total_loss: 0.5548, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8506, val/misogyny_memes/roc_auc: 0.9138, num_updates: 1150, epoch: 5, iterations: 1150, max_updates: 3000, val_time: 51s 026ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T19:39:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, train/misogyny_memes/cross_entropy: 0.2896, train/misogyny_memes/cross_entropy/avg: 0.3140, train/total_loss: 0.2896, train/total_loss/avg: 0.3140, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 3000, lr: 0.00005, ups: 0.46, time: 03m 37s 760ms, time_since_start: 01h 10m 26s 102ms, eta: 01h 06m 38s 076ms\n",
            "\u001b[32m2022-08-03T19:39:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:40:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:40:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:40:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:40:07 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:40:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, val/misogyny_memes/cross_entropy: 0.4849, val/total_loss: 0.4849, val/misogyny_memes/accuracy: 0.8290, val/misogyny_memes/binary_f1: 0.8430, val/misogyny_memes/roc_auc: 0.9105, num_updates: 1200, epoch: 5, iterations: 1200, max_updates: 3000, val_time: 41s 005ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T19:44:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:44:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:44:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:44:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:44:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:45:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/3000, val/misogyny_memes/cross_entropy: 0.6496, val/total_loss: 0.6496, val/misogyny_memes/accuracy: 0.8360, val/misogyny_memes/binary_f1: 0.8389, val/misogyny_memes/roc_auc: 0.9099, num_updates: 1250, epoch: 5, iterations: 1250, max_updates: 3000, val_time: 37s 423ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T19:48:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, train/misogyny_memes/cross_entropy: 0.2896, train/misogyny_memes/cross_entropy/avg: 0.2913, train/total_loss: 0.2896, train/total_loss/avg: 0.2913, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 3000, lr: 0.00005, ups: 0.46, time: 03m 39s 293ms, time_since_start: 01h 19m 10s 801ms, eta: 01h 03m 22s 544ms\n",
            "\u001b[32m2022-08-03T19:48:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:50:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:50:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:50:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:50:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:51:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, val/misogyny_memes/cross_entropy: 0.7932, val/total_loss: 0.7932, val/misogyny_memes/accuracy: 0.8320, val/misogyny_memes/binary_f1: 0.8340, val/misogyny_memes/roc_auc: 0.9009, num_updates: 1300, epoch: 5, iterations: 1300, max_updates: 3000, val_time: 02m 24s 821ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T19:54:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:55:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:55:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:55:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:55:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T19:55:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/3000, val/misogyny_memes/cross_entropy: 0.7296, val/total_loss: 0.7296, val/misogyny_memes/accuracy: 0.8290, val/misogyny_memes/binary_f1: 0.8253, val/misogyny_memes/roc_auc: 0.9013, num_updates: 1350, epoch: 5, iterations: 1350, max_updates: 3000, val_time: 48s 637ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T19:59:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, train/misogyny_memes/cross_entropy: 0.2687, train/misogyny_memes/cross_entropy/avg: 0.2714, train/total_loss: 0.2687, train/total_loss/avg: 0.2714, max mem: 10794.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 3000, lr: 0.00005, ups: 0.44, time: 03m 49s 549ms, time_since_start: 01h 30m 04s 787ms, eta: 01h 02m 26s 252ms\n",
            "\u001b[32m2022-08-03T19:59:33 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:59:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:59:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:59:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T19:59:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T20:00:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, val/misogyny_memes/cross_entropy: 0.7951, val/total_loss: 0.7951, val/misogyny_memes/accuracy: 0.8330, val/misogyny_memes/binary_f1: 0.8338, val/misogyny_memes/roc_auc: 0.9052, num_updates: 1400, epoch: 5, iterations: 1400, max_updates: 3000, val_time: 51s 351ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T20:03:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:03:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:03:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:03:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:03:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T20:03:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/3000, val/misogyny_memes/cross_entropy: 0.6624, val/total_loss: 0.6624, val/misogyny_memes/accuracy: 0.8190, val/misogyny_memes/binary_f1: 0.8188, val/misogyny_memes/roc_auc: 0.9036, num_updates: 1450, epoch: 6, iterations: 1450, max_updates: 3000, val_time: 47s 054ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T20:06:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, train/misogyny_memes/cross_entropy: 0.2687, train/misogyny_memes/cross_entropy/avg: 0.2534, train/total_loss: 0.2687, train/total_loss/avg: 0.2534, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 3000, lr: 0.00004, ups: 0.68, time: 02m 28s 082ms, time_since_start: 01h 36m 55s 986ms, eta: 37m 45s 655ms\n",
            "\u001b[32m2022-08-03T20:06:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:06:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:06:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:06:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:06:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T20:07:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/misogyny_memes/cross_entropy: 0.9203, val/total_loss: 0.9203, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8351, val/misogyny_memes/roc_auc: 0.9047, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 3000, val_time: 52s 017ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T20:09:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:09:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:09:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:09:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:09:54 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T20:10:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/3000, val/misogyny_memes/cross_entropy: 0.5449, val/total_loss: 0.5449, val/misogyny_memes/accuracy: 0.8330, val/misogyny_memes/binary_f1: 0.8417, val/misogyny_memes/roc_auc: 0.9121, num_updates: 1550, epoch: 6, iterations: 1550, max_updates: 3000, val_time: 42s 873ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T20:13:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, train/misogyny_memes/cross_entropy: 0.1619, train/misogyny_memes/cross_entropy/avg: 0.2441, train/total_loss: 0.1619, train/total_loss/avg: 0.2441, max mem: 10794.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 3000, lr: 0.00004, ups: 0.64, time: 02m 36s 928ms, time_since_start: 01h 43m 31s 947ms, eta: 37m 20s 941ms\n",
            "\u001b[32m2022-08-03T20:13:00 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:13:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:13:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:13:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:13:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T20:13:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, val/misogyny_memes/cross_entropy: 0.9490, val/total_loss: 0.9490, val/misogyny_memes/accuracy: 0.8330, val/misogyny_memes/binary_f1: 0.8371, val/misogyny_memes/roc_auc: 0.9052, num_updates: 1600, epoch: 6, iterations: 1600, max_updates: 3000, val_time: 51s 767ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n",
            "\u001b[32m2022-08-03T20:16:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:16:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:16:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:16:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-03T20:16:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-03T20:17:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/3000, val/misogyny_memes/cross_entropy: 0.6268, val/total_loss: 0.6268, val/misogyny_memes/accuracy: 0.8310, val/misogyny_memes/binary_f1: 0.8395, val/misogyny_memes/roc_auc: 0.9060, num_updates: 1650, epoch: 6, iterations: 1650, max_updates: 3000, val_time: 51s 415ms, best_update: 1150, best_iteration: 1150, best_val/misogyny_memes/roc_auc: 0.913794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "CEK19aiBkEha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## mmbt 5"
      ],
      "metadata": {
        "id": "jAb7S-70jNiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# Define where image features are\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "# Define where train.jsonl is\n",
        "train_dir = \"/content/drive/MyDrive/data/train.jsonl\"\n",
        "\n",
        "\n",
        "!mmf_run config=\"projects/mmbt/configs/misogyny_memes/defaults.yaml\" \\\n",
        "        model=\"mmbt\" \\\n",
        "        dataset=misogyny_memes \\\n",
        "        run_type=train_val \\\n",
        "        checkpoint.max_to_keep=1 \\\n",
        "        training.tensorboard=True \\\n",
        "        checkpoint.resume=True \\\n",
        "        checkpoint.resume_best=True \\\n",
        "        training.checkpoint_interval=50 \\\n",
        "        training.evaluation_interval=50 \\\n",
        "        training.max_updates=3000 \\\n",
        "        training.log_interval=100 \\\n",
        "        dataset_config.misogyny_memes.max_features=100 \\\n",
        "        dataset_config.misogyny_memes.annotations.train[0]=$train_dir \\\n",
        "        dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "        dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "        training.lr_ratio=0.3 \\\n",
        "        training.use_warmup=True \\\n",
        "        training.batch_size=32 \\\n",
        "        optimizer.params.lr=5.0e-05 \\\n",
        "        scheduler.params.num_warmup_steps=500 \\\n",
        "        scheduler.type=warmup_cosine \\\n",
        "        scheduler.params.num_training_steps=5000 \\\n",
        "        env.save_dir=./mmbt_final \\\n",
        "        env.tensorboard_logdir=logs/fit/mmbt_final \\"
      ],
      "metadata": {
        "id": "xza1fw75kLpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3733e486-128e-493f-b404-ae83c890338e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmbt/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option run_type to train_val\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.max_to_keep to 1\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option training.tensorboard to True\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume to True\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_best to True\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option training.checkpoint_interval to 50\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option training.evaluation_interval to 50\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option training.max_updates to 3000\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option training.log_interval to 100\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.max_features to 100\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.train[0] to /content/drive/MyDrive/data/train.jsonl\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option training.lr_ratio to 0.3\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option training.use_warmup to True\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option training.batch_size to 32\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option optimizer.params.lr to 5.0e-05\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_warmup_steps to 500\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.type to warmup_cosine\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option scheduler.params.num_training_steps to 5000\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option env.save_dir to ./mmbt_final\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.configuration: \u001b[0mOverriding option env.tensorboard_logdir to logs/fit/mmbt_final\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf: \u001b[0mLogging to: ./mmbt_final/train.log\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmbt/configs/misogyny_memes/defaults.yaml', 'model=mmbt', 'dataset=misogyny_memes', 'run_type=train_val', 'checkpoint.max_to_keep=1', 'training.tensorboard=True', 'checkpoint.resume=True', 'checkpoint.resume_best=True', 'training.checkpoint_interval=50', 'training.evaluation_interval=50', 'training.max_updates=3000', 'training.log_interval=100', 'dataset_config.misogyny_memes.max_features=100', 'dataset_config.misogyny_memes.annotations.train[0]=/content/drive/MyDrive/data/train.jsonl', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'training.lr_ratio=0.3', 'training.use_warmup=True', 'training.batch_size=32', 'optimizer.params.lr=5.0e-05', 'scheduler.params.num_warmup_steps=500', 'scheduler.type=warmup_cosine', 'scheduler.params.num_training_steps=5000', 'env.save_dir=./mmbt_final', 'env.tensorboard_logdir=logs/fit/mmbt_final'])\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf_cli.run: \u001b[0mUsing seed 20781512\n",
            "\u001b[32m2022-08-05T05:26:20 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-05T05:26:24 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "\u001b[32m2022-08-05T05:26:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-05T05:26:33 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:26:33 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:171: UserWarning: Tried to resume but checkpoint filepath ./mmbt_final/best.ckpt is not present. Trying current.ckpt, otherwise skipping.\n",
            "  ckpt_filepath, reverse_suffix\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:26:33 | py.warnings: \u001b[0m/content/drive/MyDrive/mmf/mmf/utils/checkpoint.py:171: UserWarning: Tried to resume but checkpoint filepath ./mmbt_final/best.ckpt is not present. Trying current.ckpt, otherwise skipping.\n",
            "  ckpt_filepath, reverse_suffix\n",
            "\n",
            "\u001b[32m2022-08-05T05:26:33 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
            "\u001b[32m2022-08-05T05:26:33 | mmf.trainers.mmf_trainer: \u001b[0mMMBT(\n",
            "  (model): MMBTForClassification(\n",
            "    (bert): MMBTBase(\n",
            "      (mmbt): MMBTModel(\n",
            "        (transformer): BertModelJit(\n",
            "          (embeddings): BertEmbeddingsJit(\n",
            "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "            (position_embeddings): Embedding(512, 768)\n",
            "            (token_type_embeddings): Embedding(2, 768)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (encoder): BertEncoderJit(\n",
            "            (layer): ModuleList(\n",
            "              (0): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (1): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (2): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (3): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (4): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (5): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (6): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (7): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (8): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (9): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (10): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "              (11): BertLayerJit(\n",
            "                (attention): BertAttentionJit(\n",
            "                  (self): BertSelfAttentionJit(\n",
            "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                  (output): BertSelfOutput(\n",
            "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                    (dropout): Dropout(p=0.1, inplace=False)\n",
            "                  )\n",
            "                )\n",
            "                (intermediate): BertIntermediate(\n",
            "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "                )\n",
            "                (output): BertOutput(\n",
            "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                  (dropout): Dropout(p=0.1, inplace=False)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (pooler): BertPooler(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (activation): Tanh()\n",
            "          )\n",
            "        )\n",
            "        (modal_encoder): ModalEmbeddings(\n",
            "          (encoder): ResNet152ImageEncoder(\n",
            "            (model): Sequential(\n",
            "              (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): ReLU(inplace=True)\n",
            "              (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "              (4): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (5): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (6): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (3): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (4): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (5): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (6): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (7): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (8): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (9): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (10): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (11): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (12): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (13): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (14): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (15): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (16): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (17): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (18): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (19): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (20): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (21): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (22): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (23): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (24): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (25): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (26): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (27): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (28): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (29): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (30): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (31): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (32): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (33): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (34): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (35): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "              (7): Sequential(\n",
            "                (0): Bottleneck(\n",
            "                  (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                  (downsample): Sequential(\n",
            "                    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "                    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  )\n",
            "                )\n",
            "                (1): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "                (2): Bottleneck(\n",
            "                  (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "                  (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "                  (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "                  (relu): ReLU(inplace=True)\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "          )\n",
            "          (proj_embeddings): Linear(in_features=2048, out_features=768, bias=True)\n",
            "          (position_embeddings): Embedding(512, 768)\n",
            "          (token_type_embeddings): Embedding(2, 768)\n",
            "          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Sequential(\n",
            "      (0): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (1): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (losses): Losses(\n",
            "    (losses): ModuleList(\n",
            "      (0): MMFLoss(\n",
            "        (loss_criterion): CrossEntropyLoss(\n",
            "          (loss_fn): CrossEntropyLoss()\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\u001b[32m2022-08-05T05:26:33 | mmf.utils.general: \u001b[0mTotal Parameters: 169793346. Trained Parameters: 169793346\n",
            "\u001b[32m2022-08-05T05:26:33 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
            "\u001b[32m2022-08-05T05:27:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:27:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:27:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:27:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:27:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:27:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:28:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 50/3000, val/misogyny_memes/cross_entropy: 0.7337, val/total_loss: 0.7337, val/misogyny_memes/accuracy: 0.4890, val/misogyny_memes/binary_f1: 0.0658, val/misogyny_memes/roc_auc: 0.5730, num_updates: 50, epoch: 1, iterations: 50, max_updates: 3000, val_time: 33s 324ms, best_update: 50, best_iteration: 50, best_val/misogyny_memes/roc_auc: 0.573034\n",
            "\u001b[32m2022-08-05T05:28:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:28:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:28:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:29:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, train/misogyny_memes/cross_entropy: 0.6088, train/misogyny_memes/cross_entropy/avg: 0.6088, train/total_loss: 0.6088, train/total_loss/avg: 0.6088, max mem: 11667.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 3000, lr: 0.00001, ups: 1.61, time: 01m 02s 824ms, time_since_start: 02m 38s 347ms, eta: 30m 58s 336ms\n",
            "\u001b[32m2022-08-05T05:29:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:29:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:29:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:29:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/3000, val/misogyny_memes/cross_entropy: 0.6216, val/total_loss: 0.6216, val/misogyny_memes/accuracy: 0.6430, val/misogyny_memes/binary_f1: 0.6909, val/misogyny_memes/roc_auc: 0.7240, num_updates: 100, epoch: 1, iterations: 100, max_updates: 3000, val_time: 38s 667ms, best_update: 100, best_iteration: 100, best_val/misogyny_memes/roc_auc: 0.723965\n",
            "\u001b[32m2022-08-05T05:30:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:30:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:30:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:30:53 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:31:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:31:03 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:31:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 150/3000, val/misogyny_memes/cross_entropy: 0.5760, val/total_loss: 0.5760, val/misogyny_memes/accuracy: 0.6900, val/misogyny_memes/binary_f1: 0.7395, val/misogyny_memes/roc_auc: 0.7928, num_updates: 150, epoch: 1, iterations: 150, max_updates: 3000, val_time: 37s 148ms, best_update: 150, best_iteration: 150, best_val/misogyny_memes/roc_auc: 0.792772\n",
            "\u001b[32m2022-08-05T05:32:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:32:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:32:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:32:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, train/misogyny_memes/cross_entropy: 0.4718, train/misogyny_memes/cross_entropy/avg: 0.5403, train/total_loss: 0.4718, train/total_loss/avg: 0.5403, max mem: 11667.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 3000, lr: 0.00002, ups: 1.72, time: 58s 757ms, time_since_start: 05m 55s 675ms, eta: 27m 58s 108ms\n",
            "\u001b[32m2022-08-05T05:32:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:32:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:32:38 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:33:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/3000, val/misogyny_memes/cross_entropy: 0.4853, val/total_loss: 0.4853, val/misogyny_memes/accuracy: 0.7700, val/misogyny_memes/binary_f1: 0.7745, val/misogyny_memes/roc_auc: 0.8518, num_updates: 200, epoch: 1, iterations: 200, max_updates: 3000, val_time: 38s 801ms, best_update: 200, best_iteration: 200, best_val/misogyny_memes/roc_auc: 0.851768\n",
            "\u001b[32m2022-08-05T05:33:51 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:33:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:33:51 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:34:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:34:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:34:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:34:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 250/3000, val/misogyny_memes/cross_entropy: 0.4374, val/total_loss: 0.4374, val/misogyny_memes/accuracy: 0.7940, val/misogyny_memes/binary_f1: 0.8071, val/misogyny_memes/roc_auc: 0.8831, num_updates: 250, epoch: 1, iterations: 250, max_updates: 3000, val_time: 37s 428ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.883149\n",
            "\u001b[32m2022-08-05T05:35:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:35:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:35:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:35:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, train/misogyny_memes/cross_entropy: 0.4718, train/misogyny_memes/cross_entropy/avg: 0.4745, train/total_loss: 0.4718, train/total_loss/avg: 0.4745, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 3000, lr: 0.00003, ups: 1.72, time: 58s 622ms, time_since_start: 09m 10s 342ms, eta: 26m 54s 473ms\n",
            "\u001b[32m2022-08-05T05:35:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:35:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:35:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:36:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/3000, val/misogyny_memes/cross_entropy: 0.4905, val/total_loss: 0.4905, val/misogyny_memes/accuracy: 0.7840, val/misogyny_memes/binary_f1: 0.8047, val/misogyny_memes/roc_auc: 0.8735, num_updates: 300, epoch: 2, iterations: 300, max_updates: 3000, val_time: 29s 701ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.883149\n",
            "\u001b[32m2022-08-05T05:36:57 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:36:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:36:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:37:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:37:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:37:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:37:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 350/3000, val/misogyny_memes/cross_entropy: 0.5356, val/total_loss: 0.5356, val/misogyny_memes/accuracy: 0.7790, val/misogyny_memes/binary_f1: 0.7579, val/misogyny_memes/roc_auc: 0.8736, num_updates: 350, epoch: 2, iterations: 350, max_updates: 3000, val_time: 27s 659ms, best_update: 250, best_iteration: 250, best_val/misogyny_memes/roc_auc: 0.883149\n",
            "\u001b[32m2022-08-05T05:38:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:38:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:38:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:38:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, train/misogyny_memes/cross_entropy: 0.4679, train/misogyny_memes/cross_entropy/avg: 0.4728, train/total_loss: 0.4679, train/total_loss/avg: 0.4728, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 3000, lr: 0.00004, ups: 1.64, time: 01m 01s 020ms, time_since_start: 12m 10s 741ms, eta: 26m 58s 261ms\n",
            "\u001b[32m2022-08-05T05:38:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:38:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:38:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:39:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/3000, val/misogyny_memes/cross_entropy: 0.4338, val/total_loss: 0.4338, val/misogyny_memes/accuracy: 0.7950, val/misogyny_memes/binary_f1: 0.7944, val/misogyny_memes/roc_auc: 0.8837, num_updates: 400, epoch: 2, iterations: 400, max_updates: 3000, val_time: 41s 458ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.883709\n",
            "\u001b[32m2022-08-05T05:40:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:40:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:40:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:40:24 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:40:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:40:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:40:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 450/3000, val/misogyny_memes/cross_entropy: 0.4694, val/total_loss: 0.4694, val/misogyny_memes/accuracy: 0.8030, val/misogyny_memes/binary_f1: 0.8181, val/misogyny_memes/roc_auc: 0.8801, num_updates: 450, epoch: 2, iterations: 450, max_updates: 3000, val_time: 26s 560ms, best_update: 400, best_iteration: 400, best_val/misogyny_memes/roc_auc: 0.883709\n",
            "\u001b[32m2022-08-05T05:41:34 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:41:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:41:34 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:41:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, train/misogyny_memes/cross_entropy: 0.4718, train/misogyny_memes/cross_entropy/avg: 0.4872, train/total_loss: 0.4718, train/total_loss/avg: 0.4872, max mem: 11667.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 3000, lr: 0.00005, ups: 1.64, time: 01m 01s 559ms, time_since_start: 15m 18s 595ms, eta: 26m 09s 774ms\n",
            "\u001b[32m2022-08-05T05:41:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:42:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:42:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:42:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/3000, val/misogyny_memes/cross_entropy: 0.4557, val/total_loss: 0.4557, val/misogyny_memes/accuracy: 0.7800, val/misogyny_memes/binary_f1: 0.7660, val/misogyny_memes/roc_auc: 0.8857, num_updates: 500, epoch: 2, iterations: 500, max_updates: 3000, val_time: 39s 062ms, best_update: 500, best_iteration: 500, best_val/misogyny_memes/roc_auc: 0.885715\n",
            "\u001b[32m2022-08-05T05:43:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:43:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:43:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:43:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:43:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:43:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:44:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 550/3000, val/misogyny_memes/cross_entropy: 0.4245, val/total_loss: 0.4245, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8273, val/misogyny_memes/roc_auc: 0.9021, num_updates: 550, epoch: 2, iterations: 550, max_updates: 3000, val_time: 35s 154ms, best_update: 550, best_iteration: 550, best_val/misogyny_memes/roc_auc: 0.902054\n",
            "\u001b[32m2022-08-05T05:44:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:44:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:44:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:45:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, train/misogyny_memes/cross_entropy: 0.4679, train/misogyny_memes/cross_entropy/avg: 0.4335, train/total_loss: 0.4679, train/total_loss/avg: 0.4335, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 3000, lr: 0.00005, ups: 1.72, time: 58s 552ms, time_since_start: 18m 34s 408ms, eta: 23m 53s 365ms\n",
            "\u001b[32m2022-08-05T05:45:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:45:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:45:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:45:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/3000, val/misogyny_memes/cross_entropy: 0.4751, val/total_loss: 0.4751, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8323, val/misogyny_memes/roc_auc: 0.9030, num_updates: 600, epoch: 3, iterations: 600, max_updates: 3000, val_time: 37s 430ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.903038\n",
            "\u001b[32m2022-08-05T05:46:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:46:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:46:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:46:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:46:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:46:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:47:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 650/3000, val/misogyny_memes/cross_entropy: 0.4816, val/total_loss: 0.4816, val/misogyny_memes/accuracy: 0.8080, val/misogyny_memes/binary_f1: 0.8267, val/misogyny_memes/roc_auc: 0.8945, num_updates: 650, epoch: 3, iterations: 650, max_updates: 3000, val_time: 24s 859ms, best_update: 600, best_iteration: 600, best_val/misogyny_memes/roc_auc: 0.903038\n",
            "\u001b[32m2022-08-05T05:47:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:47:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:47:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:48:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, train/misogyny_memes/cross_entropy: 0.4679, train/misogyny_memes/cross_entropy/avg: 0.4043, train/total_loss: 0.4679, train/total_loss/avg: 0.4043, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 3000, lr: 0.00005, ups: 1.69, time: 59s 639ms, time_since_start: 21m 34s 874ms, eta: 23m 19s 145ms\n",
            "\u001b[32m2022-08-05T05:48:08 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:48:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:48:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:48:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, val/misogyny_memes/cross_entropy: 0.5109, val/total_loss: 0.5109, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8170, val/misogyny_memes/roc_auc: 0.9048, num_updates: 700, epoch: 3, iterations: 700, max_updates: 3000, val_time: 36s 724ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T05:49:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:49:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:49:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:49:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:50:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:50:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:50:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 750/3000, val/misogyny_memes/cross_entropy: 0.4186, val/total_loss: 0.4186, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8298, val/misogyny_memes/roc_auc: 0.9036, num_updates: 750, epoch: 3, iterations: 750, max_updates: 3000, val_time: 25s 094ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T05:51:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:51:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:51:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:51:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, train/misogyny_memes/cross_entropy: 0.3428, train/misogyny_memes/cross_entropy/avg: 0.3602, train/total_loss: 0.3428, train/total_loss/avg: 0.3602, max mem: 11667.0, experiment: run, epoch: 3, num_updates: 800, iterations: 800, max_updates: 3000, lr: 0.00005, ups: 1.67, time: 01m 393ms, time_since_start: 24m 43s 021ms, eta: 22m 35s 226ms\n",
            "\u001b[32m2022-08-05T05:51:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:51:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:51:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:51:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/3000, val/misogyny_memes/cross_entropy: 0.4903, val/total_loss: 0.4903, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8394, val/misogyny_memes/roc_auc: 0.9033, num_updates: 800, epoch: 3, iterations: 800, max_updates: 3000, val_time: 26s 237ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T05:52:26 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:52:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:52:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:52:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:52:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:52:55 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:53:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 850/3000, val/misogyny_memes/cross_entropy: 0.4554, val/total_loss: 0.4554, val/misogyny_memes/accuracy: 0.7950, val/misogyny_memes/binary_f1: 0.7858, val/misogyny_memes/roc_auc: 0.8993, num_updates: 850, epoch: 4, iterations: 850, max_updates: 3000, val_time: 27s 533ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T05:53:57 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:53:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:53:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:54:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, train/misogyny_memes/cross_entropy: 0.3428, train/misogyny_memes/cross_entropy/avg: 0.3211, train/total_loss: 0.3428, train/total_loss/avg: 0.3211, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 3000, lr: 0.00005, ups: 1.64, time: 01m 01s 980ms, time_since_start: 27m 41s 902ms, eta: 22m 07s 622ms\n",
            "\u001b[32m2022-08-05T05:54:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:54:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:54:24 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:54:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/3000, val/misogyny_memes/cross_entropy: 0.6206, val/total_loss: 0.6206, val/misogyny_memes/accuracy: 0.8200, val/misogyny_memes/binary_f1: 0.8182, val/misogyny_memes/roc_auc: 0.9015, num_updates: 900, epoch: 4, iterations: 900, max_updates: 3000, val_time: 30s 205ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T05:55:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:55:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:55:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:55:51 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:56:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:56:01 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:56:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 950/3000, val/misogyny_memes/cross_entropy: 0.6715, val/total_loss: 0.6715, val/misogyny_memes/accuracy: 0.8200, val/misogyny_memes/binary_f1: 0.8140, val/misogyny_memes/roc_auc: 0.8983, num_updates: 950, epoch: 4, iterations: 950, max_updates: 3000, val_time: 30s 279ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T05:57:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:57:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:57:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:57:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, train/misogyny_memes/cross_entropy: 0.2289, train/misogyny_memes/cross_entropy/avg: 0.3013, train/total_loss: 0.2289, train/total_loss/avg: 0.3013, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 3000, lr: 0.00005, ups: 1.56, time: 01m 04s 164ms, time_since_start: 30m 52s 441ms, eta: 21m 48s 956ms\n",
            "\u001b[32m2022-08-05T05:57:26 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:57:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:57:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:57:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/3000, val/misogyny_memes/cross_entropy: 0.6685, val/total_loss: 0.6685, val/misogyny_memes/accuracy: 0.8050, val/misogyny_memes/binary_f1: 0.8127, val/misogyny_memes/roc_auc: 0.8886, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 3000, val_time: 27s 606ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T05:58:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:58:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:58:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:58:57 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:59:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T05:59:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T05:59:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1050/3000, val/misogyny_memes/cross_entropy: 0.5515, val/total_loss: 0.5515, val/misogyny_memes/accuracy: 0.8280, val/misogyny_memes/binary_f1: 0.8352, val/misogyny_memes/roc_auc: 0.9018, num_updates: 1050, epoch: 4, iterations: 1050, max_updates: 3000, val_time: 29s 978ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:00:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:00:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:00:11 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:00:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, train/misogyny_memes/cross_entropy: 0.2289, train/misogyny_memes/cross_entropy/avg: 0.2760, train/total_loss: 0.2289, train/total_loss/avg: 0.2760, max mem: 11667.0, experiment: run, epoch: 4, num_updates: 1100, iterations: 1100, max_updates: 3000, lr: 0.00005, ups: 1.61, time: 01m 02s 997ms, time_since_start: 33m 56s 623ms, eta: 20m 20s 887ms\n",
            "\u001b[32m2022-08-05T06:00:30 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:00:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:00:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:00:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/3000, val/misogyny_memes/cross_entropy: 0.6863, val/total_loss: 0.6863, val/misogyny_memes/accuracy: 0.7960, val/misogyny_memes/binary_f1: 0.8259, val/misogyny_memes/roc_auc: 0.8920, num_updates: 1100, epoch: 4, iterations: 1100, max_updates: 3000, val_time: 27s 084ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:01:41 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:01:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:01:41 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:02:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:02:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:02:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:02:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1150/3000, val/misogyny_memes/cross_entropy: 0.6827, val/total_loss: 0.6827, val/misogyny_memes/accuracy: 0.8230, val/misogyny_memes/binary_f1: 0.8347, val/misogyny_memes/roc_auc: 0.9030, num_updates: 1150, epoch: 5, iterations: 1150, max_updates: 3000, val_time: 29s 864ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:03:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:03:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:03:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:03:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, train/misogyny_memes/cross_entropy: 0.1854, train/misogyny_memes/cross_entropy/avg: 0.2685, train/total_loss: 0.1854, train/total_loss/avg: 0.2685, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 3000, lr: 0.00005, ups: 1.59, time: 01m 03s 488ms, time_since_start: 37m 01s 106ms, eta: 19m 25s 643ms\n",
            "\u001b[32m2022-08-05T06:03:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:03:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:03:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:04:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/3000, val/misogyny_memes/cross_entropy: 0.6146, val/total_loss: 0.6146, val/misogyny_memes/accuracy: 0.8130, val/misogyny_memes/binary_f1: 0.8117, val/misogyny_memes/roc_auc: 0.8921, num_updates: 1200, epoch: 5, iterations: 1200, max_updates: 3000, val_time: 27s 849ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:04:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:04:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:04:46 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:05:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:05:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:05:15 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:05:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1250/3000, val/misogyny_memes/cross_entropy: 0.6620, val/total_loss: 0.6620, val/misogyny_memes/accuracy: 0.8190, val/misogyny_memes/binary_f1: 0.8265, val/misogyny_memes/roc_auc: 0.8891, num_updates: 1250, epoch: 5, iterations: 1250, max_updates: 3000, val_time: 30s 942ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:06:21 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:06:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:06:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:06:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, train/misogyny_memes/cross_entropy: 0.1854, train/misogyny_memes/cross_entropy/avg: 0.2540, train/total_loss: 0.1854, train/total_loss/avg: 0.2540, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 3000, lr: 0.00005, ups: 1.59, time: 01m 03s 831ms, time_since_start: 40m 07s 364ms, eta: 18m 26s 843ms\n",
            "\u001b[32m2022-08-05T06:06:41 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:06:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:06:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/3000, val/misogyny_memes/cross_entropy: 0.6283, val/total_loss: 0.6283, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8254, val/misogyny_memes/roc_auc: 0.8893, num_updates: 1300, epoch: 5, iterations: 1300, max_updates: 3000, val_time: 32s 305ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:07:57 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:07:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:07:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:08:16 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:08:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:08:26 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:08:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1350/3000, val/misogyny_memes/cross_entropy: 0.7895, val/total_loss: 0.7895, val/misogyny_memes/accuracy: 0.8010, val/misogyny_memes/binary_f1: 0.8039, val/misogyny_memes/roc_auc: 0.8848, num_updates: 1350, epoch: 5, iterations: 1350, max_updates: 3000, val_time: 26s 592ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:09:27 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:09:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:09:27 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:09:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, train/misogyny_memes/cross_entropy: 0.1675, train/misogyny_memes/cross_entropy/avg: 0.2478, train/total_loss: 0.1675, train/total_loss/avg: 0.2478, max mem: 11667.0, experiment: run, epoch: 5, num_updates: 1400, iterations: 1400, max_updates: 3000, lr: 0.00005, ups: 1.59, time: 01m 03s 197ms, time_since_start: 43m 12s 911ms, eta: 17m 11s 390ms\n",
            "\u001b[32m2022-08-05T06:09:46 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:09:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:09:56 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:10:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/3000, val/misogyny_memes/cross_entropy: 0.7231, val/total_loss: 0.7231, val/misogyny_memes/accuracy: 0.7990, val/misogyny_memes/binary_f1: 0.8113, val/misogyny_memes/roc_auc: 0.8861, num_updates: 1400, epoch: 5, iterations: 1400, max_updates: 3000, val_time: 29s 619ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:11:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:11:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:11:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:11:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:11:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:11:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:11:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1450/3000, val/misogyny_memes/cross_entropy: 0.8599, val/total_loss: 0.8599, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8301, val/misogyny_memes/roc_auc: 0.8630, num_updates: 1450, epoch: 6, iterations: 1450, max_updates: 3000, val_time: 25s 099ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:12:29 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:12:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:12:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:12:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, train/misogyny_memes/cross_entropy: 0.1675, train/misogyny_memes/cross_entropy/avg: 0.2317, train/total_loss: 0.1675, train/total_loss/avg: 0.2317, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 3000, lr: 0.00004, ups: 1.61, time: 01m 02s 560ms, time_since_start: 46m 14s 212ms, eta: 15m 57s 169ms\n",
            "\u001b[32m2022-08-05T06:12:48 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:12:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:12:57 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:13:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/3000, val/misogyny_memes/cross_entropy: 1.0102, val/total_loss: 1.0102, val/misogyny_memes/accuracy: 0.8090, val/misogyny_memes/binary_f1: 0.8271, val/misogyny_memes/roc_auc: 0.8442, num_updates: 1500, epoch: 6, iterations: 1500, max_updates: 3000, val_time: 27s 139ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:13:58 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:13:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:13:58 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:14:18 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:14:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:14:28 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:14:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1550/3000, val/misogyny_memes/cross_entropy: 0.7381, val/total_loss: 0.7381, val/misogyny_memes/accuracy: 0.8210, val/misogyny_memes/binary_f1: 0.8277, val/misogyny_memes/roc_auc: 0.8881, num_updates: 1550, epoch: 6, iterations: 1550, max_updates: 3000, val_time: 28s 479ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:15:30 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:15:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:15:30 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:15:50 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, train/misogyny_memes/cross_entropy: 0.1652, train/misogyny_memes/cross_entropy/avg: 0.2175, train/total_loss: 0.1652, train/total_loss/avg: 0.2175, max mem: 11667.0, experiment: run, epoch: 6, num_updates: 1600, iterations: 1600, max_updates: 3000, lr: 0.00004, ups: 1.59, time: 01m 03s 535ms, time_since_start: 49m 16s 912ms, eta: 15m 07s 280ms\n",
            "\u001b[32m2022-08-05T06:15:50 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:16:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:16:00 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:16:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/3000, val/misogyny_memes/cross_entropy: 0.8781, val/total_loss: 0.8781, val/misogyny_memes/accuracy: 0.7990, val/misogyny_memes/binary_f1: 0.7930, val/misogyny_memes/roc_auc: 0.8861, num_updates: 1600, epoch: 6, iterations: 1600, max_updates: 3000, val_time: 29s 510ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:17:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:17:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:17:04 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:17:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:17:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:17:35 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:17:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1650/3000, val/misogyny_memes/cross_entropy: 0.8293, val/total_loss: 0.8293, val/misogyny_memes/accuracy: 0.7940, val/misogyny_memes/binary_f1: 0.8012, val/misogyny_memes/roc_auc: 0.8816, num_updates: 1650, epoch: 6, iterations: 1650, max_updates: 3000, val_time: 29s 507ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:18:39 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:18:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:18:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:18:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, train/misogyny_memes/cross_entropy: 0.1652, train/misogyny_memes/cross_entropy/avg: 0.2114, train/total_loss: 0.1652, train/total_loss/avg: 0.2114, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 3000, lr: 0.00004, ups: 1.59, time: 01m 03s 912ms, time_since_start: 52m 25s 352ms, eta: 14m 07s 474ms\n",
            "\u001b[32m2022-08-05T06:18:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:19:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:19:08 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:19:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/3000, val/misogyny_memes/cross_entropy: 0.6328, val/total_loss: 0.6328, val/misogyny_memes/accuracy: 0.8000, val/misogyny_memes/binary_f1: 0.8113, val/misogyny_memes/roc_auc: 0.8794, num_updates: 1700, epoch: 7, iterations: 1700, max_updates: 3000, val_time: 31s 929ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:20:14 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:20:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:20:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:20:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:20:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:20:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:21:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1750/3000, val/misogyny_memes/cross_entropy: 0.9835, val/total_loss: 0.9835, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8177, val/misogyny_memes/roc_auc: 0.8744, num_updates: 1750, epoch: 7, iterations: 1750, max_updates: 3000, val_time: 27s 731ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:21:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:21:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:21:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:22:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, train/misogyny_memes/cross_entropy: 0.1228, train/misogyny_memes/cross_entropy/avg: 0.2030, train/total_loss: 0.1228, train/total_loss/avg: 0.2030, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 3000, lr: 0.00004, ups: 1.54, time: 01m 05s 311ms, time_since_start: 55m 33s 696ms, eta: 13m 19s 412ms\n",
            "\u001b[32m2022-08-05T06:22:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:22:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:22:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:22:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/3000, val/misogyny_memes/cross_entropy: 0.9508, val/total_loss: 0.9508, val/misogyny_memes/accuracy: 0.8030, val/misogyny_memes/binary_f1: 0.8115, val/misogyny_memes/roc_auc: 0.8799, num_updates: 1800, epoch: 7, iterations: 1800, max_updates: 3000, val_time: 27s 277ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:23:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:23:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:23:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:23:37 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:23:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:23:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:24:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1850/3000, val/misogyny_memes/cross_entropy: 0.8441, val/total_loss: 0.8441, val/misogyny_memes/accuracy: 0.8050, val/misogyny_memes/binary_f1: 0.8152, val/misogyny_memes/roc_auc: 0.8920, num_updates: 1850, epoch: 7, iterations: 1850, max_updates: 3000, val_time: 31s 830ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:24:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:24:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:24:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:25:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, train/misogyny_memes/cross_entropy: 0.1228, train/misogyny_memes/cross_entropy/avg: 0.1926, train/total_loss: 0.1228, train/total_loss/avg: 0.1926, max mem: 11667.0, experiment: run, epoch: 7, num_updates: 1900, iterations: 1900, max_updates: 3000, lr: 0.00004, ups: 1.67, time: 01m 499ms, time_since_start: 58m 36s 395ms, eta: 11m 18s 810ms\n",
            "\u001b[32m2022-08-05T06:25:10 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:25:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:25:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:25:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/3000, val/misogyny_memes/cross_entropy: 0.8753, val/total_loss: 0.8753, val/misogyny_memes/accuracy: 0.7990, val/misogyny_memes/binary_f1: 0.8077, val/misogyny_memes/roc_auc: 0.8864, num_updates: 1900, epoch: 7, iterations: 1900, max_updates: 3000, val_time: 30s 091ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:26:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:26:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:26:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:26:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:26:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:26:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:27:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1950/3000, val/misogyny_memes/cross_entropy: 0.9323, val/total_loss: 0.9323, val/misogyny_memes/accuracy: 0.8020, val/misogyny_memes/binary_f1: 0.8096, val/misogyny_memes/roc_auc: 0.8848, num_updates: 1950, epoch: 7, iterations: 1950, max_updates: 3000, val_time: 30s 073ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:27:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:27:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:27:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:28:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, train/misogyny_memes/cross_entropy: 0.1132, train/misogyny_memes/cross_entropy/avg: 0.1830, train/total_loss: 0.1132, train/total_loss/avg: 0.1830, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 3000, lr: 0.00004, ups: 1.69, time: 59s 450ms, time_since_start: 01h 01m 35s 859ms, eta: 10m 06s 392ms\n",
            "\u001b[32m2022-08-05T06:28:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:28:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:28:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:28:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/3000, val/misogyny_memes/cross_entropy: 0.9613, val/total_loss: 0.9613, val/misogyny_memes/accuracy: 0.8090, val/misogyny_memes/binary_f1: 0.8213, val/misogyny_memes/roc_auc: 0.8964, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 3000, val_time: 29s 672ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:29:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:29:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:29:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:29:44 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:29:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:29:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:30:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2050/3000, val/misogyny_memes/cross_entropy: 0.9974, val/total_loss: 0.9974, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8245, val/misogyny_memes/roc_auc: 0.8962, num_updates: 2050, epoch: 8, iterations: 2050, max_updates: 3000, val_time: 34s 326ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:31:02 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:31:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:31:02 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:31:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, train/misogyny_memes/cross_entropy: 0.0797, train/misogyny_memes/cross_entropy/avg: 0.1744, train/total_loss: 0.0797, train/total_loss/avg: 0.1744, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 3000, lr: 0.00004, ups: 1.64, time: 01m 01s 355ms, time_since_start: 01h 04m 46s 273ms, eta: 09m 23s 242ms\n",
            "\u001b[32m2022-08-05T06:31:20 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:31:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:31:29 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:31:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/3000, val/misogyny_memes/cross_entropy: 0.9386, val/total_loss: 0.9386, val/misogyny_memes/accuracy: 0.8110, val/misogyny_memes/binary_f1: 0.8119, val/misogyny_memes/roc_auc: 0.8984, num_updates: 2100, epoch: 8, iterations: 2100, max_updates: 3000, val_time: 29s 025ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:32:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:32:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:32:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:32:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:33:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:33:05 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:33:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2150/3000, val/misogyny_memes/cross_entropy: 1.0667, val/total_loss: 1.0667, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8261, val/misogyny_memes/roc_auc: 0.8950, num_updates: 2150, epoch: 8, iterations: 2150, max_updates: 3000, val_time: 29s 649ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:34:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:34:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:34:09 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:34:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, train/misogyny_memes/cross_entropy: 0.0600, train/misogyny_memes/cross_entropy/avg: 0.1673, train/total_loss: 0.0600, train/total_loss/avg: 0.1673, max mem: 11667.0, experiment: run, epoch: 8, num_updates: 2200, iterations: 2200, max_updates: 3000, lr: 0.00003, ups: 1.61, time: 01m 02s 352ms, time_since_start: 01h 07m 53s 995ms, eta: 08m 28s 793ms\n",
            "\u001b[32m2022-08-05T06:34:28 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:34:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:34:37 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:34:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/3000, val/misogyny_memes/cross_entropy: 0.9363, val/total_loss: 0.9363, val/misogyny_memes/accuracy: 0.8180, val/misogyny_memes/binary_f1: 0.8230, val/misogyny_memes/roc_auc: 0.9023, num_updates: 2200, epoch: 8, iterations: 2200, max_updates: 3000, val_time: 28s 082ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:35:40 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:35:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:35:40 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:36:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:36:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:36:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:36:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2250/3000, val/misogyny_memes/cross_entropy: 0.9016, val/total_loss: 0.9016, val/misogyny_memes/accuracy: 0.8050, val/misogyny_memes/binary_f1: 0.8176, val/misogyny_memes/roc_auc: 0.8942, num_updates: 2250, epoch: 8, iterations: 2250, max_updates: 3000, val_time: 28s 329ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:37:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:37:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:37:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:37:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, train/misogyny_memes/cross_entropy: 0.0513, train/misogyny_memes/cross_entropy/avg: 0.1600, train/total_loss: 0.0513, train/total_loss/avg: 0.1600, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 3000, lr: 0.00003, ups: 1.69, time: 59s 911ms, time_since_start: 01h 10m 55s 970ms, eta: 07m 07s 769ms\n",
            "\u001b[32m2022-08-05T06:37:29 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:37:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:37:39 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:37:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/3000, val/misogyny_memes/cross_entropy: 1.0553, val/total_loss: 1.0553, val/misogyny_memes/accuracy: 0.8180, val/misogyny_memes/binary_f1: 0.8205, val/misogyny_memes/roc_auc: 0.8979, num_updates: 2300, epoch: 9, iterations: 2300, max_updates: 3000, val_time: 29s 137ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:38:42 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:38:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:38:42 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:39:01 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:39:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:39:10 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:39:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2350/3000, val/misogyny_memes/cross_entropy: 0.9697, val/total_loss: 0.9697, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8208, val/misogyny_memes/roc_auc: 0.8998, num_updates: 2350, epoch: 9, iterations: 2350, max_updates: 3000, val_time: 28s 646ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:40:13 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:40:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:40:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:40:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, train/misogyny_memes/cross_entropy: 0.0235, train/misogyny_memes/cross_entropy/avg: 0.1534, train/total_loss: 0.0235, train/total_loss/avg: 0.1534, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2400, iterations: 2400, max_updates: 3000, lr: 0.00003, ups: 1.59, time: 01m 03s 845ms, time_since_start: 01h 14m 204ms, eta: 06m 30s 735ms\n",
            "\u001b[32m2022-08-05T06:40:34 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:40:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:40:43 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:41:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/3000, val/misogyny_memes/cross_entropy: 0.9943, val/total_loss: 0.9943, val/misogyny_memes/accuracy: 0.8270, val/misogyny_memes/binary_f1: 0.8344, val/misogyny_memes/roc_auc: 0.9020, num_updates: 2400, epoch: 9, iterations: 2400, max_updates: 3000, val_time: 30s 062ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:41:47 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:41:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:41:47 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:42:06 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:42:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:42:16 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:42:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2450/3000, val/misogyny_memes/cross_entropy: 1.0869, val/total_loss: 1.0869, val/misogyny_memes/accuracy: 0.8190, val/misogyny_memes/binary_f1: 0.8238, val/misogyny_memes/roc_auc: 0.8918, num_updates: 2450, epoch: 9, iterations: 2450, max_updates: 3000, val_time: 28s 375ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:43:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:43:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:43:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:43:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, train/misogyny_memes/cross_entropy: 0.0179, train/misogyny_memes/cross_entropy/avg: 0.1473, train/total_loss: 0.0179, train/total_loss/avg: 0.1473, max mem: 11667.0, experiment: run, epoch: 9, num_updates: 2500, iterations: 2500, max_updates: 3000, lr: 0.00003, ups: 1.69, time: 59s 993ms, time_since_start: 01h 17m 01s 205ms, eta: 05m 05s 964ms\n",
            "\u001b[32m2022-08-05T06:43:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:43:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:43:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:44:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/3000, val/misogyny_memes/cross_entropy: 1.0256, val/total_loss: 1.0256, val/misogyny_memes/accuracy: 0.8150, val/misogyny_memes/binary_f1: 0.8279, val/misogyny_memes/roc_auc: 0.8947, num_updates: 2500, epoch: 9, iterations: 2500, max_updates: 3000, val_time: 31s 389ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:44:50 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:44:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:44:50 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:45:05 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:45:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:45:14 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:45:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2550/3000, val/misogyny_memes/cross_entropy: 1.0305, val/total_loss: 1.0305, val/misogyny_memes/accuracy: 0.8160, val/misogyny_memes/binary_f1: 0.8227, val/misogyny_memes/roc_auc: 0.8928, num_updates: 2550, epoch: 10, iterations: 2550, max_updates: 3000, val_time: 28s 628ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:46:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:46:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:46:17 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:46:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, train/misogyny_memes/cross_entropy: 0.0179, train/misogyny_memes/cross_entropy/avg: 0.1430, train/total_loss: 0.0179, train/total_loss/avg: 0.1430, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 3000, lr: 0.00003, ups: 1.64, time: 01m 01s 310ms, time_since_start: 01h 20m 01s 437ms, eta: 04m 10s 146ms\n",
            "\u001b[32m2022-08-05T06:46:35 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:46:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:46:44 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:47:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/3000, val/misogyny_memes/cross_entropy: 0.8963, val/total_loss: 0.8963, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8194, val/misogyny_memes/roc_auc: 0.8934, num_updates: 2600, epoch: 10, iterations: 2600, max_updates: 3000, val_time: 26s 985ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:47:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:47:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:47:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:48:04 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:48:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:48:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:48:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:48:13 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:48:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2650/3000, val/misogyny_memes/cross_entropy: 0.9906, val/total_loss: 0.9906, val/misogyny_memes/accuracy: 0.8240, val/misogyny_memes/binary_f1: 0.8346, val/misogyny_memes/roc_auc: 0.8925, num_updates: 2650, epoch: 10, iterations: 2650, max_updates: 3000, val_time: 30s 725ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:48:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2650/3000, val/misogyny_memes/cross_entropy: 0.9906, val/total_loss: 0.9906, val/misogyny_memes/accuracy: 0.8240, val/misogyny_memes/binary_f1: 0.8346, val/misogyny_memes/roc_auc: 0.8925, num_updates: 2650, epoch: 10, iterations: 2650, max_updates: 3000, val_time: 30s 725ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:49:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:49:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:49:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:49:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:49:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:49:18 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:49:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, train/misogyny_memes/cross_entropy: 0.0085, train/misogyny_memes/cross_entropy/avg: 0.1378, train/total_loss: 0.0085, train/total_loss/avg: 0.1378, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 3000, lr: 0.00003, ups: 1.64, time: 01m 01s 534ms, time_since_start: 01h 23m 02s 632ms, eta: 03m 08s 294ms\n",
            "\u001b[32m2022-08-05T06:49:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-08-05T06:49:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, train/misogyny_memes/cross_entropy: 0.0085, train/misogyny_memes/cross_entropy/avg: 0.1378, train/total_loss: 0.0085, train/total_loss/avg: 0.1378, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2700, iterations: 2700, max_updates: 3000, lr: 0.00003, ups: 1.64, time: 01m 01s 534ms, time_since_start: 01h 23m 02s 632ms, eta: 03m 08s 294ms\n",
            "\u001b[32m2022-08-05T06:49:36 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:49:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:49:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:49:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:49:45 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:50:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, val/misogyny_memes/cross_entropy: 1.1091, val/total_loss: 1.1091, val/misogyny_memes/accuracy: 0.8220, val/misogyny_memes/binary_f1: 0.8324, val/misogyny_memes/roc_auc: 0.8929, num_updates: 2700, epoch: 10, iterations: 2700, max_updates: 3000, val_time: 29s 825ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:50:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/3000, val/misogyny_memes/cross_entropy: 1.1091, val/total_loss: 1.1091, val/misogyny_memes/accuracy: 0.8220, val/misogyny_memes/binary_f1: 0.8324, val/misogyny_memes/roc_auc: 0.8929, num_updates: 2700, epoch: 10, iterations: 2700, max_updates: 3000, val_time: 29s 825ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:50:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:50:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:50:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:50:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:50:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:50:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:51:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-08-05T06:51:09 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:51:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:51:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:51:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:51:19 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:51:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2750/3000, val/misogyny_memes/cross_entropy: 0.9947, val/total_loss: 0.9947, val/misogyny_memes/accuracy: 0.8240, val/misogyny_memes/binary_f1: 0.8327, val/misogyny_memes/roc_auc: 0.8940, num_updates: 2750, epoch: 10, iterations: 2750, max_updates: 3000, val_time: 31s 673ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:51:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2750/3000, val/misogyny_memes/cross_entropy: 0.9947, val/total_loss: 0.9947, val/misogyny_memes/accuracy: 0.8240, val/misogyny_memes/binary_f1: 0.8327, val/misogyny_memes/roc_auc: 0.8940, num_updates: 2750, epoch: 10, iterations: 2750, max_updates: 3000, val_time: 31s 673ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:52:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:52:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:52:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:52:25 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:52:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:52:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:52:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, train/misogyny_memes/cross_entropy: 0.0071, train/misogyny_memes/cross_entropy/avg: 0.1329, train/total_loss: 0.0071, train/total_loss/avg: 0.1329, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 3000, lr: 0.00002, ups: 1.64, time: 01m 01s 933ms, time_since_start: 01h 26m 09s 481ms, eta: 02m 06s 344ms\n",
            "\u001b[32m2022-08-05T06:52:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-08-05T06:52:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, train/misogyny_memes/cross_entropy: 0.0071, train/misogyny_memes/cross_entropy/avg: 0.1329, train/total_loss: 0.0071, train/total_loss/avg: 0.1329, max mem: 11667.0, experiment: run, epoch: 10, num_updates: 2800, iterations: 2800, max_updates: 3000, lr: 0.00002, ups: 1.64, time: 01m 01s 933ms, time_since_start: 01h 26m 09s 481ms, eta: 02m 06s 344ms\n",
            "\u001b[32m2022-08-05T06:52:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:52:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:52:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:52:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:52:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:53:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, val/misogyny_memes/cross_entropy: 1.0578, val/total_loss: 1.0578, val/misogyny_memes/accuracy: 0.8220, val/misogyny_memes/binary_f1: 0.8241, val/misogyny_memes/roc_auc: 0.8977, num_updates: 2800, epoch: 10, iterations: 2800, max_updates: 3000, val_time: 26s 229ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:53:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/3000, val/misogyny_memes/cross_entropy: 1.0578, val/total_loss: 1.0578, val/misogyny_memes/accuracy: 0.8220, val/misogyny_memes/binary_f1: 0.8241, val/misogyny_memes/roc_auc: 0.8977, num_updates: 2800, epoch: 10, iterations: 2800, max_updates: 3000, val_time: 26s 229ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:53:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:53:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:53:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:53:53 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:53:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:53:53 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:54:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-08-05T06:54:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:54:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:54:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:54:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:54:21 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:54:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2850/3000, val/misogyny_memes/cross_entropy: 1.0518, val/total_loss: 1.0518, val/misogyny_memes/accuracy: 0.8200, val/misogyny_memes/binary_f1: 0.8295, val/misogyny_memes/roc_auc: 0.9009, num_updates: 2850, epoch: 11, iterations: 2850, max_updates: 3000, val_time: 28s 473ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:54:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2850/3000, val/misogyny_memes/cross_entropy: 1.0518, val/total_loss: 1.0518, val/misogyny_memes/accuracy: 0.8200, val/misogyny_memes/binary_f1: 0.8295, val/misogyny_memes/roc_auc: 0.9009, num_updates: 2850, epoch: 11, iterations: 2850, max_updates: 3000, val_time: 28s 473ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:55:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:55:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:55:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:55:23 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:55:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:55:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:55:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, train/misogyny_memes/cross_entropy: 0.0057, train/misogyny_memes/cross_entropy/avg: 0.1283, train/total_loss: 0.0057, train/total_loss/avg: 0.1283, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 3000, lr: 0.00002, ups: 1.67, time: 01m 046ms, time_since_start: 01h 29m 06s 525ms, eta: 01m 01s 247ms\n",
            "\u001b[32m2022-08-05T06:55:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-08-05T06:55:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, train/misogyny_memes/cross_entropy: 0.0057, train/misogyny_memes/cross_entropy/avg: 0.1283, train/total_loss: 0.0057, train/total_loss/avg: 0.1283, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 3000, lr: 0.00002, ups: 1.67, time: 01m 046ms, time_since_start: 01h 29m 06s 525ms, eta: 01m 01s 247ms\n",
            "\u001b[32m2022-08-05T06:55:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:55:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:55:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:55:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:55:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:56:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, val/misogyny_memes/cross_entropy: 0.9960, val/total_loss: 0.9960, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8225, val/misogyny_memes/roc_auc: 0.8992, num_updates: 2900, epoch: 11, iterations: 2900, max_updates: 3000, val_time: 28s 358ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:56:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/3000, val/misogyny_memes/cross_entropy: 0.9960, val/total_loss: 0.9960, val/misogyny_memes/accuracy: 0.8140, val/misogyny_memes/binary_f1: 0.8225, val/misogyny_memes/roc_auc: 0.8992, num_updates: 2900, epoch: 11, iterations: 2900, max_updates: 3000, val_time: 28s 358ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:56:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:56:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:56:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:56:52 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:56:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:56:52 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:57:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-08-05T06:57:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:57:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:57:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:57:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:57:22 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:57:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2950/3000, val/misogyny_memes/cross_entropy: 1.0806, val/total_loss: 1.0806, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8235, val/misogyny_memes/roc_auc: 0.8962, num_updates: 2950, epoch: 11, iterations: 2950, max_updates: 3000, val_time: 26s 449ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:57:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2950/3000, val/misogyny_memes/cross_entropy: 1.0806, val/total_loss: 1.0806, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8235, val/misogyny_memes/roc_auc: 0.8962, num_updates: 2950, epoch: 11, iterations: 2950, max_updates: 3000, val_time: 26s 449ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:58:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:58:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:58:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:58:22 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:58:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:58:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:58:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, train/misogyny_memes/cross_entropy: 0.0040, train/misogyny_memes/cross_entropy/avg: 0.1241, train/total_loss: 0.0040, train/total_loss/avg: 0.1241, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 3000, lr: 0.00002, ups: 1.67, time: 01m 808ms, time_since_start: 01h 32m 06s 426ms, eta: 0ms\n",
            "\u001b[32m2022-08-05T06:58:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[32m2022-08-05T06:58:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, train/misogyny_memes/cross_entropy: 0.0040, train/misogyny_memes/cross_entropy/avg: 0.1241, train/total_loss: 0.0040, train/total_loss/avg: 0.1241, max mem: 11667.0, experiment: run, epoch: 11, num_updates: 3000, iterations: 3000, max_updates: 3000, lr: 0.00002, ups: 1.67, time: 01m 808ms, time_since_start: 01h 32m 06s 426ms, eta: 0ms\n",
            "\u001b[32m2022-08-05T06:58:40 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:58:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:58:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:58:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:58:49 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:59:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, val/misogyny_memes/cross_entropy: 1.1387, val/total_loss: 1.1387, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8249, val/misogyny_memes/roc_auc: 0.8897, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 3000, val_time: 29s 703ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:59:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/3000, val/misogyny_memes/cross_entropy: 1.1387, val/total_loss: 1.1387, val/misogyny_memes/accuracy: 0.8170, val/misogyny_memes/binary_f1: 0.8249, val/misogyny_memes/roc_auc: 0.8897, num_updates: 3000, epoch: 11, iterations: 3000, max_updates: 3000, val_time: 29s 703ms, best_update: 700, best_iteration: 700, best_val/misogyny_memes/roc_auc: 0.904760\n",
            "\u001b[32m2022-08-05T06:59:10 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-05T06:59:10 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-05T06:59:10 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[32m2022-08-05T06:59:10 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
            "\u001b[32m2022-08-05T06:59:10 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
            "\u001b[32m2022-08-05T06:59:10 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:59:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:59:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:59:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-05T06:59:25 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 700\n",
            "\u001b[32m2022-08-05T06:59:25 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 700\n",
            "\u001b[32m2022-08-05T06:59:25 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:59:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-05T06:59:25 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-05T06:59:25 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-05T06:59:25 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 700\n",
            "\u001b[32m2022-08-05T06:59:25 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 700\n",
            "\u001b[32m2022-08-05T06:59:25 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-05T06:59:30 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "  0% 0/32 [00:00<?, ?it/s]\u001b[32m2022-08-05T06:59:30 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on val set\n",
            "100% 32/32 [00:09<00:00,  3.42it/s]\n",
            "\u001b[32m2022-08-05T06:59:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, val/misogyny_memes/cross_entropy: 0.5109, val/total_loss: 0.5109, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8170, val/misogyny_memes/roc_auc: 0.9048\n",
            "\u001b[32m2022-08-05T06:59:39 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 33m 05s 879ms\n",
            "100% 32/32 [00:09<00:00,  3.42it/s]\n",
            "\u001b[32m2022-08-05T06:59:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/3000, val/misogyny_memes/cross_entropy: 0.5109, val/total_loss: 0.5109, val/misogyny_memes/accuracy: 0.7890, val/misogyny_memes/binary_f1: 0.8170, val/misogyny_memes/roc_auc: 0.9048\n",
            "\u001b[32m2022-08-05T06:59:39 | mmf.trainers.callbacks.logistics: \u001b[0mFinished run in 01h 33m 05s 879ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "6YfGkmLJgtF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "UXqj0-XVgY52"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VisualBERT"
      ],
      "metadata": {
        "id": "2tFLj-3IutU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# where checkpoint is\n",
        "\n",
        "ckpt_dir = os.path.join(home, \"visualbert_final/best.ckpt\")\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "\n",
        "!mmf_predict config=\"projects/visual_bert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "    model=\"visual_bert\" \\\n",
        "    dataset=misogyny_memes \\\n",
        "    run_type=test \\\n",
        "    checkpoint.resume_file=$ckpt_dir \\\n",
        "    checkpoint.reset.optimizer=True \\\n",
        "    dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "    dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "    dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "    dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "    dataset_config.misogyny_memes.features.test[0]=$feats_dir \\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f6E3Gi2KhBd",
        "outputId": "714d4959-1384-4a83-c029-192ea6f2ecf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/visual_bert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /content/drive/MyDrive/visualbert_final/best.ckpt\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/visual_bert/configs/misogyny_memes/defaults.yaml', 'model=visual_bert', 'dataset=misogyny_memes', 'run_type=test', 'checkpoint.resume_file=/content/drive/MyDrive/visualbert_final/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'evaluation.predict=true'])\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf_cli.run: \u001b[0mUsing seed 746585\n",
            "\u001b[32m2022-08-06T08:55:00 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "Downloading: 100% 433/433 [00:00<00:00, 498kB/s]\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 3.17MB/s]\n",
            "\u001b[32m2022-08-06T08:55:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 80.9MB/s]\n",
            "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-06T08:55:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-06T08:55:16 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-06T08:55:17 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T08:55:30 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T08:55:30 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[32m2022-08-06T08:55:30 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-06T08:55:30 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 700\n",
            "\u001b[32m2022-08-06T08:55:30 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 700\n",
            "\u001b[32m2022-08-06T08:55:30 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-06T08:55:30 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
            "\u001b[32m2022-08-06T08:55:30 | mmf.common.test_reporter: \u001b[0mPredicting for misogyny_memes\n",
            "100% 32/32 [01:38<00:00,  3.08s/it]\n",
            "\u001b[32m2022-08-06T08:57:08 | mmf.common.test_reporter: \u001b[0mWrote predictions for misogyny_memes to /content/drive/MyDrive/save/misogyny_memes_visual_bert_746585/reports/misogyny_memes_run_test_2022-08-06T08:57:08.csv\n",
            "\u001b[32m2022-08-06T08:57:08 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "05SsGa_qKfh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViLBERT"
      ],
      "metadata": {
        "id": "vrhMol_wPi8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# where checkpoint is\n",
        "\n",
        "ckpt_dir = os.path.join(home, \"vilbert_final/best.ckpt\")\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "\n",
        "!mmf_predict config=\"projects/vilbert/configs/misogyny_memes/defaults.yaml\" \\\n",
        "    model=\"vilbert\" \\\n",
        "    dataset=misogyny_memes \\\n",
        "    run_type=test \\\n",
        "    checkpoint.resume_file=$ckpt_dir \\\n",
        "    checkpoint.reset.optimizer=True \\\n",
        "    dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "    dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "    dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "    dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "    dataset_config.misogyny_memes.features.test[0]=$feats_dir \\"
      ],
      "metadata": {
        "id": "wokozdfHUhAE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f55278-0167-4fdf-f1af-47e556156b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
            "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
            "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
            "  warnings.warn(message=msg, category=UserWarning)\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/vilbert/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /content/drive/MyDrive/vilbert_final/best.ckpt\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/vilbert/configs/misogyny_memes/defaults.yaml', 'model=vilbert', 'dataset=misogyny_memes', 'run_type=test', 'checkpoint.resume_file=/content/drive/MyDrive/vilbert_final/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'evaluation.predict=true'])\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf_cli.run: \u001b[0mUsing seed 56021019\n",
            "\u001b[32m2022-08-06T09:17:55 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-06T09:17:59 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2022-08-06T09:18:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-06T09:18:05 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-06T09:18:05 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T09:18:33 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T09:18:33 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T09:18:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T09:18:33 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-06T09:18:33 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-06T09:18:33 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 1150\n",
            "\u001b[32m2022-08-06T09:18:33 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 1150\n",
            "\u001b[32m2022-08-06T09:18:33 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 5\n",
            "\u001b[32m2022-08-06T09:18:33 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
            "\u001b[32m2022-08-06T09:18:33 | mmf.common.test_reporter: \u001b[0mPredicting for misogyny_memes\n",
            "100% 32/32 [00:12<00:00,  2.66it/s]\n",
            "\u001b[32m2022-08-06T09:18:45 | mmf.common.test_reporter: \u001b[0mWrote predictions for misogyny_memes to /content/drive/MyDrive/save/misogyny_memes_vilbert_56021019/reports/misogyny_memes_run_test_2022-08-06T09:18:45.csv\n",
            "\u001b[32m2022-08-06T09:18:45 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "hspJdRAGUfWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MMBT"
      ],
      "metadata": {
        "id": "a4dE5xeSUr3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(home)\n",
        "# where checkpoint is\n",
        "\n",
        "ckpt_dir = os.path.join(home, \"mmbt_final/best.ckpt\")\n",
        "feats_dir = os.path.join(home, \"features\")\n",
        "\n",
        "!mmf_predict config=\"projects/mmbt/configs/misogyny_memes/defaults.yaml\" \\\n",
        "    model=\"mmbt\" \\\n",
        "    dataset=misogyny_memes \\\n",
        "    run_type=test \\\n",
        "    checkpoint.resume_file=$ckpt_dir \\\n",
        "    checkpoint.reset.optimizer=True \\\n",
        "    dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl \\\n",
        "    dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl \\\n",
        "    dataset_config.misogyny_memes.features.train[0]=$feats_dir \\\n",
        "    dataset_config.misogyny_memes.features.val[0]=$feats_dir \\\n",
        "    dataset_config.misogyny_memes.features.test[0]=$feats_dir \\"
      ],
      "metadata": {
        "id": "JDr8w8vcU24A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d42fbb0-e8c7-427c-d4e4-f0e265765271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/mmbt/configs/misogyny_memes/defaults.yaml\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option model to mmbt\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option datasets to misogyny_memes\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option run_type to test\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_file to /content/drive/MyDrive/mmbt_final/best.ckpt\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.reset.optimizer to True\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.val[0] to misogyny_memes/defaults/annotations/dev.jsonl\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.annotations.test[0] to misogyny_memes/defaults/annotations/test.jsonl\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.train[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.val[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.misogyny_memes.features.test[0] to /content/drive/MyDrive/features\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf: \u001b[0mLogging to: ./save/train.log\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/mmbt/configs/misogyny_memes/defaults.yaml', 'model=mmbt', 'dataset=misogyny_memes', 'run_type=test', 'checkpoint.resume_file=/content/drive/MyDrive/mmbt_final/best.ckpt', 'checkpoint.reset.optimizer=True', 'dataset_config.misogyny_memes.annotations.val[0]=misogyny_memes/defaults/annotations/dev.jsonl', 'dataset_config.misogyny_memes.annotations.test[0]=misogyny_memes/defaults/annotations/test.jsonl', 'dataset_config.misogyny_memes.features.train[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.val[0]=/content/drive/MyDrive/features', 'dataset_config.misogyny_memes.features.test[0]=/content/drive/MyDrive/features', 'evaluation.predict=true'])\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu92\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla P100-PCIE-16GB\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf_cli.run: \u001b[0mUsing seed 46943901\n",
            "\u001b[32m2022-08-06T09:20:46 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
            "\u001b[32m2022-08-06T09:20:50 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
            "Downloading: 100% 440M/440M [00:05<00:00, 81.4MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
            "100% 230M/230M [00:01<00:00, 121MB/s]\n",
            "\u001b[32m2022-08-06T09:21:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
            "\u001b[32m2022-08-06T09:21:07 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
            "\u001b[32m2022-08-06T09:21:07 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T09:21:23 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T09:21:23 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T09:21:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2022-08-06T09:21:23 | py.warnings: \u001b[0m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "\n",
            "\u001b[32m2022-08-06T09:21:23 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
            "\u001b[32m2022-08-06T09:21:23 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 700\n",
            "\u001b[32m2022-08-06T09:21:23 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 700\n",
            "\u001b[32m2022-08-06T09:21:23 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 3\n",
            "\u001b[32m2022-08-06T09:21:23 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting test inference predictions\n",
            "\u001b[32m2022-08-06T09:21:23 | mmf.common.test_reporter: \u001b[0mPredicting for misogyny_memes\n",
            "100% 32/32 [00:09<00:00,  3.47it/s]\n",
            "\u001b[32m2022-08-06T09:21:32 | mmf.common.test_reporter: \u001b[0mWrote predictions for misogyny_memes to /content/drive/MyDrive/save/misogyny_memes_mmbt_46943901/reports/misogyny_memes_run_test_2022-08-06T09:21:32.csv\n",
            "\u001b[32m2022-08-06T09:21:32 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "-h5-i5PzEada"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble: Majority Voting"
      ],
      "metadata": {
        "id": "nN5SKEjIv2Cq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hard Voting"
      ],
      "metadata": {
        "id": "b1BcTXCnrnVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Store all the prediction folders\n",
        "preds = pd.DataFrame()\n",
        "\n",
        "folders = [i for i in os.listdir(\"save_final\") if i.startswith(\"misogyny_memes\")]\n",
        "\n",
        "for folder in folders:\n",
        "    pred = [i for i in os.listdir(f\"save_final/{folder}/reports/\") if i.endswith(\".csv\")]\n",
        "    pred = pd.read_csv(f\"save_final/{folder}/reports/{pred[0]}\")\n",
        "    preds = pd.concat([preds, pred], axis=1)\n",
        "\n",
        "submission = pred\n",
        "np_df = np.asarray(preds)\n",
        "\n",
        "for idx, row in enumerate(np_df[:,:]):\n",
        "    probas = row[1::3]\n",
        "    labels = row[2::3]\n",
        "\n",
        "    if sum(labels) > 1.5:\n",
        "        submission.loc[idx, 'label']=1\n",
        "        submission.loc[idx, 'proba']=probas.max()    \n",
        "    else:\n",
        "        submission.loc[idx, 'label']=0\n",
        "        submission.loc[idx, 'proba']=probas.min()"
      ],
      "metadata": {
        "id": "CNixaEYZv461"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trimodel = submission.sort_values(by=['id'],ascending=True)\n",
        "trimodel.to_csv(f\"{home}/trimodel.csv\", index=False)"
      ],
      "metadata": {
        "id": "FnEsrarMaPcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Soft Voting"
      ],
      "metadata": {
        "id": "x2FH1NW8rvmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "preds = pd.DataFrame()\n",
        "\n",
        "folders = [i for i in os.listdir(\"save_final\") if i.startswith(\"misogyny_memes\")]\n",
        "\n",
        "for folder in folders:\n",
        "    pred = [i for i in os.listdir(f\"save_final/{folder}/reports/\") if i.endswith(\".csv\")]\n",
        "    pred = pd.read_csv(f\"save_final/{folder}/reports/{pred[0]}\")\n",
        "    preds = pd.concat([preds, pred], axis=1)\n",
        "\n",
        "submission = pred\n",
        "np_df = np.asarray(preds)\n",
        "\n",
        "for idx, row in enumerate(np_df[:,:]):\n",
        "    probas = row[1::3]\n",
        "    labels = row[2::3]\n",
        "    avg_proba = np.mean(probas)\n",
        "\n",
        "    if avg_proba >= 0.5:\n",
        "        submission.loc[idx, 'label']=1   \n",
        "    else:\n",
        "        submission.loc[idx, 'label']=0\n",
        "    submission.loc[idx, 'proba']=avg_proba"
      ],
      "metadata": {
        "id": "S-w34ca3rxju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soft_tri = submission.sort_values(by=['id'],ascending=True)\n",
        "soft_tri.to_csv(f\"{home}/soft_tri.csv\", index=False)"
      ],
      "metadata": {
        "id": "JcvQxBKBs6-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Qw9UhYbkrhA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_f1(submission_path, result_path, average):\n",
        "  sub_df = pd.read_csv(submission_path)\n",
        "  r_df = pd.read_csv(result_path)\n",
        "  sort_df = sub_df.sort_values(by=\"id\")\n",
        "  sort_df2 = r_df.sort_values(by=\"id\")\n",
        "  result = sort_df['label'].tolist()\n",
        "  pred = sort_df2['label'].tolist()\n",
        "  f1 = f1_score(result, pred, average=average)\n",
        "  return f1\n"
      ],
      "metadata": {
        "id": "gY5a_HnO7IWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def compute_roc_auc(submission_path, result_path):\n",
        "  sub_df = pd.read_csv(submission_path)\n",
        "  r_df = pd.read_csv(result_path)\n",
        "  sort_df = sub_df.sort_values(by=\"id\")\n",
        "  sort_df2 = r_df.sort_values(by=\"id\")\n",
        "  result = sort_df['label'].tolist()\n",
        "  pred = sort_df2['label'].tolist()\n",
        "  auc_score = roc_auc_score(result, pred)\n",
        "  return auc_score"
      ],
      "metadata": {
        "id": "dX_NaXMalwuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "submission_path = '/content/drive/MyDrive/save/misogyny_memes_mmbt_46943901/reports/misogyny_memes_run_test_2022-08-06T09:21:32.csv'\n",
        "result_path = '/content/drive/MyDrive/data/test_label.csv'\n",
        "f1 = compute_f1(submission_path, result_path, 'binary')   #'weighted'\n",
        "auc_score = compute_roc_auc(submission_path, result_path)\n",
        "f_soft_tri = f1\n",
        "a_soft_tri = auc_score"
      ],
      "metadata": {
        "id": "FenGCQfwrm2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# '/content/drive/MyDrive/save_final/misogyny_memes_visual_bert_31403057/reports/misogyny_memes_run_test_2022-08-05T07:32:00.csv'\n",
        "print(\"AUC_ROC: %f\" %a_visualbert_final)\n",
        "print(\"Binary-f1: %f\" %f1_visualbert_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkIXrQBS5XeE",
        "outputId": "d7728249-bd64-4f78-96bc-a8336ad1630a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_ROC: 0.736657\n",
            "Binary-f1: 0.752044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# '/content/drive/MyDrive/save_final/misogyny_memes_vilbert_28245324/reports/misogyny_memes_run_test_2022-08-05T07:29:10.csv'\n",
        "print(\"AUC_ROC: %f\" %a_vilbert_final)\n",
        "print(\"Binary-f1: %f\" %f1_vilbert_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2B_lm8i5U5d",
        "outputId": "bd036da7-55e7-4dc2-a163-094c92a507d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_ROC: 0.716391\n",
            "Binary-f1: 0.732303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# '/content/drive/MyDrive/save_final/misogyny_memes_mmbt_24833728/reports/misogyny_memes_run_test_2022-08-05T07:22:01.csv'\n",
        "print(\"AUC_ROC: %f\" %a_mmbt_final)\n",
        "print(\"Binary-f1: %f\" %f1_mmbt_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkQe2xmn4qqR",
        "outputId": "e454121e-5dc1-4b87-b7cd-89de1ce34a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_ROC: 0.713984\n",
            "Binary-f1: 0.721793\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# soft-tri\n",
        "# '/content/drive/MyDrive/soft_tri.csv'\n",
        "print(\"AUC_ROC: %f\" %a_soft_tri)\n",
        "print(\"Binary-f1: %f\" %f1_soft_tri)"
      ],
      "metadata": {
        "id": "Bcy12UzFnjDp",
        "outputId": "3878eff9-5698-459e-f04c-0b63e7ddd4fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_ROC: 0.745889\n",
            "Binary-f1: 0.753289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hard-tri\n",
        "# '/content/drive/MyDrive/trimodel.csv'\n",
        "print(\"AUC_ROC: %f\" %a_hard_tri)\n",
        "print(\"Binary-f1: %f\" %f1_hard_tri)"
      ],
      "metadata": {
        "id": "_wrLb8pToCGn",
        "outputId": "a23717be-84b1-4253-ca65-b922cfd37f4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC_ROC: 0.739860\n",
            "Binary-f1: 0.748567\n"
          ]
        }
      ]
    }
  ]
}